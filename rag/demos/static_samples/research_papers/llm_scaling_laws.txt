Scaling Laws for Neural Language Models: An Empirical Study

Abstract

We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size.

1. Introduction

Understanding how language model performance scales with key factors like model size and training data is crucial for efficient resource allocation and model development. This work presents comprehensive empirical findings on scaling behavior across multiple orders of magnitude.

Our key findings include:
- Performance scales predictably with model parameters, dataset size, and compute
- Optimal allocation strategies for fixed compute budgets
- Clear evidence of sample efficiency improvements with scale
- Implications for future model development strategies

2. Methodology

2.1 Model Architecture
We use a standard transformer decoder architecture with:
- Attention dimension: d_model ∈ {768, 1024, 1536, 2048, 3072, 4096}
- Number of layers: n_layers ∈ {6, 12, 18, 24, 36, 48}
- Feed-forward ratio: 4:1 (standard)
- Vocabulary size: 50,257 tokens

2.2 Dataset
Training data consists of:
- Common Crawl web text: 570GB
- Books corpus: 67GB
- Wikipedia: 16GB
- News articles: 38GB
Total: ~40B tokens after preprocessing

2.3 Training Setup
- Optimizer: AdamW with β₁=0.9, β₂=0.95
- Learning rate: Cosine decay from 6e-4
- Batch size: Scaled with model size (1M to 4M tokens)
- Precision: Mixed precision (FP16/FP32)

3. Scaling Law Results

3.1 Power Law Relationships
We observe clear power-law relationships for cross-entropy loss L:

L(N) = (Nc/N)^αN + L∞
L(D) = (Dc/D)^αD + L∞
L(C) = (Cc/C)^αC + L∞

Where:
- N = number of parameters
- D = dataset size (tokens)
- C = compute (FLOPs)
- αN ≈ 0.076, αD ≈ 0.095, αC ≈ 0.050

3.2 Model Size Scaling
Performance improvements with parameter count:

| Parameters | Loss | PPL  | Training Time |
|------------|------|------|---------------|
| 117M       | 3.28 | 26.8 | 2.1 days      |
| 345M       | 2.95 | 19.1 | 4.8 days      |
| 762M       | 2.71 | 15.0 | 8.3 days      |
| 1.5B       | 2.52 | 12.4 | 15.2 days     |
| 2.7B       | 2.38 | 10.8 | 24.7 days     |
| 6.7B       | 2.21 | 9.1  | 41.3 days     |
| 13B        | 2.08 | 8.0  | 67.8 days     |

3.3 Data Scaling
Relationship between dataset size and performance:

Training Tokens vs Cross-Entropy Loss:
- 1B tokens: L = 3.45
- 10B tokens: L = 2.89
- 100B tokens: L = 2.41
- 300B tokens: L = 2.21
- 500B tokens: L = 2.12

3.4 Compute Scaling
Optimal compute allocation follows:
- 40% model parameters
- 60% training tokens
- Doubling compute → 1.8x model size, 1.12x data

4. Downstream Task Performance

4.1 Few-Shot Learning
Performance on few-shot tasks scales with model size:

| Model Size | LAMBADA | HellaSwag | StoryCloze | PIQA |
|------------|---------|-----------|------------|------|
| 117M       | 37.6%   | 43.2%     | 59.8%      | 64.1% |
| 345M       | 45.1%   | 50.7%     | 65.3%      | 68.9% |
| 762M       | 52.3%   | 58.4%     | 71.2%      | 73.4% |
| 1.5B       | 58.7%   | 64.8%     | 76.5%      | 77.8% |
| 6.7B       | 68.2%   | 74.1%     | 83.4%      | 82.3% |
| 13B        | 74.5%   | 78.9%     | 87.1%      | 85.2% |

4.2 Emergent Abilities
Certain capabilities emerge at specific model scales:
- Arithmetic reasoning: ~1B parameters
- Multi-step reasoning: ~6B parameters
- In-context learning: ~13B parameters
- Chain-of-thought reasoning: ~60B+ parameters (extrapolated)

5. Sample Efficiency Analysis

5.1 Few-Shot vs Fine-tuning
Sample efficiency comparison:

Task: Sentiment Classification
- 117M model: 10,000 examples needed (fine-tuning)
- 1.5B model: 1,000 examples needed (fine-tuning)
- 6.7B model: 100 examples needed (fine-tuning)
- 13B model: 10 examples needed (few-shot)

5.2 Transfer Learning
Scaling improves transfer across domains:
- Larger models transfer better to specialized domains
- Pre-training compute correlates with downstream performance
- Optimal fine-tuning strategies depend on model size

6. Implications for Model Development

6.1 Resource Allocation
For fixed compute budget C:
- Optimal model size: N_opt ∝ C^0.73
- Optimal training tokens: D_opt ∝ C^0.27
- Training should stop when loss reaches predictable threshold

6.2 Cost-Benefit Analysis
Training cost scaling:
- Linear in dataset size
- Quadratic in model size (due to attention)
- Cubic in sequence length

Performance per dollar:
- Larger models more efficient at equivalent performance
- Diminishing returns start around 10B parameters
- Specialized models outperform general models on specific tasks

6.3 Future Predictions
Extrapolating current trends:
- 100B parameter models: ~1.9 cross-entropy loss
- 1T parameter models: ~1.7 cross-entropy loss
- Human-level language understanding: ~10T parameters (estimated)

7. Limitations and Future Work

7.1 Architecture Dependence
Our findings may not generalize to:
- Non-transformer architectures
- Significantly different hyperparameters
- Multi-modal models
- Specialized architectures (e.g., mixture-of-experts)

7.2 Evaluation Limitations
- Cross-entropy loss may not capture all aspects of language understanding
- Downstream tasks may have different scaling properties
- Human evaluation needed for generation quality

7.3 Environmental Considerations
Scaling laws have environmental implications:
- Energy consumption scales super-linearly with model size
- Carbon footprint of large-scale training
- Need for efficient training methods

8. Conclusion

This work establishes predictable scaling laws for neural language models across seven orders of magnitude. Key insights include:

1. Performance scales predictably with model size, data, and compute
2. Optimal resource allocation strategies are well-defined
3. Sample efficiency improves dramatically with scale
4. Emergent abilities appear at predictable scales

These findings provide a foundation for planning future model development and resource allocation. The predictability of scaling suggests that continued improvements are feasible with sufficient compute and data.

Practical recommendations:
- Use scaling laws to plan training runs
- Invest in larger models for better sample efficiency
- Consider environmental costs in scaling decisions
- Develop specialized models for specific domains

Future research directions:
- Understanding the mechanistic basis of scaling laws
- Developing more efficient architectures
- Studying scaling in multi-modal settings
- Investigating scaling for reasoning capabilities

References

[1] Kaplan, J., et al. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.
[2] Hoffmann, J., et al. (2022). Training Compute-Optimal Large Language Models. arXiv preprint arXiv:2203.15556.
[3] Brown, T., et al. (2020). Language models are few-shot learners. Advances in neural information processing systems.
[4] Chowdhery, A., et al. (2022). PaLM: Scaling Language Modeling with Pathways. arXiv preprint arXiv:2204.02311.
[5] Wei, J., et al. (2022). Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.

Keywords: scaling laws, neural language models, transformer, compute allocation, emergent abilities