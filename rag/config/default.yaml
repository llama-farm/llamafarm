# LlamaFarm RAG System - Comprehensive Default Configuration
# This file demonstrates ALL possible configuration options with detailed comments
# Uncomment and modify sections as needed for your use case

version: v1

# =============================================================================
# GLOBAL PROMPTS - System-wide prompts for different use cases
# =============================================================================

prompts:
  - name: "default"
    prompt: "You are an intelligent assistant. Answer questions accurately and helpfully based on the provided context."
    description: "Default general-purpose prompt"
    
  - name: "customer_support"
    prompt: "You are a helpful customer support assistant. Answer questions politely and accurately based on the provided context. If you cannot find the answer in the context, politely explain what information is missing."
    description: "Customer support focused prompt"
    
  - name: "technical_documentation"
    prompt: "You are a technical documentation assistant. Provide clear, accurate, and detailed explanations of technical concepts based on the provided context. Include code examples when relevant."
    description: "Technical documentation and code-focused prompt"
    
  # - name: "medical_assistant"
  #   prompt: "You are a medical information assistant. Provide accurate information based on the provided medical literature. Always include appropriate disclaimers about consulting healthcare professionals."
  #   description: "Medical information assistant (requires compliance setup)"
  
  # - name: "legal_assistant"
  #   prompt: "You are a legal research assistant. Provide information based on legal documents and precedents in the context. Always include disclaimers about seeking professional legal advice."
  #   description: "Legal research assistant (requires compliance setup)"

# =============================================================================
# RAG PIPELINE CONFIGURATION
# =============================================================================

rag:
  description: "Comprehensive RAG configuration demonstrating all available options"
  
  # ---------------------------------------------------------------------------
  # DOCUMENT PARSERS - Extract content from various file formats
  # ---------------------------------------------------------------------------
  
  parsers:
    # CSV Parser - For structured data files
    csv:
      type: "CustomerSupportCSVParser"
      config:
        # Field mapping
        content_fields: ["question", "answer", "solution", "explanation", "description"]
        metadata_fields: ["category", "timestamp", "priority", "tags", "author", "department", "product"]
        id_field: "id"
        
        # Content processing
        combine_content: true
        content_separator: "\n\n"
        max_content_length: 10000
        
        # Advanced CSV options
        # delimiter: ","                    # CSV delimiter
        # quotechar: '"'                   # Quote character
        # encoding: "utf-8"                # File encoding
        # skip_empty_rows: true            # Skip empty rows
        # header_row: 0                    # Header row index
        
        # Metadata extraction options
        chunk_metadata:
          generate_summary: true
          extract_keywords: true
          include_statistics: true
          
          # Extractor configuration (applied automatically)
          extractors:
            # YAKE keyword extractor
            yake:
              max_keywords: 15
              language: "en"                # Language for extraction
              n: 3                          # N-gram size
              dedupLim: 0.9                # Deduplication threshold
              top: 20                       # Top keywords to consider
              
            # Named Entity Recognition
            entities:
              model: "en_core_web_sm"       # spaCy model
              labels: ["PERSON", "ORG", "GPE", "MONEY", "DATE", "PRODUCT"]
              confidence_threshold: 0.8
              
            # Date/time extraction
            datetime:
              formats: ["%Y-%m-%d", "%m/%d/%Y", "%B %d, %Y", "%d/%m/%Y"]
              extract_relative: true        # Extract relative dates
              timezone_aware: true
              
            # Text statistics
            statistics:
              include_readability: true
              include_sentiment: true
              word_frequency_top_n: 10
              include_pos_tags: false       # Part-of-speech tags
      
      file_extensions: [".csv"]
      mime_types: ["text/csv", "application/csv"]
    
    # PDF Parser - For document files
    pdf:
      type: "PDFParser"
      config:
        extract_metadata: true
        extract_images: false             # Set to true for image extraction
        extract_tables: true              # Extract table content
        preserve_layout: false            # Preserve document layout
        
        # Chunking configuration
        chunk_size: 1000
        chunk_overlap: 200
        
        # OCR settings (for scanned PDFs)
        # ocr_enabled: false
        # ocr_language: "eng"
        # ocr_dpi: 300
        
        # Advanced PDF options
        # password: "${PDF_PASSWORD}"      # For password-protected PDFs
        # ignore_errors: true             # Continue on parse errors
        # extract_annotations: false      # Extract PDF annotations
        
        chunk_metadata:
          extractors:
            # RAKE keyword extractor (alternative to YAKE)
            rake:
              max_keywords: 20
              min_keyword_frequency: 2
              max_keyword_length: 3
              
            entities:
              model: "en_core_web_sm"
              labels: ["PERSON", "ORG", "PRODUCT", "EVENT", "LAW", "GPE"]
              
            datetime:
              formats: ["%Y-%m-%d", "%m/%d/%Y", "%B %d, %Y", "%d/%m/%Y"]
              extract_relative: true
              timezone_aware: true
              
            # Document structure extraction
            # structure:
            #   extract_headings: true
            #   extract_lists: true
            #   extract_tables: true
      
      file_extensions: [".pdf"]
      mime_types: ["application/pdf"]
    
    # Text Parser - For plain text files
    text:
      type: "TextParser"
      config:
        chunk_size: 500
        chunk_overlap: 100
        preserve_paragraphs: true         # Try to keep paragraphs intact
        
        # Text cleaning options
        # normalize_whitespace: true       # Normalize whitespace
        # remove_extra_newlines: true     # Remove excessive newlines
        # strip_html: false               # Remove HTML tags
        
        chunk_metadata:
          extractors:
            yake:
              max_keywords: 10
              language: "en"
              
            statistics:
              include_readability: true
              include_sentiment: true
              word_frequency_top_n: 5
      
      file_extensions: [".txt", ".md", ".rst"]
      mime_types: ["text/plain", "text/markdown"]
    
    # JSON Parser - For structured JSON data
    # json:
    #   type: "JSONParser"
    #   config:
    #     content_path: "$.content"        # JSONPath to content
    #     metadata_paths:                  # JSONPath to metadata fields
    #       title: "$.title"
    #       author: "$.author"
    #       category: "$.category"
    #     chunk_size: 800
    #     chunk_overlap: 150
    #   file_extensions: [".json", ".jsonl"]
    #   mime_types: ["application/json"]
    
    # Word Document Parser
    # docx:
    #   type: "DocxParser"
    #   config:
    #     extract_metadata: true
    #     extract_tables: true
    #     extract_images: false
    #     chunk_size: 1000
    #     chunk_overlap: 200
    #   file_extensions: [".docx", ".doc"]
    #   mime_types: ["application/vnd.openxmlformats-officedocument.wordprocessingml.document"]
    
    # HTML Parser
    # html:
    #   type: "HTMLParser"
    #   config:
    #     extract_links: true
    #     remove_scripts: true
    #     remove_styles: true
    #     chunk_size: 800
    #     chunk_overlap: 160
    #   file_extensions: [".html", ".htm"]
    #   mime_types: ["text/html"]
    
    # Email Parser (EML/MSG files)
    # email:
    #   type: "EmailParser"
    #   config:
    #     extract_attachments: false
    #     include_headers: true
    #     chunk_size: 1000
    #     chunk_overlap: 200
    #   file_extensions: [".eml", ".msg"]
    #   mime_types: ["message/rfc822"]

  # ---------------------------------------------------------------------------
  # EMBEDDING MODELS - Convert text to vector representations
  # ---------------------------------------------------------------------------
  
  embedders:
    # Primary embedder - Ollama local embedding
    default:
      type: "OllamaEmbedder"
      config:
        model: "mxbai-embed-large"         # High-quality embedding model
        base_url: "http://localhost:11434"
        batch_size: 32                    # Process 32 texts at once
        timeout: 60                       # Request timeout
        
        # Advanced options
        # dimensions: 1024                 # Force specific dimensions
        # normalize: true                  # Normalize embeddings
        # truncate: true                  # Truncate long texts
    
    # Fast embedder - For development/testing
    fast:
      type: "OllamaEmbedder"
      config:
        model: "nomic-embed-text"          # Faster, smaller model
        base_url: "http://localhost:11434"
        batch_size: 64                    # Larger batch for speed
        timeout: 30
    
    # Cloud embedder - OpenAI (commented out, requires API key)
    # openai:
    #   type: "OpenAIEmbedder"
    #   config:
    #     model: "text-embedding-3-small"   # or "text-embedding-3-large"
    #     api_key: "${OPENAI_API_KEY}"
    #     batch_size: 100                   # OpenAI allows larger batches
    #     timeout: 30
    #     dimensions: 1536                  # Embedding dimensions
    
    # Cohere embedder
    # cohere:
    #   type: "CohereEmbedder"
    #   config:
    #     model: "embed-english-v3.0"
    #     api_key: "${COHERE_API_KEY}"
    #     input_type: "search_document"     # "search_document" or "search_query"
    #     batch_size: 96
    
    # Hugging Face local embedder
    # huggingface:
    #   type: "HuggingFaceEmbedder"
    #   config:
    #     model: "sentence-transformers/all-MiniLM-L6-v2"
    #     device: "cpu"                     # "cpu", "cuda", "mps"
    #     batch_size: 16
    #     cache_dir: "~/.cache/huggingface"
    #     normalize_embeddings: true

  # ---------------------------------------------------------------------------
  # VECTOR STORES - Store and search embeddings
  # ---------------------------------------------------------------------------
  
  vector_stores:
    # Primary vector store - ChromaDB with persistence
    default:
      type: "ChromaStore"
      config:
        collection_name: "llamafarm_documents"
        persist_directory: "./data/vector_store/chroma"
        
        # ChromaDB-specific settings
        # distance_function: "cosine"      # "cosine", "euclidean", "manhattan"
        # hnsw_space: "cosine"            # HNSW index space
        # hnsw_construction_ef: 200       # Construction parameter
        # hnsw_m: 16                      # HNSW connectivity
        
        # Metadata configuration
        metadata_config:
          enable_versioning: true          # Track document versions
          enable_soft_delete: true         # Soft delete for recovery
          hash_algorithm: "sha256"         # Content hashing
          
          # Data retention
          retention_policy:
            default_ttl_days: 365          # Documents expire after 1 year
            max_versions: 10               # Keep max 10 versions
            cleanup_schedule: "weekly"     # Weekly cleanup
          
          # Required metadata fields
          required_metadata:
            - "doc_id"
            - "chunk_id"
            - "filename"
            - "created_at"
            - "updated_at"
          
          # Indexed metadata fields (for fast filtering)
          indexed_metadata:
            - "doc_id"
            - "filename"
            - "created_at"
            - "priority"
            - "category"
            - "author"
    
    # Development vector store - Separate collection for testing
    dev:
      type: "ChromaStore"
      config:
        collection_name: "dev_test_collection"
        persist_directory: "./data/vector_store/chroma_dev"
        metadata_config:
          enable_versioning: false         # Simpler setup for dev
          enable_soft_delete: false
    
    # Production vector store examples (commented out)
    # pinecone:
    #   type: "PineconeStore"
    #   config:
    #     api_key: "${PINECONE_API_KEY}"
    #     environment: "${PINECONE_ENVIRONMENT}"
    #     index_name: "llamafarm-prod"
    #     dimension: 1536
    #     metric: "cosine"
    #     cloud: "aws"
    #     region: "us-east-1"
    
    # qdrant:
    #   type: "QdrantStore"
    #   config:
    #     host: "localhost"
    #     port: 6333
    #     collection_name: "llamafarm_docs"
    #     vector_size: 1024
    #     distance: "Cosine"
    #     # api_key: "${QDRANT_API_KEY}"    # For Qdrant Cloud
    #     # url: "https://xyz.qdrant.tech"  # For Qdrant Cloud
    
    # weaviate:
    #   type: "WeaviateStore"
    #   config:
    #     url: "http://localhost:8080"
    #     class_name: "LlamaFarmDocument"
    #     # api_key: "${WEAVIATE_API_KEY}"  # For Weaviate Cloud
    #     # additional_headers:
    #     #   "X-OpenAI-Api-Key": "${OPENAI_API_KEY}"  # For OpenAI integration
    
    # milvus:
    #   type: "MilvusStore"
    #   config:
    #     host: "localhost"
    #     port: 19530
    #     collection_name: "llamafarm_collection"
    #     dimension: 1024
    #     index_type: "IVF_FLAT"
    #     metric_type: "COSINE"
    #     # user: "${MILVUS_USER}"
    #     # password: "${MILVUS_PASSWORD}"

  # ---------------------------------------------------------------------------
  # RETRIEVAL STRATEGIES - How to find relevant documents
  # ---------------------------------------------------------------------------
  
  retrieval_strategies:
    # Basic similarity search - Simple and fast
    default:
      type: "BasicSimilarityStrategy"
      config:
        distance_metric: "cosine"          # "cosine", "euclidean", "manhattan"
        # include_metadata: true           # Include metadata in results
        # score_threshold: 0.0             # Minimum similarity score
      description: "Basic similarity search using embeddings"
    
    # Metadata filtered search - Filter by document properties
    metadata_filtered:
      type: "MetadataFilteredStrategy"
      config:
        distance_metric: "cosine"
        
        # Default filters applied to all searches
        default_filters:
          priority: ["high", "medium"]     # Only high/medium priority docs
          # category: ["technical", "support"]
          # author: ["expert", "verified"]
          # created_after: "2024-01-01"
        
        # Advanced filtering options
        # filter_operators:                 # Custom filter operators
        #   date_range: "between"
        #   priority_min: "gte"
        #   tags_any: "in"
        # native_filtering: true            # Use database native filtering
      description: "Similarity search with metadata filtering"
    
    # Multi-query search - Generate multiple query variations
    multi_query:
      type: "MultiQueryStrategy"
      config:
        num_queries: 3                     # Generate 3 query variations
        aggregation_method: "reciprocal_rank_fusion"  # How to combine results
        
        # Query generation options
        # query_generator: "llm"           # "llm", "template", "paraphrase"
        # llm_provider: "openai_gpt4o_mini" # LLM for query generation
        # template_variations: [
        #   "What is {query}?",
        #   "Explain {query}",
        #   "How does {query} work?"
        # ]
      description: "Generate multiple query variations and combine results"
    
    # Re-ranked search - Apply additional scoring
    reranked:
      type: "RerankedStrategy"
      config:
        base_strategy: "basic_similarity"   # Base strategy to re-rank
        
        # Re-ranking factors
        rerank_factors:
          recency_weight: 0.2              # Boost recent documents
          popularity_weight: 0.1           # Boost frequently accessed docs
          authority_weight: 0.1            # Boost authoritative sources
          
        # Advanced re-ranking
        # cross_encoder_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
        # max_candidates: 100              # Re-rank top 100 candidates
        # final_top_k: 10                  # Return top 10 after re-ranking
      description: "Re-rank results using multiple factors"
    
    # Hybrid search - Combine multiple strategies
    hybrid_balanced:
      type: "HybridUniversalStrategy"
      config:
        combination_method: "weighted_average"  # "weighted_average", "reciprocal_rank_fusion"
        
        strategies:
          - type: "BasicSimilarityStrategy"
            weight: 0.7                    # 70% semantic similarity
            config:
              distance_metric: "cosine"
              
          - type: "MetadataFilteredStrategy"
            weight: 0.3                    # 30% metadata relevance
            config:
              distance_metric: "cosine"
              default_filters:
                category: ["technical", "support", "documentation"]
                
        # Advanced hybrid options
        # normalization: "min_max"         # Score normalization method
        # diversity_factor: 0.1            # Promote result diversity
      description: "Balanced hybrid search combining similarity and metadata"
    
    # Keyword + Semantic hybrid (requires additional setup)
    # hybrid_keyword_semantic:
    #   type: "HybridKeywordSemanticStrategy"
    #   config:
    #     semantic_weight: 0.6             # 60% semantic, 40% keyword
    #     keyword_weight: 0.4
    #     keyword_index: "bm25"           # "bm25", "tfidf"
    #     fusion_method: "reciprocal_rank_fusion"
    
    # Conversational search (maintains context)
    # conversational:
    #   type: "ConversationalStrategy"
    #   config:
    #     base_strategy: "hybrid_balanced"
    #     context_window: 5                # Remember last 5 exchanges
    #     context_weight: 0.3              # Weight of conversation context
    #     query_rewriting: true            # Rewrite query with context

  # ---------------------------------------------------------------------------
  # CONFIGURATION DEFAULTS
  # ---------------------------------------------------------------------------
  
  defaults:
    parser: "auto"                         # Auto-detect parser based on file type
    embedder: "default"                    # Use default embedder
    vector_store: "default"                # Use default vector store
    retrieval_strategy: "default"          # Use default retrieval strategy
    
    # Default retrieval parameters
    top_k: 5                              # Return top 5 results
    score_threshold: 0.0                  # No minimum score threshold
    include_metadata: true                # Include metadata in results
    include_scores: true                  # Include similarity scores

# =============================================================================
# MODEL INTEGRATION - LLM models for query processing
# =============================================================================

models:
  # Local models (via Ollama)
  - provider: "local"
    model: "llama3.1:8b"
    description: "Primary local model for general queries"
    
  - provider: "local"
    model: "llama3.1:70b"
    description: "Larger local model for complex queries"
    # enabled: false                      # Disable if insufficient resources
  
  # Cloud models (require API keys)
  # - provider: "openai"
  #   model: "gpt-4o-mini"
  #   api_key: "${OPENAI_API_KEY}"
  #   description: "Fast cloud model for general use"
  
  # - provider: "openai"
  #   model: "gpt-4"
  #   api_key: "${OPENAI_API_KEY}"
  #   description: "Advanced cloud model for complex queries"
  
  # - provider: "anthropic"
  #   model: "claude-3-sonnet-20240229"
  #   api_key: "${ANTHROPIC_API_KEY}"
  #   description: "Anthropic Claude for balanced performance"

# =============================================================================
# ADVANCED FEATURES
# =============================================================================

# Document processing pipeline
processing:
  # Batch processing settings
  batch_size: 100                        # Process 100 documents at once
  max_workers: 4                         # Parallel processing workers
  
  # Error handling
  continue_on_error: true                # Continue processing on individual errors
  max_errors_per_batch: 10              # Stop batch if too many errors
  
  # Progress tracking
  show_progress: true                    # Show progress bars
  progress_update_interval: 10          # Update every 10 documents
  
  # Validation
  # validate_documents: true             # Validate documents before processing
  # min_content_length: 10               # Minimum content length
  # max_content_length: 1000000          # Maximum content length

# Search optimization
search:
  # Caching
  enable_search_cache: true             # Cache search results
  cache_ttl_seconds: 300                # Cache for 5 minutes
  
  # Performance
  max_search_time_ms: 5000              # Maximum search time
  enable_parallel_search: true          # Parallel vector search
  
  # Result enhancement
  # enable_snippet_generation: true      # Generate result snippets
  # snippet_length: 200                  # Snippet length in characters
  # highlight_matches: true              # Highlight matching terms

# Monitoring and logging
monitoring:
  # Basic monitoring
  enable_metrics: true                   # Collect performance metrics
  metrics_interval_seconds: 60          # Metrics collection interval
  
  # Logging
  log_level: "INFO"                     # DEBUG, INFO, WARNING, ERROR
  log_queries: true                     # Log search queries
  log_results: false                    # Log search results (verbose)
  
  # Performance monitoring
  track_latency: true                   # Track search latency
  track_accuracy: false                 # Track search accuracy (requires ground truth)
  
  # Usage monitoring
  track_popular_queries: true           # Track frequently asked questions
  track_failed_queries: true            # Track queries with no results
  
  # Alerting (requires external setup)
  # alerts:
  #   webhook_url: "${ALERT_WEBHOOK_URL}"
  #   error_threshold: 10                # Alert after 10 errors
  #   latency_threshold_ms: 10000        # Alert if search takes >10s

# Security and privacy
security:
  # Data protection
  # encrypt_at_rest: false               # Encrypt stored embeddings
  # hash_queries: false                  # Hash queries for privacy
  # anonymize_metadata: false            # Remove PII from metadata
  
  # Access control
  # enable_auth: false                   # Enable authentication
  # auth_provider: "oauth2"              # Authentication provider
  # allowed_users: []                    # List of allowed users
  
  # Content filtering
  # filter_sensitive_content: false     # Filter sensitive content
  # content_filters: ["pii", "profanity"] # Content filters to apply

# =============================================================================
# ENVIRONMENT-SPECIFIC CONFIGURATIONS
# =============================================================================

# Development environment overrides
# environments:
#   development:
#     rag:
#       defaults:
#         top_k: 3                       # Fewer results in dev
#         embedder: "fast"               # Use faster embedder
#         vector_store: "dev"            # Use dev vector store
#     monitoring:
#       log_level: "DEBUG"               # More verbose logging
#       log_results: true                # Log search results
#     processing:
#       batch_size: 10                   # Smaller batches
#       max_workers: 2                   # Fewer workers
#   
#   staging:
#     rag:
#       defaults:
#         retrieval_strategy: "hybrid_balanced"  # Test hybrid in staging
#     monitoring:
#       enable_metrics: true             # Full monitoring in staging
#   
#   production:
#     rag:
#       defaults:
#         retrieval_strategy: "hybrid_balanced"  # Use best strategy
#         top_k: 10                      # More results in production
#     processing:
#       batch_size: 200                  # Larger batches
#       max_workers: 8                   # More workers
#     monitoring:
#       log_level: "WARNING"             # Less verbose logging
#       enable_metrics: true             # Full monitoring
#     security:
#       encrypt_at_rest: true            # Enable encryption
#       enable_auth: true                # Enable authentication

# =============================================================================
# USAGE EXAMPLES IN COMMENTS
# =============================================================================

# Example CLI usage:
#
# 1. Initialize RAG system:
#    uv run python rag/cli.py init --config rag/config/default.yaml
#
# 2. Ingest documents:
#    uv run python rag/cli.py ingest samples/ --config rag/config/default.yaml
#
# 3. Basic search:
#    uv run python rag/cli.py search "How do I reset my password?" --config rag/config/default.yaml
#
# 4. Search with specific strategy:
#    uv run python rag/cli.py search "technical question" --retrieval hybrid_balanced
#
# 5. Search with metadata filtering:
#    uv run python rag/cli.py search "billing question" --retrieval metadata_filtered --filters '{"category": "billing"}'
#
# 6. Use different embedder:
#    uv run python rag/cli.py search "query" --embedder fast
#
# 7. Use different vector store:
#    uv run python rag/cli.py search "query" --vector-store dev
#
# 8. Batch processing:
#    uv run python rag/cli.py ingest large_dataset/ --batch-size 200 --workers 8
#
# 9. Test configuration:
#    uv run python rag/cli.py test --config rag/config/default.yaml