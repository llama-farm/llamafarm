# RAG System - Comprehensive Default Configuration
# This file demonstrates ALL possible configuration options with detailed comments
# You can use individual components OR strategy-based configuration

version: v1

# =============================================================================
# STRATEGY-BASED CONFIGURATION (RECOMMENDED APPROACH)
# =============================================================================
# Use predefined strategies instead of configuring individual components
# Available strategies: simple, customer_support, legal, research, business, technical, production

# strategy: "simple"                        # Uncomment to use strategy-based config
# strategy_overrides:                       # Override specific strategy settings
#   embedder:
#     config:
#       batch_size: 32
#   retrieval_strategy:
#     config:
#       distance_metric: "euclidean"

# =============================================================================
# COMPONENT-BASED CONFIGURATION (ADVANCED USERS)
# =============================================================================
# Configure individual components for complete control
# This approach gives you full flexibility but requires more configuration

# =============================================================================
# GLOBAL PROMPTS - System-wide prompts for different use cases
# =============================================================================

prompts:
  - name: "default"
    prompt: "You are an intelligent assistant. Answer questions accurately and helpfully based on the provided context."
    description: "Default general-purpose prompt"
    
  - name: "customer_support"
    prompt: "You are a helpful customer support assistant. Answer questions politely and accurately based on the provided context. If you cannot find the answer in the context, politely explain what information is missing."
    description: "Customer support focused prompt"
    
  - name: "technical_documentation"
    prompt: "You are a technical documentation assistant. Provide clear, accurate, and detailed explanations of technical concepts based on the provided context. Include code examples when relevant."
    description: "Technical documentation and code-focused prompt"
    
  # - name: "medical_assistant"
  #   prompt: "You are a medical information assistant. Provide accurate information based on the provided medical literature. Always include appropriate disclaimers about consulting healthcare professionals."
  #   description: "Medical information assistant (requires compliance setup)"
  
  # - name: "legal_assistant"
  #   prompt: "You are a legal research assistant. Provide information based on legal documents and precedents in the context. Always include disclaimers about seeking professional legal advice."
  #   description: "Legal research assistant (requires compliance setup)"

# =============================================================================
# RAG PIPELINE CONFIGURATION
# =============================================================================

rag:
  description: "Comprehensive RAG configuration demonstrating all available options"
  
  # ---------------------------------------------------------------------------
  # DOCUMENT PARSERS - Extract content from various file formats
  # ---------------------------------------------------------------------------
  
  parsers:
    # CSV Parser - For structured data files
    csv:
      type: "CustomerSupportCSVParser"
      config:
        # Field mapping
        content_fields: ["question", "answer", "solution", "explanation", "description"]
        metadata_fields: ["category", "timestamp", "priority", "tags", "author", "department", "product"]
        id_field: "id"
        
        # Content processing
        combine_content: true
        content_separator: "\n\n"
        max_content_length: 10000
        
        # Advanced CSV options
        # delimiter: ","                    # CSV delimiter
        # quotechar: '"'                   # Quote character
        # encoding: "utf-8"                # File encoding
        # skip_empty_rows: true            # Skip empty rows
        # header_row: 0                    # Header row index
        
        # Metadata extraction options
        chunk_metadata:
          generate_summary: true
          extract_keywords: true
          include_statistics: true
          
          # Extractor configuration (applied automatically)
          extractors:
            # YAKE keyword extractor
            yake:
              max_keywords: 15
              language: "en"                # Language for extraction
              n: 3                          # N-gram size
              dedupLim: 0.9                # Deduplication threshold
              top: 20                       # Top keywords to consider
              
            # Named Entity Recognition
            entities:
              model: "en_core_web_sm"       # spaCy model
              labels: ["PERSON", "ORG", "GPE", "MONEY", "DATE", "PRODUCT"]
              confidence_threshold: 0.8
              
            # Date/time extraction
            datetime:
              formats: ["%Y-%m-%d", "%m/%d/%Y", "%B %d, %Y", "%d/%m/%Y"]
              extract_relative: true        # Extract relative dates
              timezone_aware: true
              
            # Text statistics
            statistics:
              include_readability: true
              include_sentiment: true
              word_frequency_top_n: 10
              include_pos_tags: false       # Part-of-speech tags
      
      file_extensions: [".csv"]
      mime_types: ["text/csv", "application/csv"]
    
    # PDF Parser - For document files
    pdf:
      type: "PDFParser"
      config:
        extract_metadata: true
        extract_images: false             # Set to true for image extraction
        extract_tables: true              # Extract table content
        preserve_layout: false            # Preserve document layout
        
        # Chunking configuration
        chunk_size: 1000
        chunk_overlap: 200
        
        # OCR settings (for scanned PDFs)
        # ocr_enabled: false
        # ocr_language: "eng"
        # ocr_dpi: 300
        
        # Advanced PDF options
        # password: "${PDF_PASSWORD}"      # For password-protected PDFs
        # ignore_errors: true             # Continue on parse errors
        # extract_annotations: false      # Extract PDF annotations
        
        chunk_metadata:
          extractors:
            # RAKE keyword extractor (alternative to YAKE)
            rake:
              max_keywords: 20
              min_keyword_frequency: 2
              max_keyword_length: 3
              
            entities:
              model: "en_core_web_sm"
              labels: ["PERSON", "ORG", "PRODUCT", "EVENT", "LAW", "GPE"]
              
            datetime:
              formats: ["%Y-%m-%d", "%m/%d/%Y", "%B %d, %Y", "%d/%m/%Y"]
              extract_relative: true
              timezone_aware: true
              
            # Document structure extraction
            # structure:
            #   extract_headings: true
            #   extract_lists: true
            #   extract_tables: true
      
      file_extensions: [".pdf"]
      mime_types: ["application/pdf"]
    
    # Plain Text Parser - For plain text files
    text:
      type: "PlainTextParser"
      config:
        chunk_size: 500
        chunk_overlap: 100
        preserve_line_breaks: true        # Preserve line breaks
        strip_empty_lines: true           # Remove empty lines
        detect_structure: true            # Detect headers, lists, etc.
        encoding: "auto"                  # Auto-detect encoding
        
        chunk_metadata:
          extractors:
            yake:
              max_keywords: 10
              language: "en"
              
            statistics:
              include_readability: true
              include_sentiment: true
              word_frequency_top_n: 5
      
      file_extensions: [".txt", ".log", ".text", ".asc", ".readme"]
      mime_types: ["text/plain"]
    
    # Markdown Parser - For markdown files
    markdown:
      type: "MarkdownParser"
      config:
        extract_frontmatter: true         # Extract YAML frontmatter
        chunk_by_headings: true           # Chunk by heading structure
        preserve_code_blocks: true        # Keep code blocks intact
        extract_links: true               # Extract markdown links
        
        chunk_metadata:
          extractors:
            heading:
              extract_markdown_headings: true
              extract_numbered_headings: false
              generate_toc: true
              
            link:
              extract_urls: true
              extract_emails: false
              extract_mentions: true
      
      file_extensions: [".md", ".markdown", ".rst"]
      mime_types: ["text/markdown", "text/x-markdown"]
    
    # JSON Parser - For structured JSON data
    # json:
    #   type: "JSONParser"
    #   config:
    #     content_path: "$.content"        # JSONPath to content
    #     metadata_paths:                  # JSONPath to metadata fields
    #       title: "$.title"
    #       author: "$.author"
    #       category: "$.category"
    #     chunk_size: 800
    #     chunk_overlap: 150
    #   file_extensions: [".json", ".jsonl"]
    #   mime_types: ["application/json"]
    
    # Word Document Parser - For Microsoft Word documents
    docx:
      type: "DocxParser"
      config:
        extract_tables: true              # Extract table content
        extract_headers_footers: true     # Include headers/footers
        preserve_formatting: false        # Preserve basic formatting
        include_document_properties: true # Include metadata
        
        chunk_metadata:
          extractors:
            table:
              min_columns: 2
              min_rows: 2
              detect_headers: true
              
            heading:
              extract_markdown_headings: false
              extract_underlined_headings: true
              extract_numbered_headings: true
      
      file_extensions: [".docx"]
      mime_types: ["application/vnd.openxmlformats-officedocument.wordprocessingml.document"]
    
    # HTML Parser - For web content and HTML files
    html:
      type: "HTMLParser"
      config:
        extract_links: true               # Extract all links
        extract_images: true              # Extract image references
        extract_tables: true              # Extract HTML tables
        extract_meta: true                # Extract meta tags
        preserve_structure: true          # Preserve heading structure
        remove_scripts: true              # Remove script tags
        remove_styles: true               # Remove style tags
        include_alt_text: true            # Include image alt text
        
        chunk_metadata:
          extractors:
            link:
              extract_urls: true
              extract_emails: true
              extract_file_paths: false
              categorize_domains: true
              
            table:
              table_formats: ["pipe", "grid", "html_marker"]
              min_columns: 2
      
      file_extensions: [".html", ".htm", ".xhtml"]
      mime_types: ["text/html", "application/xhtml+xml"]
    
    # Excel Parser - For spreadsheet files
    excel:
      type: "ExcelParser"
      config:
        parse_all_sheets: true            # Parse all worksheets
        include_sheet_stats: true         # Include sheet statistics
        max_rows: 10000                   # Limit rows per sheet
        skip_empty_rows: true             # Skip empty rows
        convert_to_text: "table"          # "table", "csv", "markdown"
        
        chunk_metadata:
          extractors:
            table:
              min_columns: 1
              min_rows: 1
              detect_headers: true
              
            statistics:
              include_readability: false
              include_sentiment: false
              word_frequency_top_n: 5
      
      file_extensions: [".xlsx", ".xls", ".xlsm"]
      mime_types: ["application/vnd.openxmlformats-officedocument.spreadsheetml.sheet", "application/vnd.ms-excel"]
    
    # Email Parser (EML/MSG files)
    # email:
    #   type: "EmailParser"
    #   config:
    #     extract_attachments: false
    #     include_headers: true
    #     chunk_size: 1000
    #     chunk_overlap: 200
    #   file_extensions: [".eml", ".msg"]
    #   mime_types: ["message/rfc822"]

  # ---------------------------------------------------------------------------
  # EMBEDDING MODELS - Convert text to vector representations
  # ---------------------------------------------------------------------------
  
  embedders:
    # Primary embedder - Ollama local embedding
    default:
      type: "OllamaEmbedder"
      config:
        model: "mxbai-embed-large"         # High-quality embedding model
        base_url: "http://localhost:11434"
        batch_size: 32                    # Process 32 texts at once
        timeout: 60                       # Request timeout
        
        # Advanced options
        # dimensions: 1024                 # Force specific dimensions
        # normalize: true                  # Normalize embeddings
        # truncate: true                  # Truncate long texts
    
    # Fast embedder - For development/testing
    fast:
      type: "OllamaEmbedder"
      config:
        model: "nomic-embed-text"          # Faster, smaller model
        base_url: "http://localhost:11434"
        batch_size: 64                    # Larger batch for speed
        timeout: 30
    
    # Cloud embedder - OpenAI (requires API key)
    openai:
      type: "OpenAIEmbedder"
      config:
        model: "text-embedding-3-small"    # "text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"
        api_key: "${OPENAI_API_KEY}"       # Set environment variable
        batch_size: 100                    # OpenAI allows larger batches
        timeout: 30
        max_retries: 3                     # Retry on failures
        dimensions: 1536                   # Embedding dimensions (for supported models)
    
    # HuggingFace Transformers embedder - Local models
    huggingface:
      type: "HuggingFaceEmbedder"
      config:
        model_name: "sentence-transformers/all-MiniLM-L6-v2"  # Lightweight model
        device: "auto"                     # "auto", "cpu", "cuda", "mps"
        max_length: 512                    # Maximum input length
        batch_size: 32
        normalize_embeddings: true         # Normalize for cosine similarity
        trust_remote_code: false           # Security setting
    
    # Sentence Transformers embedder - Optimized for semantic similarity
    sentence_transformers:
      type: "SentenceTransformerEmbedder"
      config:
        model_name: "all-mpnet-base-v2"    # High-quality model
        device: "auto"                     # "auto", "cpu", "cuda", "mps"
        batch_size: 32
        normalize_embeddings: true         # Normalize embeddings
        show_progress_bar: false           # Show progress during encoding
        convert_to_tensor: false           # Return as lists, not tensors
    
    # Alternative sentence transformer models (commented examples)
    # sentence_transformers_fast:
    #   type: "SentenceTransformerEmbedder" 
    #   config:
    #     model_name: "all-MiniLM-L6-v2"    # Faster, smaller model
    #     device: "auto"
    #     batch_size: 64
    #     normalize_embeddings: true
    
    # sentence_transformers_multilingual:
    #   type: "SentenceTransformerEmbedder"
    #   config:
    #     model_name: "paraphrase-multilingual-mpnet-base-v2"  # Multilingual support
    #     device: "auto"
    #     batch_size: 16                    # Larger model, smaller batch
    #     normalize_embeddings: true
    
    # sentence_transformers_qa:
    #   type: "SentenceTransformerEmbedder"
    #   config:
    #     model_name: "multi-qa-mpnet-base-cos-v1"  # Optimized for Q&A
    #     device: "auto"
    #     batch_size: 32
    #     normalize_embeddings: true

  # ---------------------------------------------------------------------------
  # VECTOR STORES - Store and search embeddings
  # ---------------------------------------------------------------------------
  
  vector_stores:
    # Primary vector store - ChromaDB with persistence
    default:
      type: "ChromaStore"
      config:
        collection_name: "llamafarm_documents"
        persist_directory: "./data/vector_store/chroma"
        
        # ChromaDB-specific settings
        # distance_function: "cosine"      # "cosine", "euclidean", "manhattan"
        # hnsw_space: "cosine"            # HNSW index space
        # hnsw_construction_ef: 200       # Construction parameter
        # hnsw_m: 16                      # HNSW connectivity
        
        # Metadata configuration
        metadata_config:
          enable_versioning: true          # Track document versions
          enable_soft_delete: true         # Soft delete for recovery
          hash_algorithm: "sha256"         # Content hashing
          
          # Data retention
          retention_policy:
            default_ttl_days: 365          # Documents expire after 1 year
            max_versions: 10               # Keep max 10 versions
            cleanup_schedule: "weekly"     # Weekly cleanup
          
          # Required metadata fields
          required_metadata:
            - "doc_id"
            - "chunk_id"
            - "filename"
            - "created_at"
            - "updated_at"
          
          # Indexed metadata fields (for fast filtering)
          indexed_metadata:
            - "doc_id"
            - "filename"
            - "created_at"
            - "priority"
            - "category"
            - "author"
    
    # Development vector store - Separate collection for testing
    dev:
      type: "ChromaStore"
      config:
        collection_name: "dev_test_collection"
        persist_directory: "./data/vector_store/chroma_dev"
        metadata_config:
          enable_versioning: false         # Simpler setup for dev
          enable_soft_delete: false
    
    # FAISS vector store - Fast local similarity search
    faiss:
      type: "FAISSStore"
      config:
        persist_directory: "./data/vector_store/faiss"
        index_type: "IndexFlatIP"          # "IndexFlatIP", "IndexFlatL2", "IndexIVFFlat", "IndexIVFPQ"
        normalize_vectors: true            # Normalize for cosine similarity
        use_gpu: false                     # Use GPU acceleration (if available)
        # nlist: 100                       # For IVF indices
        # nprobe: 10                       # For IVF indices
    
    # Pinecone cloud vector store - Managed service
    pinecone:
      type: "PineconeStore"
      config:
        api_key: "${PINECONE_API_KEY}"     # Required API key
        environment: "${PINECONE_ENVIRONMENT}"  # e.g., "us-east-1-aws"
        index_name: "llamafarm-prod"       # Index name
        dimension: 1536                    # Must match embedder dimension
        metric: "cosine"                   # "cosine", "euclidean", "dotproduct"
        pod_type: "p1.x1"                  # Pod configuration
        replicas: 1                        # Number of replicas
        shards: 1                          # Number of shards
    
    # Qdrant vector store - Open source, high performance
    qdrant:
      type: "QdrantStore"
      config:
        host: "localhost"                  # Qdrant server host
        port: 6333                         # HTTP port
        collection_name: "llamafarm_docs"  # Collection name
        dimension: 1536                    # Vector dimension
        distance: "Cosine"                 # "Cosine", "Dot", "Euclid"
        use_grpc: false                    # Use gRPC for better performance
        # grpc_port: 6334                  # gRPC port
        # url: "https://xyz.qdrant.tech"   # For Qdrant Cloud
        # api_key: "${QDRANT_API_KEY}"     # For Qdrant Cloud
    
    # Alternative vector store configurations (commented examples)
    # weaviate:
    #   type: "WeaviateStore"
    #   config:
    #     url: "http://localhost:8080"
    #     class_name: "LlamaFarmDocument"
    #     # api_key: "${WEAVIATE_API_KEY}"  # For Weaviate Cloud
    #     # additional_headers:
    #     #   "X-OpenAI-Api-Key": "${OPENAI_API_KEY}"  # For OpenAI integration
    
    # milvus:
    #   type: "MilvusStore"
    #   config:
    #     host: "localhost"
    #     port: 19530
    #     collection_name: "llamafarm_collection"
    #     dimension: 1024
    #     index_type: "IVF_FLAT"
    #     metric_type: "COSINE"
    #     # user: "${MILVUS_USER}"
    #     # password: "${MILVUS_PASSWORD}"

  # ---------------------------------------------------------------------------
  # RETRIEVAL STRATEGIES - How to find relevant documents
  # ---------------------------------------------------------------------------
  
  retrieval_strategies:
    # Basic similarity search - Simple and fast
    default:
      type: "BasicSimilarityStrategy"
      config:
        distance_metric: "cosine"          # "cosine", "euclidean", "manhattan"
        # include_metadata: true           # Include metadata in results
        # score_threshold: 0.0             # Minimum similarity score
      description: "Basic similarity search using embeddings"
    
    # Metadata filtered search - Filter by document properties
    metadata_filtered:
      type: "MetadataFilteredStrategy"
      config:
        distance_metric: "cosine"
        
        # Default filters applied to all searches
        default_filters:
          priority: ["high", "medium"]     # Only high/medium priority docs
          # category: ["technical", "support"]
          # author: ["expert", "verified"]
          # created_after: "2024-01-01"
        
        # Advanced filtering options
        # filter_operators:                 # Custom filter operators
        #   date_range: "between"
        #   priority_min: "gte"
        #   tags_any: "in"
        # native_filtering: true            # Use database native filtering
      description: "Similarity search with metadata filtering"
    
    # Multi-query search - Generate multiple query variations
    multi_query:
      type: "MultiQueryStrategy"
      config:
        num_queries: 3                     # Generate 3 query variations
        aggregation_method: "reciprocal_rank_fusion"  # How to combine results
        
        # Query generation options
        # query_generator: "llm"           # "llm", "template", "paraphrase"
        # llm_provider: "openai_gpt4o_mini" # LLM for query generation
        # template_variations: [
        #   "What is {query}?",
        #   "Explain {query}",
        #   "How does {query} work?"
        # ]
      description: "Generate multiple query variations and combine results"
    
    # Re-ranked search - Apply additional scoring
    reranked:
      type: "RerankedStrategy"
      config:
        base_strategy: "basic_similarity"   # Base strategy to re-rank
        
        # Re-ranking factors
        rerank_factors:
          recency_weight: 0.2              # Boost recent documents
          popularity_weight: 0.1           # Boost frequently accessed docs
          authority_weight: 0.1            # Boost authoritative sources
          
        # Advanced re-ranking
        # cross_encoder_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
        # max_candidates: 100              # Re-rank top 100 candidates
        # final_top_k: 10                  # Return top 10 after re-ranking
      description: "Re-rank results using multiple factors"
    
    # Hybrid search - Combine multiple strategies
    hybrid_balanced:
      type: "HybridUniversalStrategy"
      config:
        combination_method: "weighted_average"  # "weighted_average", "reciprocal_rank_fusion"
        
        strategies:
          - type: "BasicSimilarityStrategy"
            weight: 0.7                    # 70% semantic similarity
            config:
              distance_metric: "cosine"
              
          - type: "MetadataFilteredStrategy"
            weight: 0.3                    # 30% metadata relevance
            config:
              distance_metric: "cosine"
              default_filters:
                category: ["technical", "support", "documentation"]
                
        # Advanced hybrid options
        # normalization: "min_max"         # Score normalization method
        # diversity_factor: 0.1            # Promote result diversity
      description: "Balanced hybrid search combining similarity and metadata"
    
    # Keyword + Semantic hybrid (requires additional setup)
    # hybrid_keyword_semantic:
    #   type: "HybridKeywordSemanticStrategy"
    #   config:
    #     semantic_weight: 0.6             # 60% semantic, 40% keyword
    #     keyword_weight: 0.4
    #     keyword_index: "bm25"           # "bm25", "tfidf"
    #     fusion_method: "reciprocal_rank_fusion"
    
    # Conversational search (maintains context)
    # conversational:
    #   type: "ConversationalStrategy"
    #   config:
    #     base_strategy: "hybrid_balanced"
    #     context_window: 5                # Remember last 5 exchanges
    #     context_weight: 0.3              # Weight of conversation context
    #     query_rewriting: true            # Rewrite query with context

  # ---------------------------------------------------------------------------
  # CONFIGURATION DEFAULTS
  # ---------------------------------------------------------------------------
  
  defaults:
    parser: "auto"                         # Auto-detect parser based on file type
    embedder: "default"                    # Use default embedder
    vector_store: "default"                # Use default vector store
    retrieval_strategy: "default"          # Use default retrieval strategy
    
    # Default retrieval parameters
    top_k: 5                              # Return top 5 results
    score_threshold: 0.0                  # No minimum score threshold
    include_metadata: true                # Include metadata in results
    include_scores: true                  # Include similarity scores

# =============================================================================
# MODEL INTEGRATION - LLM models for query processing
# =============================================================================

models:
  # Local models (via Ollama)
  - provider: "local"
    model: "llama3.1:8b"
    description: "Primary local model for general queries"
    
  - provider: "local"
    model: "llama3.1:70b"
    description: "Larger local model for complex queries"
    # enabled: false                      # Disable if insufficient resources
  
  # Cloud models (require API keys)
  # - provider: "openai"
  #   model: "gpt-4o-mini"
  #   api_key: "${OPENAI_API_KEY}"
  #   description: "Fast cloud model for general use"
  
  # - provider: "openai"
  #   model: "gpt-4"
  #   api_key: "${OPENAI_API_KEY}"
  #   description: "Advanced cloud model for complex queries"
  
  # - provider: "anthropic"
  #   model: "claude-3-sonnet-20240229"
  #   api_key: "${ANTHROPIC_API_KEY}"
  #   description: "Anthropic Claude for balanced performance"

# =============================================================================
# ADVANCED FEATURES
# =============================================================================

# Document processing pipeline
processing:
  # Batch processing settings
  batch_size: 100                        # Process 100 documents at once
  max_workers: 4                         # Parallel processing workers
  
  # Error handling
  continue_on_error: true                # Continue processing on individual errors
  max_errors_per_batch: 10              # Stop batch if too many errors
  
  # Progress tracking
  show_progress: true                    # Show progress bars
  progress_update_interval: 10          # Update every 10 documents
  
  # Validation
  # validate_documents: true             # Validate documents before processing
  # min_content_length: 10               # Minimum content length
  # max_content_length: 1000000          # Maximum content length

# Search optimization
search:
  # Caching
  enable_search_cache: true             # Cache search results
  cache_ttl_seconds: 300                # Cache for 5 minutes
  
  # Performance
  max_search_time_ms: 5000              # Maximum search time
  enable_parallel_search: true          # Parallel vector search
  
  # Result enhancement
  # enable_snippet_generation: true      # Generate result snippets
  # snippet_length: 200                  # Snippet length in characters
  # highlight_matches: true              # Highlight matching terms

# Monitoring and logging
monitoring:
  # Basic monitoring
  enable_metrics: true                   # Collect performance metrics
  metrics_interval_seconds: 60          # Metrics collection interval
  
  # Logging
  log_level: "INFO"                     # DEBUG, INFO, WARNING, ERROR
  log_queries: true                     # Log search queries
  log_results: false                    # Log search results (verbose)
  
  # Performance monitoring
  track_latency: true                   # Track search latency
  track_accuracy: false                 # Track search accuracy (requires ground truth)
  
  # Usage monitoring
  track_popular_queries: true           # Track frequently asked questions
  track_failed_queries: true            # Track queries with no results
  
  # Alerting (requires external setup)
  # alerts:
  #   webhook_url: "${ALERT_WEBHOOK_URL}"
  #   error_threshold: 10                # Alert after 10 errors
  #   latency_threshold_ms: 10000        # Alert if search takes >10s

# Security and privacy
security:
  # Data protection
  # encrypt_at_rest: false               # Encrypt stored embeddings
  # hash_queries: false                  # Hash queries for privacy
  # anonymize_metadata: false            # Remove PII from metadata
  
  # Access control
  # enable_auth: false                   # Enable authentication
  # auth_provider: "oauth2"              # Authentication provider
  # allowed_users: []                    # List of allowed users
  
  # Content filtering
  # filter_sensitive_content: false     # Filter sensitive content
  # content_filters: ["pii", "profanity"] # Content filters to apply

# =============================================================================
# ENVIRONMENT-SPECIFIC CONFIGURATIONS
# =============================================================================

# Development environment overrides
# environments:
#   development:
#     rag:
#       defaults:
#         top_k: 3                       # Fewer results in dev
#         embedder: "fast"               # Use faster embedder
#         vector_store: "dev"            # Use dev vector store
#     monitoring:
#       log_level: "DEBUG"               # More verbose logging
#       log_results: true                # Log search results
#     processing:
#       batch_size: 10                   # Smaller batches
#       max_workers: 2                   # Fewer workers
#   
#   staging:
#     rag:
#       defaults:
#         retrieval_strategy: "hybrid_balanced"  # Test hybrid in staging
#     monitoring:
#       enable_metrics: true             # Full monitoring in staging
#   
#   production:
#     rag:
#       defaults:
#         retrieval_strategy: "hybrid_balanced"  # Use best strategy
#         top_k: 10                      # More results in production
#     processing:
#       batch_size: 200                  # Larger batches
#       max_workers: 8                   # More workers
#     monitoring:
#       log_level: "WARNING"             # Less verbose logging
#       enable_metrics: true             # Full monitoring
#     security:
#       encrypt_at_rest: true            # Enable encryption
#       enable_auth: true                # Enable authentication

# =============================================================================
# USAGE EXAMPLES IN COMMENTS
# =============================================================================

# Example CLI usage:
#
# 1. Initialize RAG system:
#    uv run python rag/cli.py init --config rag/config/default.yaml
#
# 2. Ingest documents:
#    uv run python rag/cli.py ingest samples/ --config rag/config/default.yaml
#
# 3. Basic search:
#    uv run python rag/cli.py search "How do I reset my password?" --config rag/config/default.yaml
#
# 4. Search with specific strategy:
#    uv run python rag/cli.py search "technical question" --retrieval hybrid_balanced
#
# 5. Search with metadata filtering:
#    uv run python rag/cli.py search "billing question" --retrieval metadata_filtered --filters '{"category": "billing"}'
#
# 6. Use different embedder:
#    uv run python rag/cli.py search "query" --embedder fast
#
# 7. Use different vector store:
#    uv run python rag/cli.py search "query" --vector-store dev
#
# 8. Batch processing:
#    uv run python rag/cli.py ingest large_dataset/ --batch-size 200 --workers 8
#
# 9. Test configuration:
#    uv run python rag/cli.py test --config rag/config/default.yaml