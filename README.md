# ğŸ¦™ LlamaFarm - Build Powerful AI Locally, Deploy Anywhere

<div align="center">
  <img src="docs/images/rocket-llama.png" alt="Llama Building a Rocket" width="400">
  
  **Empowering developers to build production-ready AI applications with complete local control**
  
  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
  [![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
  [![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg)](CONTRIBUTING.md)
  [![Discord](https://img.shields.io/discord/1234567890?color=7289da&logo=discord&logoColor=white)](https://discord.gg/llamafarm)
  
   [Getting Started](#-getting-started) â€¢ [Features](#-features) â€¢ [Contributing](#-contributing)
</div>

---

## ğŸš€ What is LlamaFarm?

LlamaFarm is a comprehensive, modular framework for building AI Projects that run locally, collaborate, and deploy anywhere. We provide battle-tested components for RAG systems, vector databases, model management, prompt engineering, and fine-tuning - all designed to work seamlessly together or independently.  

### ğŸ¯ Our Mission

We believe AI development should be:
- **Local First**: Full control over your data and models
- **Production Ready**: Built for scale from day one
- **Developer Friendly**: Configuration over code, with sensible defaults
- **Modular**: Use what you need, ignore what you don't
- **Open**: No vendor lock-in, works with any provider

## ğŸ—ï¸ Building in the Open

We're building LlamaFarm in public! Join us:
- ğŸ› [Report bugs](https://github.com/llama-farm/llamafarm/issues)
- ğŸ’¡ [Request features](https://github.com/llama-farm/llamafarm/discussions)
- ğŸ¤ [Contribute code](CONTRIBUTING.md)
- ğŸ’¬ [Join our Discord](https://discord.gg/llamafarm)

---

## âœ¨ Features

### ğŸ” RAG (Retrieval-Augmented Generation)
*Transform any document into AI-accessible knowledge with strategy-based configuration*

- **ğŸ¯ Strategy System**: Choose from 9 pre-configured strategies (simple, legal, customer_support, etc.)
- **ğŸ“„ Universal Document Support**: Parse PDFs, CSVs, Markdown, web pages, and more
- **ğŸ§© Modular Pipeline**: Mix and match parsers, embedders, and vector stores
- **ğŸ¯ Smart Retrieval**: 5+ retrieval strategies including hybrid search and re-ranking
- **ğŸ”Œ Database Agnostic**: Works with ChromaDB, Pinecone, Weaviate, Qdrant, and more
- **ğŸ“Š Local Extractors**: Built-in extractors for metadata enrichment without LLMs
- **âš¡ Production Ready**: Batch processing, error handling, and progress tracking

**Quick Example (Strategy-Based):**
```bash
# Use predefined strategy for quick setup
uv run python rag/cli.py --strategy customer_support ingest support_docs/

# Search with strategy-optimized retrieval
uv run python rag/cli.py --strategy legal search "contract termination clause"

# List available strategies
uv run python rag/cli.py strategies list
```

[Learn more about RAG â†’](rag/README.md)

### ğŸ¤– Model Management
*Run and manage AI models locally or in the cloud with complete fine-tuning support*

- **ğŸŒ Multi-Provider Support**: OpenAI, Anthropic, Google, Cohere, Together, Groq, Ollama, HuggingFace
- **ğŸ“ Fine-Tuning System**: Production-ready fine-tuning with LoRA, QLoRA, and full training
- **ğŸ’° Cost Optimization**: Automatic provider fallbacks and smart routing
- **ğŸ“Š Usage Tracking**: Monitor tokens, costs, and performance
- **ğŸ”„ Load Balancing**: Distribute requests across multiple providers
- **ğŸ›ï¸ Fine Control**: Rate limiting, retry logic, and timeout management
- **ğŸ  Local Models**: Full support for Ollama and HuggingFace models

**Quick Example:**
```yaml
# config/models.yaml
providers:
  primary:
    provider: "openai"
    model: "gpt-4o-mini"
    fallback_to: "local_llama"
  
  local_llama:
    provider: "ollama"
    model: "llama3.2"
    temperature: 0.7
```

**Fine-Tuning Example:**
```bash
# Use strategy-based fine-tuning
uv run python models/cli.py finetune start --strategy mac_m1_lora --dataset data.jsonl

# Custom fine-tuning
uv run python models/cli.py finetune start \
  --dataset my_data.jsonl \
  --base-model llama3.1-8b \
  --method lora
```

[Learn more about Models â†’](models/README.md)

### ğŸ“ Prompt Engineering
*Enterprise-grade prompt management system*

- **ğŸ“š 20+ Templates**: Pre-built templates for common use cases
- **ğŸ§  Smart Selection**: Context-aware template selection
- **ğŸ”„ A/B Testing**: Built-in experimentation framework
- **ğŸ¯ 6 Template Categories**: Basic, Chat, Few-shot, Advanced, Domain-specific, Agentic
- **ğŸ¤ Multi-Agent Support**: Coordinate multiple AI agents
- **ğŸ“Š Evaluation Tools**: 5 evaluation templates for quality assessment

**Quick Example:**
```bash
# List all templates
uv run python prompts/cli.py template list

# Execute with smart selection
uv run python prompts/cli.py execute "Analyze this medical report" --domain medical

# Evaluate responses
uv run python prompts/cli.py evaluate "AI response text" --template llm_judge
```

[Learn more about Prompts â†’](prompts/README.md)

---

## ğŸš€ Getting Started

### Prerequisites
- Python 3.8+
- [uv](https://github.com/astral-sh/uv) (recommended) or pip
- Optional: Ollama for local models
- Optional: Docker for containerized deployment

### Quick Install

```bash
# Clone the repository
git clone https://github.com/llama-farm/llamafarm.git
cd llamafarm

# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Set up RAG system
cd rag
uv sync
uv run python cli.py strategies list

# Set up Models system  
cd ../models
uv sync
uv run python cli.py list

# Set up Prompts system
cd ../prompts
uv sync
uv run python -m prompts.cli template list
```

### ğŸ® Try the Interactive Demos

#### RAG System Demos
```bash
cd rag
# Run all 5 demos sequentially
uv run python demos/master_demo.py

# Or run individual demos:
uv run python demos/demo1_research_papers.py    # Academic paper analysis
uv run python demos/demo2_customer_support.py   # Support ticket processing
uv run python demos/demo3_code_documentation.py # Code documentation analysis
uv run python demos/demo4_news_analysis.py      # News article processing
uv run python demos/demo5_business_reports.py   # Business report analysis
```

#### Models System Demos
```bash
cd models
# Run all demos
uv run python demos/run_all_demos.py

# Or run individual demos:
uv run python demos/demo1_cloud_with_fallback.py  # Cloud with local fallback
uv run python demos/demo2_multi_model_cloud.py    # Multiple cloud models
uv run python demos/demo3_quick_training.py       # Quick fine-tuning demo
uv run python demos/demo4_complex_training.py     # Advanced training pipeline
```

#### Prompts System Demos
```bash
cd prompts
# Run all demos
uv run python demos/run_all_demos.py

# Or run individual demos:
uv run python demos/demo1_simple_qa.py          # Basic Q&A templates
uv run python demos/demo2_customer_support.py   # Support response generation
uv run python demos/demo3_code_assistant.py     # Code generation templates
uv run python demos/demo4_rag_research.py       # RAG integration demo
uv run python demos/demo5_advanced_reasoning.py # Chain-of-thought reasoning
```

---

## ğŸ“š Documentation

### Component Guides
- ğŸ“– [RAG System Guide](rag/README.md) - Document processing and retrieval
- ğŸ¤– [Models Guide](models/README.md) - Model management, providers, and fine-tuning
- ğŸ“ [Prompts Guide](prompts/README.md) - Prompt engineering and templates

### Developer Guides
- ğŸ—ï¸ [RAG Architecture](rag/STRUCTURE.md) - RAG system internals
- ğŸ”§ [Models Architecture](models/STRUCTURE.md) - Models system internals
- ğŸ“‹ [Prompts Architecture](prompts/STRUCTURE.md) - Prompts system internals

### Strategy Documentation
- ğŸ¯ [RAG Strategies](rag/STRATEGY_SYSTEM_SUMMARY.md) - Complete RAG strategy guide
- ğŸ“ [Fine-Tuning Strategies](models/strategies/README.md) - Training strategy guide

### API Reference
- ğŸ”Œ [RAG API](rag/api.py) - RESTful API for RAG operations
- ğŸ¤– [Models CLI](models/cli.py) - Complete CLI documentation
- ğŸ“ [Prompts CLI](prompts/prompts/core/cli/strategy_cli.py) - Prompts CLI reference

---

## ğŸ› ï¸ Architecture

LlamaFarm follows a modular, strategy-driven architecture:

```
llamafarm/
â”œâ”€â”€ rag/                          # Document processing and retrieval
â”‚   â”œâ”€â”€ components/               # Component-based architecture
â”‚   â”‚   â”œâ”€â”€ embedders/           # Text embedding models
â”‚   â”‚   â”œâ”€â”€ extractors/          # Content extractors
â”‚   â”‚   â”œâ”€â”€ parsers/             # Document parsers
â”‚   â”‚   â”œâ”€â”€ retrievers/          # Retrieval strategies
â”‚   â”‚   â””â”€â”€ stores/              # Vector databases
â”‚   â”œâ”€â”€ strategies/              # Pre-configured strategies
â”‚   â”œâ”€â”€ demos/                   # Interactive demonstrations
â”‚   â””â”€â”€ schema.yaml              # Component schemas
â”‚
â”œâ”€â”€ models/                      # Model management & fine-tuning
â”‚   â”œâ”€â”€ components/              # Component-based architecture
â”‚   â”‚   â”œâ”€â”€ cloud_apis/          # Cloud provider integrations
â”‚   â”‚   â”œâ”€â”€ fine_tuners/         # Fine-tuning implementations
â”‚   â”‚   â”œâ”€â”€ model_apps/          # Local model runners
â”‚   â”‚   â””â”€â”€ model_repositories/  # Model registry integrations
â”‚   â”œâ”€â”€ strategies/              # Training strategies
â”‚   â”œâ”€â”€ demos/                   # Interactive demonstrations
â”‚   â””â”€â”€ cli.py                   # Unified CLI interface
â”‚
â”œâ”€â”€ prompts/                     # Prompt engineering
â”‚   â”œâ”€â”€ templates/               # Template library
â”‚   â”œâ”€â”€ strategies/              # Template strategies
â”‚   â”œâ”€â”€ demos/                   # Interactive demonstrations
â”‚   â””â”€â”€ core/                    # Core functionality
â”‚
â””â”€â”€ config/                      # Shared configuration
    â””â”€â”€ *.yaml                   # YAML-first configuration
```

### Key Design Principles
- **Strategy System**: Pre-configured setups for specific use cases
- **Component Architecture**: Modular, extensible components
- **Factory Pattern**: Dynamic component creation and registration
- **YAML Configuration**: Human-readable, well-documented configs
- **Local-First**: Prioritize local execution with cloud options

---

## ğŸŒŸ Why LlamaFarm?

### For Developers
- **ğŸ  Local First**: Run everything on your machine, no API keys required
- **ğŸ”§ Hackable**: Clean, modular code that's easy to understand and extend
- **ğŸ“¦ Batteries Included**: Pre-built components for common use cases
- **ğŸ¯ Production Ready**: Built with scale, monitoring, and reliability in mind

### For Teams
- **ğŸ’° Cost Control**: Optimize spending with multi-provider support
- **ğŸ”’ Data Privacy**: Keep sensitive data on-premise
- **ğŸš€ Fast Iteration**: Hot-reload configs, no redeploys needed
- **ğŸ“Š Full Visibility**: Built-in monitoring and analytics

### For Enterprises
- **ğŸ¢ Multi-Tenant**: Isolated environments for different teams
- **ğŸ” Security First**: SOC2-ready with audit logging
- **ğŸ“ˆ Scalable**: From laptop to cluster without code changes
- **ğŸ¤ Vendor Neutral**: No lock-in, works with any provider

---

## ğŸ¤ Contributing

<div align="center">
  <img src="docs/images/iron-workers-llama.png" alt="Iron Worker Llamas" width="400">
  
  **Join our herd of contributors building the future of local AI!**
</div>

We love contributions! Whether you're fixing bugs, adding features, or improving documentation, we'd love to have you aboard.

### How to Contribute
1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/amazing-feature`)
3. ğŸ’» Make your changes
4. âœ… Run tests (`uv run pytest`)
5. ğŸ“ Commit your changes (`git commit -m 'Add amazing feature'`)
6. ğŸš€ Push to the branch (`git push origin feature/amazing-feature`)
7. ğŸ‰ Open a Pull Request

See our [Contributing Guide](CONTRIBUTING.md) for more details.

### Good First Issues
- ğŸ·ï¸ [good-first-issue](https://github.com/llama-farm/llamafarm/labels/good-first-issue)
- ğŸ“š [documentation](https://github.com/llama-farm/llamafarm/labels/documentation)
- ğŸ§ª [testing](https://github.com/llama-farm/llamafarm/labels/testing)

---

## ğŸ™ Open Source Credits

LlamaFarm is built on the shoulders of giants. Special thanks to:

### Core Dependencies
- ğŸ¦œ [LangChain](https://github.com/hwchase17/langchain) - LLM orchestration
- ğŸ¤— [Transformers](https://github.com/huggingface/transformers) - Model library
- ğŸ¯ [ChromaDB](https://github.com/chroma-core/chroma) - Vector database
- ğŸ“Š [Pandas](https://github.com/pandas-dev/pandas) - Data manipulation
- ğŸ”¥ [PyTorch](https://github.com/pytorch/pytorch) - Deep learning

### Development Tools
- ğŸš€ [uv](https://github.com/astral-sh/uv) - Fast Python package management
- ğŸ§ª [pytest](https://github.com/pytest-dev/pytest) - Testing framework
- ğŸ“ [Pydantic](https://github.com/pydantic/pydantic) - Data validation
- ğŸ¨ [Rich](https://github.com/Textualize/rich) - Beautiful terminal output

See [CREDITS.md](docs/CREDITS.md) for a complete list.

---

## ğŸ“„ License

LlamaFarm is MIT licensed. See [LICENSE](LICENSE) for details.

---

## ğŸ’¬ Community

Join the LlamaFarm community:

- ğŸ’¬ [Discord Server](https://discord.gg/llamafarm) - Chat with the community
- ğŸ› [GitHub Issues](https://github.com/llama-farm/llamafarm/issues) - Report bugs
- ğŸ’¡ [GitHub Discussions](https://github.com/llama-farm/llamafarm/discussions) - Share ideas

---

<div align="center">
  <p>
    <b>Ready to farm some AI? ğŸ¦™ğŸšœ</b>
  </p>
  <p>
    <a href="https://github.com/llama-farm/llamafarm">â­ Star us on GitHub</a> â€¢ 
    <a href="https://discord.gg/llamafarm">ğŸ’¬ Join Discord</a> â€¢ 
    <a href="https://twitter.com/llamafarm">ğŸ¦ Follow on Twitter</a>
  </p>
</div>