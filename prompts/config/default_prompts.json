{
  "name": "LlamaFarm Default Prompts Configuration",
  "version": "1.0.0",
  "description": "Default configuration with comprehensive prompt templates and strategies",
  "enabled": true,
  "default_strategy": "context_aware_strategy",
  "global_prompts": [
    {
      "global_id": "system_context",
      "name": "System Context",
      "description": "Provides general system context for all prompts",
      "system_prompt": "You are a helpful AI assistant integrated with LlamaFarm, a comprehensive document processing and RAG system. You have access to retrieved documents and should provide accurate, helpful responses based on the available information.",
      "prefix_prompt": null,
      "suffix_prompt": null,
      "applies_to": [
        "*"
      ],
      "excludes": [],
      "conditions": {},
      "priority": 10,
      "enabled": true,
      "tags": [],
      "created_by": null
    },
    {
      "global_id": "quality_guidelines",
      "name": "Quality Guidelines",
      "description": "Ensures high-quality responses",
      "system_prompt": null,
      "prefix_prompt": "Please provide a clear, accurate, and helpful response. If you're uncertain about something, acknowledge the uncertainty.",
      "suffix_prompt": null,
      "applies_to": [
        "*"
      ],
      "excludes": [
        "debug_*",
        "test_*"
      ],
      "conditions": {},
      "priority": 20,
      "enabled": true,
      "tags": [],
      "created_by": null
    },
    {
      "global_id": "domain_medical",
      "name": "Medical Domain Context",
      "description": "Medical domain expertise context",
      "system_prompt": "You are analyzing medical documents and should be precise, cite sources when available, and note any limitations in the provided information. Always recommend consulting healthcare professionals for medical decisions.",
      "prefix_prompt": null,
      "suffix_prompt": null,
      "applies_to": [
        "medical_*"
      ],
      "excludes": [],
      "conditions": {
        "domain": "medical"
      },
      "priority": 30,
      "enabled": true,
      "tags": [],
      "created_by": null
    }
  ],
  "templates": {
    "summarization": {
      "template_id": "summarization",
      "name": "Document Summarization",
      "type": "basic",
      "template": "Please summarize the following document(s):\n\n{{ context | format_documents }}\n\nProvide a concise summary highlighting the key points:\n\nSummary:",
      "input_variables": [
        "context"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "summarization",
        "complexity": "low",
        "domain": "general",
        "tags": [
          "summary",
          "document",
          "overview"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.350757",
        "updated_at": "2025-07-31 10:06:57.350762",
        "version": "1.0.0",
        "description": "General document summarization template",
        "examples": [
          {
            "description": "Summarizing a research article",
            "input": {
              "context": [
                {
                  "title": "AI Research Paper",
                  "content": "This paper presents a novel approach to neural network optimization using gradient descent variations. The proposed method shows 15% improvement in training speed and 8% better accuracy on benchmark datasets."
                }
              ]
            },
            "expected_output": "The research paper introduces a new neural network optimization method that improves training speed by 15% and accuracy by 8% compared to standard approaches."
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "context": {
          "type": "list",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "qa_detailed": {
      "template_id": "qa_detailed",
      "name": "Detailed Question Answering",
      "type": "basic",
      "template": "Context Information:\n{{ context | format_documents(max_length=2000) }}\n\nQuestion: {{ query }}\n\nPlease provide a comprehensive answer based on the context above. If the context doesn't contain sufficient information to answer the question completely, please indicate what information is missing.\n\nAnswer:",
      "input_variables": [
        "context",
        "query"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "detailed_qa",
        "complexity": "medium",
        "domain": "general",
        "tags": [
          "qa",
          "detailed",
          "comprehensive"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.350929",
        "updated_at": "2025-07-31 10:06:57.350930",
        "version": "1.0.0",
        "description": "Detailed question answering with comprehensive responses",
        "examples": [
          {
            "description": "Complex question requiring detailed analysis",
            "input": {
              "query": "How does machine learning impact modern healthcare?",
              "context": [
                {
                  "title": "ML in Healthcare",
                  "content": "Machine learning applications in healthcare include diagnostic imaging, drug discovery, personalized treatment plans, and predictive analytics for patient outcomes."
                }
              ]
            },
            "expected_output": "Based on the context, machine learning significantly impacts modern healthcare through multiple applications..."
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 1,
          "max_length": 1000,
          "required": true
        },
        "context": {
          "type": "list",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "qa_basic": {
      "template_id": "qa_basic",
      "name": "Basic Question Answering",
      "type": "basic",
      "template": "Based on the following context:\n\n{{ context | format_documents }}\n\nQuestion: {{ query }}\n\nAnswer:",
      "input_variables": [
        "context",
        "query"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "general_qa",
        "complexity": "low",
        "domain": "general",
        "tags": [
          "qa",
          "basic",
          "general"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.351053",
        "updated_at": "2025-07-31 10:06:57.351053",
        "version": "1.0.0",
        "description": "Simple question answering template for general queries",
        "examples": [
          {
            "description": "Basic factual question",
            "input": {
              "query": "What is machine learning?",
              "context": [
                {
                  "title": "ML Introduction",
                  "content": "Machine learning is a subset of AI that enables computers to learn from data."
                }
              ]
            },
            "expected_output": "Based on the context, machine learning is a subset of AI that enables computers to learn from data."
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 1,
          "max_length": 1000,
          "required": true
        },
        "context": {
          "type": "list",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "chat_assistant": {
      "template_id": "chat_assistant",
      "name": "Chat Assistant",
      "type": "chat",
      "template": "{% if context %}Available Information:\n{{ context | format_documents(max_length=1000) }}\n\n{% endif %}User: {{ query }}\n\nAssistant: I'll help you with that. {% if context %}Based on the information available, {% endif %}",
      "input_variables": [
        "query"
      ],
      "optional_variables": [
        "context"
      ],
      "metadata": {
        "use_case": "conversational",
        "complexity": "low",
        "domain": "general",
        "tags": [
          "chat",
          "assistant",
          "conversation"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.351468",
        "updated_at": "2025-07-31 10:06:57.351471",
        "version": "1.0.0",
        "description": "Conversational assistant template",
        "examples": [
          {
            "description": "Basic chat interaction",
            "input": {
              "query": "Hello! Can you help me understand what machine learning is?",
              "context": [
                {
                  "title": "ML Basics",
                  "content": "Machine learning is a method of data analysis that automates analytical model building."
                }
              ]
            },
            "expected_output": "User: Hello! Can you help me understand what machine learning is?\n\nAssistant: I'll help you with that. Based on the information available, machine learning is a method of data analysis that automates analytical model building..."
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 1,
          "max_length": 1000,
          "required": true
        },
        "context": {
          "type": "list",
          "required": false
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "few_shot_classification": {
      "template_id": "few_shot_classification",
      "name": "Few-Shot Classification",
      "type": "few_shot",
      "template": "Classify the following text based on these examples:\n\nExamples:\n{{ examples }}\n\nText to classify: {{ query }}\n\nCategory:",
      "input_variables": [
        "examples",
        "query"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "classification",
        "complexity": "medium",
        "domain": "general",
        "tags": [
          "classification",
          "few-shot",
          "examples",
          "categorization"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.351856",
        "updated_at": "2025-07-31 10:06:57.351857",
        "version": "1.0.0",
        "description": "Few-shot learning classification template",
        "examples": [
          {
            "description": "Email classification",
            "input": {
              "query": "Meeting scheduled for next Tuesday at 2 PM",
              "examples": "\u2022 'Please review the attached document' \u2192 Business\n\u2022 'Happy birthday! Hope you have a great day!' \u2192 Personal\n\u2022 'Your order has been shipped' \u2192 Notification"
            },
            "expected_output": "Business"
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 1,
          "max_length": 2000,
          "required": true
        },
        "examples": {
          "type": "str",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "ab_testing": {
      "template_id": "ab_testing",
      "name": "A/B Testing Comparison",
      "type": "advanced",
      "template": "# A/B Testing Analysis\n\n## Test Overview\n{{ test_description }}\n\n## Variant A Details\n**Name**: {{ variant_a_name }}\n**Configuration**: {{ variant_a_config }}\n**Sample Responses**:\n{{ variant_a_responses }}\n\n## Variant B Details\n**Name**: {{ variant_b_name }}\n**Configuration**: {{ variant_b_config }}\n**Sample Responses**:\n{{ variant_b_responses }}\n\n## Evaluation Metrics\n{{ evaluation_metrics }}\n\n\ud83e\uddea **A/B TESTING COMPARATIVE ANALYSIS**\n\n\ud83d\udcca **Performance Comparison**\n\n**Response Quality Metrics**\n\u2022 **Accuracy**: Which variant provides more accurate information?\n\u2022 **Completeness**: Which variant gives more comprehensive answers?\n\u2022 **Relevance**: Which variant better addresses the user queries?\n\u2022 **Consistency**: Which variant shows more consistent performance?\n\n**User Experience Metrics**\n\u2022 **Clarity**: Which variant communicates more clearly?\n\u2022 **Usefulness**: Which variant provides more actionable insights?\n\u2022 **Engagement**: Which variant better engages users?\n\u2022 **Satisfaction**: Which variant would users prefer?\n\n**Technical Performance**\n\u2022 **Response Time**: How do processing speeds compare?\n\u2022 **Resource Usage**: What are the computational requirements?\n\u2022 **Reliability**: Which variant fails less often?\n\u2022 **Scalability**: Which variant handles load better?\n\n\u2696\ufe0f **Side-by-Side Evaluation**\n\n**Variant A Strengths:**\n\u2022 [List specific advantages of Variant A]\n\u2022 [Examples of superior performance]\n\u2022 [Unique capabilities or features]\n\n**Variant A Weaknesses:**\n\u2022 [List specific limitations of Variant A]\n\u2022 [Examples of inferior performance]\n\u2022 [Missing features or capabilities]\n\n**Variant B Strengths:**\n\u2022 [List specific advantages of Variant B]\n\u2022 [Examples of superior performance]\n\u2022 [Unique capabilities or features]\n\n**Variant B Weaknesses:**\n\u2022 [List specific limitations of Variant B]\n\u2022 [Examples of inferior performance]\n\u2022 [Missing features or capabilities]\n\n\ud83d\udcc8 **Quantitative Comparison**\n\n**Scoring Matrix** (1-10 scale):\n\n| Metric | Variant A | Variant B | Winner |\n|--------|-----------|-----------|--------|\n| Accuracy | [Score] | [Score] | [A/B/Tie] |\n| Completeness | [Score] | [Score] | [A/B/Tie] |\n| Relevance | [Score] | [Score] | [A/B/Tie] |\n| Clarity | [Score] | [Score] | [A/B/Tie] |\n| Usefulness | [Score] | [Score] | [A/B/Tie] |\n| Consistency | [Score] | [Score] | [A/B/Tie] |\n| Engagement | [Score] | [Score] | [A/B/Tie] |\n| Performance | [Score] | [Score] | [A/B/Tie] |\n\n**Total Scores:**\n\u2022 Variant A: [Total]/80 ([Percentage]%)\n\u2022 Variant B: [Total]/80 ([Percentage]%)\n\n\ud83c\udfc6 **Test Results & Recommendation**\n\n**Statistical Significance:**\n\u2022 Sample size: [Number of test cases]\n\u2022 Confidence level: [Percentage]%\n\u2022 Effect size: [Small/Medium/Large]\n\n**Winner**: [Variant A / Variant B / No Clear Winner]\n**Confidence**: [High/Medium/Low]\n\n**Key Findings:**\n\u2022 [Most important discovery from the test]\n\u2022 [Surprising or unexpected results]\n\u2022 [Patterns observed across test cases]\n\n**Contextual Considerations:**\n\u2022 **Use Case Fit**: Which variant is better for specific scenarios?\n\u2022 **User Segments**: Do different user types prefer different variants?\n\u2022 **Trade-offs**: What are the key trade-offs between variants?\n\u2022 **Edge Cases**: How do variants handle unusual or challenging inputs?\n\n\ud83d\udca1 **Strategic Recommendations**\n\n**Immediate Actions:**\n\u2022 [What should be implemented right away]\n\n**Long-term Strategy:**\n\u2022 [How to evolve based on these findings]\n\n**Future Testing:**\n\u2022 [What should be tested next]\n\u2022 [Additional variants to consider]\n\u2022 [Metrics to add or refine]\n\n**Implementation Plan:**\n\u2022 [Steps to roll out the winning variant]\n\u2022 [Monitoring and measurement plan]\n\u2022 [Rollback strategy if needed]\n\n\ud83d\udd0d **Additional Insights**\n\n**Unexpected Behaviors:**\n\u2022 [Any surprising findings or edge cases]\n\n**User Feedback Themes:**\n\u2022 [Common patterns in user responses]\n\n**Technical Observations:**\n\u2022 [Performance or implementation insights]\n\n**Bias Considerations:**\n\u2022 [Potential biases in the test setup or evaluation]\n\n**Next Steps:**\n\u2022 [Specific actions to take based on results]",
      "input_variables": [
        "test_description",
        "variant_a_name",
        "variant_a_config",
        "variant_a_responses",
        "variant_b_name",
        "variant_b_config",
        "variant_b_responses",
        "evaluation_metrics"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "ab_testing",
        "complexity": "high",
        "domain": "evaluation",
        "tags": [
          "ab_testing",
          "comparison",
          "evaluation",
          "optimization",
          "statistics"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.352329",
        "updated_at": "2025-07-31 10:06:57.352330",
        "version": "1.0.0",
        "description": "Comprehensive A/B testing comparison template for evaluating different AI configurations",
        "examples": [
          {
            "description": "Comparing two prompt strategies",
            "input": {
              "test_description": "Testing chain-of-thought vs direct answer prompts for math problems",
              "variant_a_name": "Chain of Thought",
              "variant_a_config": "Prompts include step-by-step reasoning instructions",
              "variant_a_responses": "Response 1: Shows detailed work, correct answer\nResponse 2: Clear steps, correct answer\nResponse 3: Verbose but accurate",
              "variant_b_name": "Direct Answer",
              "variant_b_config": "Prompts ask for immediate answers without showing work",
              "variant_b_responses": "Response 1: Quick answer, correct\nResponse 2: Fast response, minor error\nResponse 3: Concise, correct",
              "evaluation_metrics": "Accuracy, response time, user preference for explanation detail"
            },
            "expected_output": "**Winner**: Chain of Thought\n**Key Finding**: Higher accuracy (95% vs 87%) despite longer response time\n**Recommendation**: Use Chain of Thought for complex problems, Direct Answer for simple queries"
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "test_description": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        },
        "variant_a_name": {
          "type": "str",
          "min_length": 1,
          "max_length": 100,
          "required": true
        },
        "variant_a_config": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        },
        "variant_a_responses": {
          "type": "str",
          "min_length": 20,
          "max_length": 5000,
          "required": true
        },
        "variant_b_name": {
          "type": "str",
          "min_length": 1,
          "max_length": 100,
          "required": true
        },
        "variant_b_config": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        },
        "variant_b_responses": {
          "type": "str",
          "min_length": 20,
          "max_length": 5000,
          "required": true
        },
        "evaluation_metrics": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "chain_of_thought": {
      "template_id": "chain_of_thought",
      "name": "Chain of Thought Reasoning",
      "type": "advanced",
      "template": "Context: {{ context | format_documents }}\n\nQuestion: {{ query }}\n\nLet me work through this step by step:\n\n1\ufe0f\u20e3 **Understanding the Question**\nWhat exactly is being asked and what kind of analysis is needed?\n\n2\ufe0f\u20e3 **Key Information from Context**  \nWhat relevant facts and data can I extract from the provided context?\n\n3\ufe0f\u20e3 **Step-by-Step Reasoning**\nHow do these pieces of information connect to answer the question?\n\n4\ufe0f\u20e3 **Final Conclusion**\nBased on my analysis, here is my reasoned answer:\n\n**Answer:**",
      "input_variables": [
        "context",
        "query"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "analytical_reasoning",
        "complexity": "high",
        "domain": "general",
        "tags": [
          "reasoning",
          "analysis",
          "step-by-step",
          "complex"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.352449",
        "updated_at": "2025-07-31 10:06:57.352449",
        "version": "1.0.0",
        "description": "Chain of thought reasoning for complex questions",
        "examples": [
          {
            "description": "Complex analytical question",
            "input": {
              "query": "Why might renewable energy adoption vary between countries?",
              "context": [
                {
                  "title": "Energy Policy",
                  "content": "Countries have different energy policies, economic conditions, natural resources, and technological capabilities that influence renewable energy adoption."
                }
              ]
            },
            "expected_output": "Let me work through this step by step: 1. Understanding the question: We need to analyze factors affecting renewable energy adoption across different countries..."
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        },
        "context": {
          "type": "list",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "comparative_analysis": {
      "template_id": "comparative_analysis",
      "name": "Comparative Analysis",
      "type": "advanced",
      "template": "Documents to compare:\n{{ context | format_documents }}\n\nAnalysis request: {{ query }}\n\n\ud83d\udcca **COMPARATIVE ANALYSIS**\n\n\ud83e\udd1d **Similarities**\n\u2022 What common themes, approaches, or characteristics do these items share?\n\u2022 Where do they align or agree?\n\n\u26a1 **Key Differences**  \n\u2022 What are the main contrasts and distinctions?\n\u2022 How do their approaches, outcomes, or features differ?\n\n\ud83d\udd0d **Analysis & Insights**\n\u2022 What patterns emerge from this comparison?\n\u2022 What are the implications of these similarities and differences?\n\n**Conclusion:**",
      "input_variables": [
        "context",
        "query"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "comparative_analysis",
        "complexity": "high",
        "domain": "general",
        "tags": [
          "comparison",
          "analysis",
          "contrast",
          "insights"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.352576",
        "updated_at": "2025-07-31 10:06:57.352576",
        "version": "1.0.0",
        "description": "Compare and contrast multiple documents or concepts",
        "examples": [
          {
            "description": "Technology comparison",
            "input": {
              "query": "Compare the advantages and disadvantages of cloud vs on-premises infrastructure",
              "context": [
                {
                  "title": "Cloud Computing",
                  "content": "Cloud computing offers scalability, cost-effectiveness, and automatic updates, but may have security concerns and dependency on internet connectivity."
                },
                {
                  "title": "On-Premises Infrastructure",
                  "content": "On-premises infrastructure provides complete control and security but requires significant upfront investment and ongoing maintenance."
                }
              ]
            },
            "expected_output": "## Similarities:\nBoth cloud and on-premises solutions provide computing infrastructure for business needs...\n\n## Differences:\nCloud offers scalability and lower upfront costs, while on-premises provides more control..."
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        },
        "context": {
          "type": "list",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "prompt_evaluation": {
      "template_id": "prompt_evaluation",
      "name": "Prompt Effectiveness Evaluation",
      "type": "domain_specific",
      "template": "# Prompt Effectiveness Evaluation\n\n## Prompt Being Evaluated\n```\n{{ prompt_template }}\n```\n\n## Test Cases and Responses\n{{ test_cases }}\n\n## Evaluation Context\n{{ evaluation_context }}\n\n\ud83d\udcdd **PROMPT EVALUATION FRAMEWORK**\n\n\ud83c\udfaf **Clarity & Instructions**\n\u2022 **Instruction Clarity**: Are the instructions clear and unambiguous?\n\u2022 **Task Definition**: Is the desired task/output clearly defined?\n\u2022 **Format Specification**: Are output format requirements clearly stated?\n\u2022 **Role Definition**: Is the AI's role or persona clearly established?\n\n\ud83e\udde0 **Cognitive Load & Complexity**\n\u2022 **Cognitive Demand**: How much reasoning does the prompt require?\n\u2022 **Context Length**: Is the prompt appropriately concise yet comprehensive?\n\u2022 **Step Breakdown**: Are complex tasks broken into manageable steps?\n\u2022 **Mental Model**: Does the prompt help establish the right mental framework?\n\n\u26a1 **Performance Effectiveness**\n\u2022 **Consistency**: Does the prompt produce consistent outputs across similar inputs?\n\u2022 **Accuracy**: How accurate are the responses generated by this prompt?\n\u2022 **Completeness**: Do responses fully address the intended requirements?\n\u2022 **Reliability**: How reliably does the prompt work across different scenarios?\n\n\ud83c\udfa8 **Design Quality**\n\u2022 **Structure**: Is the prompt well-organized and logically structured?\n\u2022 **Examples**: Are examples provided where helpful?\n\u2022 **Constraints**: Are appropriate constraints and boundaries set?\n\u2022 **Flexibility**: Can the prompt handle variations in input?\n\n\ud83d\udcca **Evaluation Scores**\n\n**Clarity Metrics:**\n\u2022 Instruction Clarity: [Score 1-5] - [Justification]\n\u2022 Task Definition: [Score 1-5] - [Justification]\n\u2022 Format Specification: [Score 1-5] - [Justification]\n\u2022 Role Definition: [Score 1-5] - [Justification]\n\n**Effectiveness Metrics:**\n\u2022 Consistency: [Score 1-5] - [Justification]\n\u2022 Accuracy: [Score 1-5] - [Justification]\n\u2022 Completeness: [Score 1-5] - [Justification]\n\u2022 Reliability: [Score 1-5] - [Justification]\n\n**Design Metrics:**\n\u2022 Structure: [Score 1-5] - [Justification]\n\u2022 Examples Usage: [Score 1-5] - [Justification]\n\u2022 Constraint Setting: [Score 1-5] - [Justification]\n\u2022 Flexibility: [Score 1-5] - [Justification]\n\n**Overall Prompt Score**: [Total]/60\n**Grade**: [Excellent (50-60) / Good (40-49) / Fair (30-39) / Poor (20-29) / Very Poor (0-19)]\n\n\ud83d\udd0d **Detailed Analysis**\n\n**What Works Well:**\n\u2022 [Specific strengths of the prompt]\n\n**Areas for Improvement:**\n\u2022 [Specific weaknesses and issues]\n\n**Common Response Patterns:**\n\u2022 [Patterns observed in AI responses]\n\n**Edge Cases & Failure Modes:**\n\u2022 [Scenarios where the prompt fails or underperforms]\n\n\ud83d\udca1 **Optimization Recommendations**\n\n**High Priority:**\n\u2022 [Most critical improvements needed]\n\n**Medium Priority:**\n\u2022 [Secondary improvements that would help]\n\n**Optimization Techniques:**\n\u2022 [Specific prompt engineering techniques to apply]\n\n**Revised Prompt Suggestion:**\n```\n[Provide an improved version of the prompt if significant changes are recommended]\n```",
      "input_variables": [
        "prompt_template",
        "test_cases",
        "evaluation_context"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "prompt_optimization",
        "complexity": "high",
        "domain": "evaluation",
        "tags": [
          "prompt_engineering",
          "evaluation",
          "optimization",
          "testing",
          "effectiveness"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.352993",
        "updated_at": "2025-07-31 10:06:57.352997",
        "version": "1.0.0",
        "description": "Comprehensive evaluation template for prompt effectiveness and optimization",
        "examples": [
          {
            "description": "Evaluating a summarization prompt",
            "input": {
              "prompt_template": "Summarize the following text in 3 sentences: {{ text }}",
              "test_cases": "Test 1: Long research paper -> Generated a 2-sentence summary missing key points\nTest 2: News article -> Generated a 4-sentence summary with good coverage\nTest 3: Technical documentation -> Generated unclear summary with jargon",
              "evaluation_context": "Testing for general document summarization across different text types"
            },
            "expected_output": "**Overall Prompt Score**: 35/60 - Fair\n**Key Issues**: Inconsistent length adherence, lacks domain adaptation\n**Recommendations**: Add length constraints, specify clarity requirements, include examples"
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "prompt_template": {
          "type": "str",
          "min_length": 10,
          "max_length": 5000,
          "required": true
        },
        "test_cases": {
          "type": "str",
          "min_length": 20,
          "max_length": 10000,
          "required": true
        },
        "evaluation_context": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "response_scoring": {
      "template_id": "response_scoring",
      "name": "Response Quality Scoring",
      "type": "domain_specific",
      "template": "# Response Quality Scoring\n\n## Original Query\n{{ query }}\n\n## AI Response to Score\n{{ response }}\n\n## Reference Materials (if available)\n{{ reference_materials | format_documents }}\n\n## Scoring Criteria\n{{ scoring_criteria }}\n\n\ud83c\udfc6 **RESPONSE QUALITY SCORING**\n\n\ud83d\udccb **Core Quality Dimensions**\n\n**1. Content Quality**\n\u2022 **Accuracy**: Is the information factually correct and reliable?\n\u2022 **Relevance**: Does the response directly address the query?\n\u2022 **Depth**: How thorough and comprehensive is the response?\n\u2022 **Currency**: Is the information up-to-date and current?\n\n**2. Communication Quality** \n\u2022 **Clarity**: Is the response clear and easy to understand?\n\u2022 **Structure**: Is the response well-organized and logical?\n\u2022 **Conciseness**: Is the response appropriately concise without losing important information?\n\u2022 **Tone**: Is the tone appropriate for the context and audience?\n\n**3. Technical Quality**\n\u2022 **Coherence**: Does the response flow logically from point to point?\n\u2022 **Completeness**: Are all aspects of the query addressed?\n\u2022 **Citations**: Are sources properly referenced when applicable?\n\u2022 **Formatting**: Is the response well-formatted and readable?\n\n**4. Utility & Actionability**\n\u2022 **Usefulness**: How helpful is the response to the user?\n\u2022 **Actionability**: Does the response provide actionable insights or next steps?\n\u2022 **Practical Value**: Can the user apply this information effectively?\n\u2022 **Follow-up Guidance**: Does it help the user understand next steps?\n\n\ud83d\udcca **Detailed Scoring Matrix**\n\n**Content Quality Scores:**\n\u2022 Accuracy: [Score 1-10] - [Specific reasoning]\n\u2022 Relevance: [Score 1-10] - [Specific reasoning]\n\u2022 Depth: [Score 1-10] - [Specific reasoning]\n\u2022 Currency: [Score 1-10] - [Specific reasoning]\n\n**Communication Quality Scores:**\n\u2022 Clarity: [Score 1-10] - [Specific reasoning]\n\u2022 Structure: [Score 1-10] - [Specific reasoning]\n\u2022 Conciseness: [Score 1-10] - [Specific reasoning]\n\u2022 Tone: [Score 1-10] - [Specific reasoning]\n\n**Technical Quality Scores:**\n\u2022 Coherence: [Score 1-10] - [Specific reasoning]\n\u2022 Completeness: [Score 1-10] - [Specific reasoning]\n\u2022 Citations: [Score 1-10] - [Specific reasoning]\n\u2022 Formatting: [Score 1-10] - [Specific reasoning]\n\n**Utility & Actionability Scores:**\n\u2022 Usefulness: [Score 1-10] - [Specific reasoning]\n\u2022 Actionability: [Score 1-10] - [Specific reasoning]\n\u2022 Practical Value: [Score 1-10] - [Specific reasoning]\n\u2022 Follow-up Guidance: [Score 1-10] - [Specific reasoning]\n\n\ud83d\udcc8 **Summary Scores**\n\u2022 **Content Quality**: [Total]/40 ([Percentage]%)\n\u2022 **Communication Quality**: [Total]/40 ([Percentage]%)\n\u2022 **Technical Quality**: [Total]/40 ([Percentage]%)\n\u2022 **Utility & Actionability**: [Total]/40 ([Percentage]%)\n\n**OVERALL RESPONSE SCORE**: [Total]/160 ([Percentage]%)\n\n**Quality Grade**: \n\u2022 90-100%: Exceptional \u2b50\u2b50\u2b50\u2b50\u2b50\n\u2022 80-89%: Excellent \u2b50\u2b50\u2b50\u2b50\n\u2022 70-79%: Good \u2b50\u2b50\u2b50\n\u2022 60-69%: Fair \u2b50\u2b50\n\u2022 Below 60%: Needs Improvement \u2b50\n\n\ud83d\udca1 **Qualitative Assessment**\n\n**Key Strengths:**\n\u2022 [What the response does exceptionally well]\n\n**Areas for Improvement:**\n\u2022 [Specific areas that need enhancement]\n\n**Missing Elements:**\n\u2022 [Important information or aspects not addressed]\n\n**Standout Features:**\n\u2022 [Particularly impressive or noteworthy aspects]\n\n\ud83c\udfaf **Improvement Recommendations**\n\n**High Priority:**\n\u2022 [Most critical improvements needed]\n\n**Medium Priority:**\n\u2022 [Secondary improvements that would enhance quality]\n\n**Enhancement Suggestions:**\n\u2022 [Specific ways to make the response even better]\n\n**Comparative Analysis:**\n\u2022 How does this response compare to typical responses for similar queries?\n\u2022 What would make this response exemplary?",
      "input_variables": [
        "query",
        "response",
        "scoring_criteria"
      ],
      "optional_variables": [
        "reference_materials"
      ],
      "metadata": {
        "use_case": "response_evaluation",
        "complexity": "medium",
        "domain": "evaluation",
        "tags": [
          "scoring",
          "quality_assessment",
          "response_evaluation",
          "metrics",
          "benchmarking"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.353124",
        "updated_at": "2025-07-31 10:06:57.353124",
        "version": "1.0.0",
        "description": "Comprehensive scoring template for AI response quality assessment",
        "examples": [
          {
            "description": "Scoring a technical explanation response",
            "input": {
              "query": "How does machine learning work?",
              "response": "Machine learning is a type of AI where computers learn patterns from data without being explicitly programmed. It works by training algorithms on large datasets to recognize patterns and make predictions.",
              "scoring_criteria": "Technical accuracy, clarity for general audience, completeness of explanation",
              "reference_materials": [
                {
                  "title": "ML Fundamentals",
                  "content": "Machine learning involves training algorithms on data to make predictions or decisions without explicit programming."
                }
              ]
            },
            "expected_output": "**OVERALL RESPONSE SCORE**: 112/160 (70%)\n**Quality Grade**: Good \u2b50\u2b50\u2b50\n**Key Strengths**: Clear explanation, accurate basics\n**Areas for Improvement**: Could include examples, more depth on algorithm types"
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 5,
          "max_length": 1000,
          "required": true
        },
        "response": {
          "type": "str",
          "min_length": 10,
          "max_length": 10000,
          "required": true
        },
        "scoring_criteria": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        },
        "reference_materials": {
          "type": "list",
          "required": false
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "rag_evaluation": {
      "template_id": "rag_evaluation",
      "name": "RAG System Evaluation",
      "type": "domain_specific",
      "template": "# RAG System Evaluation\n\n## Query\n{{ query }}\n\n## Retrieved Documents\n{{ retrieved_docs | format_documents }}\n\n## Generated Response\n{{ generated_response }}\n\n## Ground Truth (if available)\n{{ ground_truth }}\n\n\ud83d\udd0d **RAG SYSTEM EVALUATION**\n\n\ud83d\udcda **Retrieval Quality Assessment**\n\u2022 **Relevance**: Are the retrieved documents relevant to the query?\n\u2022 **Coverage**: Do the documents contain information needed to answer the query?\n\u2022 **Diversity**: Do the documents provide diverse perspectives or complementary information?\n\u2022 **Ranking**: Are the most relevant documents ranked highest?\n\n\ud83c\udfaf **Answer Quality Assessment**\n\u2022 **Accuracy**: Is the generated response factually correct?\n\u2022 **Completeness**: Does the response fully address the query?\n\u2022 **Coherence**: Is the response well-structured and coherent?\n\u2022 **Citation**: Does the response appropriately reference the source documents?\n\n\ud83d\udd17 **Retrieval-Generation Alignment**\n\u2022 **Grounding**: Is the response properly grounded in the retrieved documents?\n\u2022 **Hallucination Check**: Does the response contain information not present in the retrieved docs?\n\u2022 **Consistency**: Is the response consistent with the retrieved information?\n\u2022 **Source Attribution**: Are claims properly attributed to sources?\n\n\ud83d\udcca **Scoring Matrix**\n\n**Retrieval Metrics:**\n\u2022 Relevance: [Score 1-5] - [Justification]\n\u2022 Coverage: [Score 1-5] - [Justification]\n\u2022 Diversity: [Score 1-5] - [Justification]\n\u2022 Ranking Quality: [Score 1-5] - [Justification]\n\n**Generation Metrics:**\n\u2022 Accuracy: [Score 1-5] - [Justification]\n\u2022 Completeness: [Score 1-5] - [Justification]\n\u2022 Coherence: [Score 1-5] - [Justification]\n\u2022 Citation Quality: [Score 1-5] - [Justification]\n\n**Alignment Metrics:**\n\u2022 Grounding: [Score 1-5] - [Justification]\n\u2022 Hallucination: [Score 1-5] - [Lower = more hallucination]\n\u2022 Consistency: [Score 1-5] - [Justification]\n\n**Overall RAG Score**: [Total]/55\n**Grade**: [Excellent/Good/Fair/Poor]\n\n\u26a0\ufe0f **Key Issues Identified**\n\u2022 [List major problems or concerns]\n\n\u2705 **Strengths**\n\u2022 [List what worked well]\n\n\ud83d\udd27 **Recommendations**\n\u2022 [Specific suggestions for improvement]",
      "input_variables": [
        "query",
        "retrieved_docs",
        "generated_response"
      ],
      "optional_variables": [
        "ground_truth"
      ],
      "metadata": {
        "use_case": "rag_evaluation",
        "complexity": "high",
        "domain": "evaluation",
        "tags": [
          "rag",
          "evaluation",
          "retrieval",
          "generation",
          "quality_assessment"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.353313",
        "updated_at": "2025-07-31 10:06:57.353314",
        "version": "1.0.0",
        "description": "Comprehensive evaluation template for RAG system performance",
        "examples": [
          {
            "description": "Evaluating RAG response quality",
            "input": {
              "query": "What are the benefits of renewable energy?",
              "retrieved_docs": [
                {
                  "title": "Solar Energy Benefits",
                  "content": "Solar energy reduces carbon emissions and provides long-term cost savings."
                },
                {
                  "title": "Wind Power Advantages",
                  "content": "Wind power is sustainable and creates jobs in rural communities."
                }
              ],
              "generated_response": "Renewable energy offers multiple benefits including reduced carbon emissions from solar power, long-term cost savings, job creation in rural areas through wind power, and overall sustainability.",
              "ground_truth": "Benefits include environmental protection, economic advantages, and energy independence."
            },
            "expected_output": "**Retrieval Quality**: 4/5 - Documents are relevant and provide good coverage\n**Generation Quality**: 4/5 - Response is accurate and well-grounded\n**Overall RAG Score**: 44/55"
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 5,
          "max_length": 1000,
          "required": true
        },
        "retrieved_docs": {
          "type": "list",
          "required": true
        },
        "generated_response": {
          "type": "str",
          "min_length": 10,
          "max_length": 5000,
          "required": true
        },
        "ground_truth": {
          "type": "str",
          "required": false
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "code_analysis": {
      "template_id": "code_analysis",
      "name": "Code Analysis",
      "type": "domain_specific",
      "template": "Code to analyze:\n```\n{{ context }}\n```\n\nAnalysis request: {{ query }}\n\n\ud83d\udcbb **CODE ANALYSIS REPORT**\n\n\ud83c\udfd7\ufe0f **Structure & Organization**\n\u2022 How is the code structured and organized?\n\u2022 Is it readable and well-formatted?\n\u2022 Are naming conventions consistent?\n\n\u2699\ufe0f **Logic & Functionality**\n\u2022 Does the code accomplish its intended purpose?\n\u2022 Is the logic clear and efficient?\n\u2022 Are there any logical errors or edge cases missed?\n\n\u26a0\ufe0f **Potential Issues**\n\u2022 Are there any bugs, security vulnerabilities, or performance problems?\n\u2022 What improvements could be made?\n\u2022 Are there any code smells or anti-patterns?\n\n\ud83d\udccb **Recommendations**\n\u2022 What specific changes would improve this code?\n\u2022 Are there better approaches or design patterns to consider?\n\u2022 What would make this code more maintainable?\n\n\ud83d\udcca **Overall Assessment**\nSummary of the code quality and key takeaways:\n\n**Final Rating:** [Rate the code quality and provide reasoning]",
      "input_variables": [
        "context",
        "query"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "code_review",
        "complexity": "medium",
        "domain": "software",
        "tags": [
          "code",
          "programming",
          "review",
          "analysis"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.353452",
        "updated_at": "2025-07-31 10:06:57.353457",
        "version": "1.0.0",
        "description": "Code analysis and review template",
        "examples": [
          {
            "description": "Python function review",
            "input": {
              "query": "Review this function for potential improvements",
              "context": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"
            },
            "expected_output": "## Code Analysis:\n\n**Structure & Organization:**\nThe function is clearly structured with a simple recursive approach...\n\n**Potential Issues:**\nThis recursive implementation has exponential time complexity..."
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 5,
          "max_length": 1000,
          "required": true
        },
        "context": {
          "type": "str",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "medical_qa": {
      "template_id": "medical_qa",
      "name": "Medical Question Answering",
      "type": "domain_specific",
      "template": "Medical Information:\n{{ context | format_documents }}\n\nMedical Question: {{ query }}\n\n\ud83c\udfe5 **MEDICAL ANALYSIS**\n\n\ud83d\udccb **Clinical Findings**\n\u2022 What relevant medical information can be extracted from the sources?\n\u2022 What symptoms, conditions, or treatments are mentioned?\n\u2022 Are there any diagnostic criteria or clinical indicators noted?\n\n\ud83d\udd2c **Medical Analysis**\n\u2022 Based on the available information, what can be determined?\n\u2022 How do the findings relate to the specific medical question?\n\u2022 What are the key medical concepts or mechanisms involved?\n\n\ud83d\udcda **Evidence-Based Response**\nBased on the provided medical information:\n\n\u26a0\ufe0f **IMPORTANT MEDICAL DISCLAIMER**\n\u2022 This analysis is based solely on the provided documents\n\u2022 This information is for educational purposes only\n\u2022 Always consult qualified healthcare professionals for medical advice\n\u2022 Individual cases may vary significantly and require personalized assessment\n\u2022 Do not use this information for self-diagnosis or treatment decisions\n\n**Clinical Response:**",
      "input_variables": [
        "context",
        "query"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "medical_qa",
        "complexity": "high",
        "domain": "medical",
        "tags": [
          "medical",
          "healthcare",
          "clinical",
          "analysis"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.353641",
        "updated_at": "2025-07-31 10:06:57.353642",
        "version": "1.0.0",
        "description": "Medical document analysis and question answering",
        "examples": [
          {
            "description": "Medical symptom inquiry",
            "input": {
              "query": "What are the common symptoms of hypertension?",
              "context": [
                {
                  "title": "Hypertension Overview",
                  "content": "Hypertension often presents with headaches, dizziness, chest pain, and shortness of breath, though many cases are asymptomatic."
                }
              ]
            },
            "expected_output": "Based on the medical information provided, here is my analysis: **Clinical Findings:** Hypertension commonly presents with headaches, dizziness, chest pain, and shortness of breath..."
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "query": {
          "type": "str",
          "min_length": 5,
          "max_length": 1000,
          "required": true
        },
        "context": {
          "type": "list",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "llm_judge": {
      "template_id": "llm_judge",
      "name": "LLM as Judge Evaluation",
      "type": "domain_specific",
      "template": "# LLM Judge Evaluation\n\n## Task\nEvaluate the quality of an AI response according to the specified criteria.\n\n## Original Query\n{{ original_query }}\n\n## Context Information\n{{ context | format_documents }}\n\n## AI Response to Evaluate\n{{ response_to_evaluate }}\n\n## Evaluation Criteria\n{{ evaluation_criteria }}\n\n\u2696\ufe0f **EVALUATION FRAMEWORK**\n\n\ud83d\udcca **Scoring Dimensions**\n\u2022 **Relevance** (1-5): How well does the response address the original query?\n\u2022 **Accuracy** (1-5): How factually correct is the information provided?\n\u2022 **Completeness** (1-5): Does the response fully answer the question?\n\u2022 **Clarity** (1-5): How clear and well-structured is the response?\n\u2022 **Context Usage** (1-5): How effectively does the response use the provided context?\n\n\ud83d\udd0d **Detailed Analysis**\n\u2022 What are the response's main strengths?\n\u2022 What are the key weaknesses or areas for improvement?\n\u2022 Are there any factual errors or misleading statements?\n\u2022 Does the response appropriately cite or reference the context?\n\u2022 Is the tone and style appropriate for the query?\n\n\ud83d\udcc8 **Scoring Summary**\n\u2022 Relevance: [Score]/5 - [Brief justification]\n\u2022 Accuracy: [Score]/5 - [Brief justification]\n\u2022 Completeness: [Score]/5 - [Brief justification]\n\u2022 Clarity: [Score]/5 - [Brief justification]\n\u2022 Context Usage: [Score]/5 - [Brief justification]\n\n**Overall Score**: [Total]/25\n**Grade**: [A/B/C/D/F]\n\n**Key Recommendation**: [One actionable improvement suggestion]",
      "input_variables": [
        "original_query",
        "context",
        "response_to_evaluate",
        "evaluation_criteria"
      ],
      "optional_variables": [],
      "metadata": {
        "use_case": "llm_evaluation",
        "complexity": "high",
        "domain": "evaluation",
        "tags": [
          "evaluation",
          "judge",
          "scoring",
          "quality_assessment",
          "llm_testing"
        ],
        "author": "LlamaFarm Team",
        "created_at": "2025-07-31 10:06:57.353811",
        "updated_at": "2025-07-31 10:06:57.353813",
        "version": "1.0.0",
        "description": "LLM as Judge template for evaluating AI response quality",
        "examples": [
          {
            "description": "Evaluating a medical Q&A response",
            "input": {
              "original_query": "What are the symptoms of diabetes?",
              "context": [
                {
                  "title": "Diabetes Overview",
                  "content": "Diabetes symptoms include frequent urination, excessive thirst, unexplained weight loss, and fatigue."
                }
              ],
              "response_to_evaluate": "Diabetes symptoms include feeling thirsty and urinating frequently. You might also lose weight without trying.",
              "evaluation_criteria": "Medical accuracy, completeness, and appropriate medical disclaimers"
            },
            "expected_output": "## Evaluation:\n**Relevance**: 5/5 - Directly addresses the query\n**Accuracy**: 4/5 - Information is correct but incomplete\n**Completeness**: 3/5 - Missing fatigue and other symptoms\n**Overall Score**: 18/25"
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "original_query": {
          "type": "str",
          "min_length": 5,
          "max_length": 1000,
          "required": true
        },
        "response_to_evaluate": {
          "type": "str",
          "min_length": 10,
          "max_length": 5000,
          "required": true
        },
        "evaluation_criteria": {
          "type": "str",
          "min_length": 10,
          "max_length": 1000,
          "required": true
        },
        "context": {
          "type": "list",
          "required": true
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "tool_execution": {
      "template_id": "tool_execution",
      "name": "Structured Tool Execution",
      "type": "agentic",
      "template": "# \ud83d\udd27 Tool Execution Framework\n\n## Current Task\n**Action**: {{action}}\n**Tool**: {{tool_name}}\n**Step**: {{current_step}} of {{total_steps}}\n\n## \ud83d\udccb Execution Context\n\n### Previous Steps Completed:\n{% if previous_results %}\n{% for result in previous_results %}\n- **Step {{result.step}}**: {{result.action}} \u2192 {{result.outcome}}\n{% endfor %}\n{% else %}\n- This is the first step\n{% endif %}\n\n### Current Tool Information:\n- **Tool Name**: {{tool_name}}\n- **Purpose**: {{tool_purpose}}\n- **Input Format**: {{input_format}}\n- **Expected Output**: {{expected_output}}\n\n## \ud83c\udfaf Execution Protocol\n\n### 1\ufe0f\u20e3 **Pre-Execution Validation**\n- \u2705 Confirm tool is appropriate for this task\n- \u2705 Validate input parameters are correctly formatted\n- \u2705 Check dependencies from previous steps are satisfied\n- \u2705 Verify expected output will serve the next step\n\n### 2\ufe0f\u20e3 **Tool Invocation**\n```\nTool: {{tool_name}}\nInput: {{tool_input}}\nParameters: {{tool_parameters | default('None')}}\nMode: {{execution_mode | default('standard')}}\n```\n\n### 3\ufe0f\u20e3 **Output Processing**\n- **Capture**: Record the tool's raw output\n- **Validate**: Ensure output meets expected format\n- **Transform**: Convert output for next step if needed\n- **Store**: Save results for future reference\n\n### 4\ufe0f\u20e3 **Error Handling**\n{% if error_handling %}\n{{error_handling}}\n{% else %}\n- If tool fails: Log error, attempt retry with modified parameters\n- If output invalid: Validate format, request correction\n- If unexpected result: Analyze cause, adjust approach\n- If critical failure: Escalate to human oversight\n{% endif %}\n\n## \ud83d\udcca Quality Checks\n\n### Output Validation:\n- **Format Check**: Does output match expected structure?\n- **Content Check**: Is the information relevant and accurate?\n- **Completeness Check**: Are all required fields populated?\n- **Consistency Check**: Does output align with previous steps?\n\n### Next Step Preparation:\n{% if next_step %}\n- **Next Action**: {{next_step.action}}\n- **Required Data**: {{next_step.required_data}}\n- **Handoff Format**: {{next_step.input_format}}\n{% else %}\n- This is the final step - prepare summary\n{% endif %}\n\n## \ud83d\udd04 **Execute Tool and Process Result**\n\n**Step {{current_step}} Result**:\n[Tool execution output will be processed here]\n\n**Status**: [Success/Failure/Retry Needed]\n**Next Action**: [What to do next based on result]\n\n---\n\n{% if current_step < total_steps %}\n**Ready for Step {{current_step + 1}}**: {{next_step.action if next_step else 'Final processing'}}\n{% else %}\n**Workflow Complete**: All {{total_steps}} steps executed successfully\n{% endif %}",
      "input_variables": [
        "action",
        "tool_name",
        "current_step",
        "total_steps"
      ],
      "optional_variables": [
        "previous_results",
        "tool_purpose",
        "input_format",
        "expected_output",
        "tool_input",
        "tool_parameters",
        "execution_mode",
        "error_handling",
        "next_step"
      ],
      "metadata": {
        "use_case": "Structured execution of individual tools within multi-step workflows",
        "complexity": "medium",
        "domain": "agentic",
        "tags": [
          "tool-execution",
          "workflow",
          "validation",
          "error-handling",
          "structured"
        ],
        "author": null,
        "created_at": "2025-07-31 10:06:57.354300",
        "updated_at": "2025-07-31 10:06:57.354301",
        "version": "1.0.0",
        "description": "Template for executing individual tools with proper validation and error handling",
        "examples": [
          {
            "action": "Search for recent AI research papers",
            "tool_name": "academic_search",
            "current_step": 2,
            "total_steps": 5,
            "tool_purpose": "Find relevant academic papers on specified topic"
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "action": {
          "type": "str"
        },
        "tool_name": {
          "type": "str"
        },
        "current_step": {
          "type": "int"
        },
        "total_steps": {
          "type": "int"
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "tool_selection": {
      "template_id": "tool_selection",
      "name": "Intelligent Tool Selection",
      "type": "agentic",
      "template": "# \ud83d\udd27 Intelligent Tool Selection System\n\n## Task Context\n**Objective**: {{objective}}\n**Domain**: {{domain | default('general')}}\n**Complexity Level**: {{complexity_level | default('medium')}}\n**Available Resources**: {{available_resources | default('standard')}}\n**Time Constraints**: {{time_constraints | default('normal')}}\n\n## \ud83d\udee0\ufe0f Available Tools Inventory\n\n{% for tool in available_tools %}\n### {{tool.name}}\n- **Category**: {{tool.category}}\n- **Capabilities**: {{tool.capabilities | join(', ')}}\n- **Best For**: {{tool.best_for}}\n- **Limitations**: {{tool.limitations}}\n- **Resource Cost**: {{tool.cost | default('medium')}}\n- **Execution Time**: {{tool.execution_time | default('varies')}}\n- **Reliability**: {{tool.reliability | default('high')}}\n- **Prerequisites**: {{tool.prerequisites | default('none')}}\n\n{% endfor %}\n\n## \ud83c\udfaf Tool Selection Framework\n\n### Step 1: Requirement Analysis\n**Primary Requirements**:\n{% if primary_requirements %}\n{% for req in primary_requirements %}\n- {{req.type}}: {{req.description}} (Priority: {{req.priority}})\n{% endfor %}\n{% else %}\n- Analyze what the task specifically needs\n- Identify input/output format requirements\n- Determine quality vs speed tradeoffs\n- Consider integration complexity\n{% endif %}\n\n**Secondary Requirements**:\n{% if secondary_requirements %}\n{% for req in secondary_requirements %}\n- {{req.type}}: {{req.description}} (Nice-to-have: {{req.importance}})\n{% endfor %}\n{% else %}\n- Performance optimization needs\n- User experience considerations\n- Maintainability requirements\n- Scalability factors\n{% endif %}\n\n### Step 2: Tool Compatibility Matrix\n\n| Tool | Capability Match | Resource Fit | Time Fit | Integration Ease | Overall Score |\n|------|------------------|--------------|----------|------------------|---------------|\n{% for tool in available_tools %}\n| {{tool.name}} | {{tool.capability_match | default('TBD')}} | {{tool.resource_fit | default('TBD')}} | {{tool.time_fit | default('TBD')}} | {{tool.integration_ease | default('TBD')}} | {{tool.overall_score | default('TBD')}} |\n{% endfor %}\n\n### Step 3: Selection Criteria Weighting\n\n**Criteria Weights** (Total = 100%):\n- **Capability Match**: {{capability_weight | default('40%')}}\n- **Resource Efficiency**: {{resource_weight | default('20%')}}\n- **Execution Speed**: {{speed_weight | default('15%')}}\n- **Reliability**: {{reliability_weight | default('15%')}}\n- **Integration Ease**: {{integration_weight | default('10%')}}\n\n### Step 4: Decision Logic\n\n```\nIF task_type == \"{{task_type | default('analysis')}}\":\n    PREFER tools with: {{preferred_capabilities | join(', ') if preferred_capabilities else 'analytical capabilities'}}\n    \nIF time_constraints == \"tight\":\n    WEIGHT execution_speed *= 2\n    \nIF resource_constraints == \"limited\":\n    FILTER OUT high_cost_tools\n    \nIF integration_complexity == \"high\":\n    PREFER tools with: good_apis, documentation, support\n```\n\n## \ud83d\ude80 Recommended Tool Selection\n\n### Primary Tool Choice: {{primary_tool | default('[To be determined]')}}\n**Justification**:\n{% if primary_justification %}\n{{primary_justification}}\n{% else %}\n- Best match for core requirements\n- Optimal resource/performance balance\n- Proven reliability for this use case\n- Good integration characteristics\n{% endif %}\n\n**Configuration**:\n- **Parameters**: {{primary_config | default('Default settings recommended')}}\n- **Resource Allocation**: {{primary_resources | default('Standard allocation')}}\n- **Expected Performance**: {{primary_performance | default('High quality results expected')}}\n\n### Backup Tool Choice: {{backup_tool | default('[Fallback if needed]')}}\n**Fallback Scenario**: {{fallback_scenario | default('If primary tool fails or is unavailable')}}\n**Differences**: {{backup_differences | default('May have different performance characteristics')}}\n\n### Tool Combination Strategy\n{% if tool_combination %}\n**Multi-Tool Approach**: {{tool_combination.strategy}}\n**Workflow**:\n{% for step in tool_combination.steps %}\n{{loop.index}}. **{{step.tool}}**: {{step.purpose}}\n   - Input: {{step.input}}\n   - Output: {{step.output}}\n{% endfor %}\n{% else %}\n**Single Tool Approach**: Use primary tool for entire task\n**Rationale**: Task complexity doesn't warrant multiple tools\n{% endif %}\n\n## \u26a0\ufe0f Risk Assessment & Mitigation\n\n### Potential Issues:\n{% if potential_issues %}\n{% for issue in potential_issues %}\n- **{{issue.type}}**: {{issue.description}}\n  - **Probability**: {{issue.probability}}\n  - **Impact**: {{issue.impact}}\n  - **Mitigation**: {{issue.mitigation}}\n{% endfor %}\n{% else %}\n- Tool availability issues\n- Performance degradation under load\n- Integration compatibility problems\n- Unexpected output format changes\n{% endif %}\n\n### Monitoring Strategy:\n- **Success Metrics**: {{success_metrics | default('Output quality, execution time, error rate')}}\n- **Warning Thresholds**: {{warning_thresholds | default('Performance 20% below expected')}}\n- **Failover Triggers**: {{failover_triggers | default('3 consecutive failures or 50% performance drop')}}\n\n## \ud83d\udcca Expected Outcomes\n\n### Performance Predictions:\n- **Execution Time**: {{predicted_time | default('Based on tool specifications')}}\n- **Resource Usage**: {{predicted_resources | default('Within allocated limits')}}\n- **Output Quality**: {{predicted_quality | default('High quality expected')}}\n- **Success Probability**: {{success_probability | default('85-95% based on tool reliability')}}\n\n### Quality Gates:\n1. **Input Validation**: Ensure input meets tool requirements\n2. **Process Monitoring**: Track execution progress and performance\n3. **Output Verification**: Validate output format and content\n4. **Integration Testing**: Confirm compatibility with downstream processes\n\n## \ud83c\udfaf **Final Tool Selection Decision**\n\n**Selected Tool(s)**: {{final_selection | default('[Decision pending analysis]')}}\n**Confidence Level**: {{confidence_level | default('High')}}\n**Implementation Priority**: {{implementation_priority | default('Immediate')}}\n\n**Next Steps**:\n1. {{next_step_1 | default('Validate tool availability and configuration')}}\n2. {{next_step_2 | default('Set up monitoring and error handling')}}\n3. {{next_step_3 | default('Execute initial test run')}}\n4. {{next_step_4 | default('Deploy to production workflow')}}\n\n---\n\n**Selection Rationale Summary**: {{selection_summary | default('Comprehensive analysis of requirements, capabilities, and constraints led to optimal tool choice for this specific task context.')}}",
      "input_variables": [
        "objective",
        "available_tools"
      ],
      "optional_variables": [
        "domain",
        "complexity_level",
        "available_resources",
        "time_constraints",
        "primary_requirements",
        "secondary_requirements",
        "task_type",
        "preferred_capabilities",
        "capability_weight",
        "resource_weight",
        "speed_weight",
        "reliability_weight",
        "integration_weight",
        "primary_tool",
        "primary_justification",
        "primary_config",
        "primary_resources",
        "primary_performance",
        "backup_tool",
        "fallback_scenario",
        "backup_differences",
        "tool_combination",
        "potential_issues",
        "success_metrics",
        "warning_thresholds",
        "failover_triggers",
        "predicted_time",
        "predicted_resources",
        "predicted_quality",
        "success_probability",
        "final_selection",
        "confidence_level",
        "implementation_priority",
        "next_step_1",
        "next_step_2",
        "next_step_3",
        "next_step_4",
        "selection_summary"
      ],
      "metadata": {
        "use_case": "Intelligently selecting the best tools for specific tasks based on requirements and constraints",
        "complexity": "high",
        "domain": "agentic",
        "tags": [
          "tool-selection",
          "decision-making",
          "optimization",
          "analysis",
          "strategy"
        ],
        "author": null,
        "created_at": "2025-07-31 10:06:57.354535",
        "updated_at": "2025-07-31 10:06:57.354536",
        "version": "1.0.0",
        "description": "Template for systematic tool selection based on task requirements, constraints, and available options",
        "examples": [
          {
            "objective": "Generate comprehensive market research report",
            "available_tools": [
              {
                "name": "web_scraper",
                "category": "data_collection",
                "capabilities": [
                  "web_scraping",
                  "data_extraction"
                ],
                "best_for": "gathering online information"
              },
              {
                "name": "data_analyzer",
                "category": "analysis",
                "capabilities": [
                  "statistical_analysis",
                  "visualization"
                ],
                "best_for": "processing large datasets"
              }
            ]
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "objective": {
          "type": "str"
        },
        "available_tools": {
          "type": "list"
        },
        "complexity_level": {
          "type": "str",
          "enum": [
            "low",
            "medium",
            "high",
            "very_high"
          ]
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "tool_planning": {
      "template_id": "tool_planning",
      "name": "Multi-Step Tool Planning",
      "type": "agentic",
      "template": "# \ud83e\udd16 Agent Tool Planning System\n\n## Task Overview\n**Goal**: {{goal}}\n**Available Tools**: {{available_tools}}\n{% if context %}**Context**: {{context}}{% endif %}\n\n## \ud83d\udccb Planning Framework\n\n### 1\ufe0f\u20e3 **Task Analysis**\n- Break down the goal into actionable steps\n- Identify dependencies between steps\n- Determine required tools for each step\n\n### 2\ufe0f\u20e3 **Tool Selection Strategy**\n{% for tool in available_tools %}\n- **{{tool.name}}**: {{tool.description}}\n  - Use when: {{tool.use_case}}\n  - Input requirements: {{tool.inputs}}\n{% endfor %}\n\n### 3\ufe0f\u20e3 **Execution Plan**\nCreate a step-by-step plan in this format:\n\n**Step X**: [Action description]\n- Tool: [tool_name]\n- Input: [specific inputs needed]\n- Expected output: [what this step will produce]\n- Dependencies: [what must be completed first]\n\n### 4\ufe0f\u20e3 **Contingency Planning**\n- What could go wrong at each step?\n- Alternative approaches if primary plan fails\n- Recovery strategies\n\n## \ud83c\udfaf **Execution Plan**\n[Your detailed step-by-step plan here]",
      "input_variables": [
        "goal",
        "available_tools"
      ],
      "optional_variables": [
        "context",
        "constraints",
        "success_criteria"
      ],
      "metadata": {
        "use_case": "Planning multi-step tool usage for AI agents",
        "complexity": "high",
        "domain": "agentic",
        "tags": [
          "agents",
          "planning",
          "tools",
          "multi-step",
          "workflow"
        ],
        "author": null,
        "created_at": "2025-07-31 10:06:57.354688",
        "updated_at": "2025-07-31 10:06:57.354689",
        "version": "1.0.0",
        "description": "Template for planning complex multi-step workflows with tool usage",
        "examples": [
          {
            "goal": "Research and create a market analysis report",
            "available_tools": [
              {
                "name": "web_search",
                "description": "Search web for information",
                "use_case": "gathering market data",
                "inputs": "search query"
              },
              {
                "name": "data_analyzer",
                "description": "Analyze datasets",
                "use_case": "statistical analysis",
                "inputs": "data file"
              }
            ]
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "goal": {
          "type": "str"
        },
        "available_tools": {
          "type": "list"
        },
        "context": {
          "type": "str"
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "workflow_orchestration": {
      "template_id": "workflow_orchestration",
      "name": "Complex Workflow Orchestration",
      "type": "agentic",
      "template": "# \ud83c\udfbc Workflow Orchestration Engine\n\n## Workflow Overview\n**Workflow Name**: {{workflow_name}}\n**Total Steps**: {{total_steps}}\n**Current Status**: {{current_status | default('Initializing')}}\n**Priority Level**: {{priority | default('Medium')}}\n\n## \ud83d\udccb Workflow Definition\n\n### Input Requirements:\n{% for input in inputs %}\n- **{{input.name}}**: {{input.description}} (Type: {{input.type}}, Required: {{input.required | default('Yes')}})\n{% endfor %}\n\n### Expected Outputs:\n{% for output in expected_outputs %}\n- **{{output.name}}**: {{output.description}} (Format: {{output.format}})\n{% endfor %}\n\n## \ud83d\udd04 Step-by-Step Orchestration\n\n{% for step in workflow_steps %}\n### Step {{step.id}}: {{step.name}}\n**Type**: {{step.type}}\n**Agent/Tool**: {{step.executor}}\n**Dependencies**: {{step.dependencies | join(', ') if step.dependencies else 'None'}}\n**Estimated Duration**: {{step.duration | default('Unknown')}}\n\n**Inputs**:\n{% for input in step.inputs %}\n- {{input.name}}: {{input.source}} ({{input.type}})\n{% endfor %}\n\n**Processing Logic**:\n{{step.logic}}\n\n**Success Criteria**:\n{% for criteria in step.success_criteria %}\n- {{criteria}}\n{% endfor %}\n\n**Error Handling**:\n- **Retry Logic**: {{step.retry_logic | default('Exponential backoff, max 3 attempts')}}\n- **Fallback**: {{step.fallback | default('Skip step and continue with degraded functionality')}}\n- **Escalation**: {{step.escalation | default('Alert supervisor after 2 failures')}}\n\n**Output Specification**:\n{% for output in step.outputs %}\n- {{output.name}}: {{output.description}} \u2192 Goes to: {{output.destination}}\n{% endfor %}\n\n---\n{% endfor %}\n\n## \ud83d\udd00 Orchestration Control Flow\n\n### Parallel Execution Opportunities:\n{% if parallel_groups %}\n{% for group in parallel_groups %}\n- **Group {{group.id}}**: Steps {{group.steps | join(', ')}} can run simultaneously\n  - Resource requirements: {{group.resources}}\n  - Synchronization point: Step {{group.sync_point}}\n{% endfor %}\n{% else %}\n- Analyze steps for parallelization opportunities\n- Identify resource conflicts and dependencies\n- Optimize execution order for minimum total time\n{% endif %}\n\n### Critical Path Analysis:\n- **Longest sequence**: {{critical_path | join(' \u2192 ') if critical_path else 'To be determined'}}\n- **Total estimated time**: {{total_estimated_time | default('Sum of critical path')}}\n- **Bottleneck steps**: {{bottlenecks | join(', ') if bottlenecks else 'To be identified'}}\n\n### Decision Points:\n{% if decision_points %}\n{% for decision in decision_points %}\n- **After Step {{decision.after_step}}**: {{decision.condition}}\n  - If True: Continue to Step {{decision.if_true}}\n  - If False: {{decision.if_false}}\n{% endfor %}\n{% else %}\n- No conditional branching in this workflow\n{% endif %}\n\n## \ud83d\udcca Monitoring & Control\n\n### Progress Tracking:\n- **Completion Percentage**: {{completion_percentage | default('0%')}}\n- **Steps Completed**: {{completed_steps | default('None')}} / {{total_steps}}\n- **Current Bottlenecks**: {{current_bottlenecks | default('None identified')}}\n- **Resource Utilization**: {{resource_utilization | default('Normal')}}\n\n### Quality Gates:\n{% if quality_gates %}\n{% for gate in quality_gates %}\n- **After Step {{gate.step}}**: {{gate.check}}\n  - Pass Criteria: {{gate.criteria}}\n  - Fail Action: {{gate.fail_action}}\n{% endfor %}\n{% else %}\n- Quality validation after each major milestone\n- Output format verification before handoffs\n- Final quality check before workflow completion\n{% endif %}\n\n### Error Recovery Strategies:\n1. **Step-Level Recovery**: Individual step retry with modified parameters\n2. **Workflow-Level Recovery**: Restart from last successful checkpoint\n3. **Human Intervention**: Escalate complex errors to human operators\n4. **Graceful Degradation**: Complete workflow with reduced functionality\n\n## \ud83c\udfaf Execution Commands\n\n### Start Workflow:\n```\nSTART_WORKFLOW: {{workflow_name}}\nMODE: {{execution_mode | default('automatic')}}\nPRIORITY: {{priority}}\nINPUTS: [Validated input parameters]\n```\n\n### Monitor Progress:\n```\nGET_STATUS: {{workflow_name}}\nDETAIL_LEVEL: {{detail_level | default('summary')}}\nINCLUDE_METRICS: {{include_metrics | default('true')}}\n```\n\n### Handle Interventions:\n```\nPAUSE_WORKFLOW: [At decision points or errors]\nRESUME_WORKFLOW: [After manual intervention]\nMODIFY_STEP: [Adjust parameters mid-execution]\nSKIP_STEP: [Continue without problematic step]\n```\n\n## \ud83d\ude80 **Initialize Workflow Execution**\n\n**Pre-flight Checklist**:\n- \u2705 All required inputs validated\n- \u2705 Agent/tool availability confirmed\n- \u2705 Resource allocation verified\n- \u2705 Error handling procedures activated\n- \u2705 Monitoring systems engaged\n\n**Execution Status**: {{execution_status | default('Ready to Start')}}\n**Next Action**: {{next_action | default('Begin Step 1')}}\n\n---\n\n{% if current_status == 'Running' %}\n**Currently Executing**: Step {{current_step}} - {{current_step_name}}\n**Progress**: {{step_progress | default('In progress...')}}\n**ETA**: {{estimated_completion | default('Calculating...')}}\n{% elif current_status == 'Completed' %}\n**Workflow Completed Successfully** \u2705\n**Total Duration**: {{actual_duration}}\n**Final Outputs**: {{final_outputs | join(', ')}}\n{% elif current_status == 'Error' %}\n**Workflow Paused Due to Error** \u26a0\ufe0f\n**Error Location**: Step {{error_step}}\n**Error Details**: {{error_details}}\n**Recovery Options**: {{recovery_options | join(', ')}}\n{% endif %}",
      "input_variables": [
        "workflow_name",
        "total_steps",
        "inputs",
        "expected_outputs",
        "workflow_steps"
      ],
      "optional_variables": [
        "current_status",
        "priority",
        "parallel_groups",
        "critical_path",
        "total_estimated_time",
        "bottlenecks",
        "decision_points",
        "completion_percentage",
        "completed_steps",
        "current_bottlenecks",
        "resource_utilization",
        "quality_gates",
        "execution_mode",
        "detail_level",
        "include_metrics",
        "execution_status",
        "next_action",
        "current_step",
        "current_step_name",
        "step_progress",
        "estimated_completion",
        "actual_duration",
        "final_outputs",
        "error_step",
        "error_details",
        "recovery_options"
      ],
      "metadata": {
        "use_case": "Orchestrating complex multi-step workflows with multiple agents and tools",
        "complexity": "high",
        "domain": "agentic",
        "tags": [
          "orchestration",
          "workflow",
          "multi-step",
          "coordination",
          "monitoring",
          "error-handling"
        ],
        "author": null,
        "created_at": "2025-07-31 10:06:57.354840",
        "updated_at": "2025-07-31 10:06:57.354841",
        "version": "1.0.0",
        "description": "Template for managing complex workflows with multiple agents, tools, and decision points",
        "examples": [
          {
            "workflow_name": "Automated Content Creation Pipeline",
            "total_steps": 6,
            "inputs": [
              {
                "name": "topic",
                "description": "Content topic",
                "type": "string",
                "required": true
              },
              {
                "name": "target_audience",
                "description": "Intended audience",
                "type": "string",
                "required": true
              }
            ],
            "expected_outputs": [
              {
                "name": "final_article",
                "description": "Complete article with images",
                "format": "markdown"
              }
            ]
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "workflow_name": {
          "type": "str"
        },
        "total_steps": {
          "type": "int"
        },
        "inputs": {
          "type": "list"
        },
        "expected_outputs": {
          "type": "list"
        },
        "workflow_steps": {
          "type": "list"
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "agent_reflection": {
      "template_id": "agent_reflection",
      "name": "Agent Self-Reflection and Learning",
      "type": "agentic",
      "template": "# \ud83e\udde0 Agent Reflection & Learning System\n\n## Task Performance Review\n**Completed Task**: {{task_description}}\n**Execution Duration**: {{duration | default('Not specified')}}\n**Final Outcome**: {{outcome}}\n**Success Level**: {{success_level | default('Partial Success')}}\n\n## \ud83d\udcca Performance Analysis\n\n### What Went Well? \u2705\n{% if successes %}\n{% for success in successes %}\n- **{{success.category}}**: {{success.description}}\n  - Impact: {{success.impact}}\n  - Why it worked: {{success.reason}}\n{% endfor %}\n{% else %}\n[Analyze successful aspects of the task execution]\n- Tool usage effectiveness\n- Problem-solving approach\n- Resource utilization\n- Communication clarity\n{% endif %}\n\n### What Could Be Improved? \ud83d\udd04\n{% if improvements %}\n{% for improvement in improvements %}\n- **{{improvement.area}}**: {{improvement.issue}}\n  - Root cause: {{improvement.cause}}\n  - Suggested fix: {{improvement.solution}}\n  - Priority: {{improvement.priority}}\n{% endfor %}\n{% else %}\n[Identify areas for improvement]\n- Efficiency bottlenecks\n- Decision-making quality\n- Tool selection accuracy\n- Error handling effectiveness\n{% endif %}\n\n### Challenges Encountered \ud83d\udea7\n{% if challenges %}\n{% for challenge in challenges %}\n- **Challenge**: {{challenge.description}}\n  - How handled: {{challenge.response}}\n  - Effectiveness: {{challenge.effectiveness}}\n  - Alternative approaches: {{challenge.alternatives}}\n{% endfor %}\n{% else %}\n[Document significant challenges faced]\n- Unexpected obstacles\n- Resource limitations\n- Tool failures or limitations\n- Coordination difficulties\n{% endif %}\n\n## \ud83c\udfaf Strategy Effectiveness Assessment\n\n### Tool Selection & Usage\n{% if tools_used %}\n{% for tool in tools_used %}\n- **{{tool.name}}**\n  - Appropriateness: {{tool.appropriateness}}/10\n  - Efficiency: {{tool.efficiency}}/10\n  - Result Quality: {{tool.quality}}/10\n  - Would use again: {{tool.reuse | default('Yes')}}\n{% endfor %}\n{% else %}\n[Evaluate tool choices and their effectiveness]\n- Were the right tools selected for each task?\n- How efficiently were tools utilized?\n- Did tools produce expected quality results?\n{% endif %}\n\n### Decision-Making Process\n- **Speed**: How quickly were decisions made?\n- **Accuracy**: Were decisions well-informed and correct?\n- **Adaptability**: How well did the agent adapt when plans changed?\n- **Risk Assessment**: Were potential risks properly evaluated?\n\n## \ud83d\udcda Learning Extraction\n\n### Key Insights Gained \ud83d\udca1\n{% if insights %}\n{% for insight in insights %}\n- **{{insight.topic}}**: {{insight.learning}}\n  - Application: {{insight.application}}\n  - Confidence: {{insight.confidence}}\n{% endfor %}\n{% else %}\n[Extract actionable insights for future tasks]\n- Pattern recognition improvements\n- Strategy refinements\n- Tool usage optimizations\n- Error prevention methods\n{% endif %}\n\n### Updated Mental Models \ud83e\udde9\n- What assumptions were challenged?\n- How has understanding of the problem domain evolved?\n- What new patterns or relationships were discovered?\n- How should approach be modified for similar future tasks?\n\n## \ud83d\ude80 Optimization Recommendations\n\n### For Similar Future Tasks:\n1. **Planning Phase**:\n   - {{planning_recommendations | default('Apply learnings to improve initial planning')}}\n   \n2. **Execution Phase**:\n   - {{execution_recommendations | default('Implement identified efficiency improvements')}}\n   \n3. **Monitoring Phase**:\n   - {{monitoring_recommendations | default('Enhance progress tracking and early warning systems')}}\n\n### Skill Development Priorities:\n{% if skill_priorities %}\n{% for skill in skill_priorities %}\n- **{{skill.name}}**: {{skill.description}} (Priority: {{skill.priority}})\n{% endfor %}\n{% else %}\n- [Identify specific skills to develop based on performance gaps]\n- Tool mastery improvements\n- Problem decomposition techniques\n- Error recovery strategies\n{% endif %}\n\n## \ud83d\udcc8 Success Metrics for Next Time\n\n{% if future_metrics %}\n{% for metric in future_metrics %}\n- **{{metric.name}}**: {{metric.description}} (Target: {{metric.target}})\n{% endfor %}\n{% else %}\n- **Efficiency**: Complete similar tasks X% faster\n- **Accuracy**: Achieve higher success rate on first attempt\n- **Resource Usage**: Optimize tool usage for better resource efficiency\n- **Quality**: Improve output quality metrics\n{% endif %}\n\n## \ud83c\udfaf **Action Items for Continuous Improvement**\n\n**Immediate Actions** (Next 1-3 tasks):\n- [List 2-3 specific improvements to implement immediately]\n\n**Medium-term Development** (Next 5-10 tasks):\n- [List strategic improvements requiring more practice]\n\n**Long-term Learning Goals** (Ongoing development):\n- [List fundamental skill areas for continued growth]\n\n---\n\n**Reflection Confidence**: {{reflection_confidence | default('80%')}}\n**Ready for Next Challenge**: {{readiness_level | default('Yes, with improvements implemented')}}",
      "input_variables": [
        "task_description",
        "outcome"
      ],
      "optional_variables": [
        "duration",
        "success_level",
        "successes",
        "improvements",
        "challenges",
        "tools_used",
        "insights",
        "planning_recommendations",
        "execution_recommendations",
        "monitoring_recommendations",
        "skill_priorities",
        "future_metrics",
        "reflection_confidence",
        "readiness_level"
      ],
      "metadata": {
        "use_case": "Post-task reflection and learning for AI agents to improve future performance",
        "complexity": "high",
        "domain": "agentic",
        "tags": [
          "reflection",
          "learning",
          "self-improvement",
          "performance-analysis",
          "optimization"
        ],
        "author": null,
        "created_at": "2025-07-31 10:06:57.354960",
        "updated_at": "2025-07-31 10:06:57.354961",
        "version": "1.0.0",
        "description": "Template for agents to analyze their performance and extract learning for future improvement",
        "examples": [
          {
            "task_description": "Research and create market analysis report for new product launch",
            "outcome": "Completed comprehensive 15-page report with market size, competition analysis, and recommendations",
            "success_level": "Partial Success"
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "task_description": {
          "type": "str"
        },
        "outcome": {
          "type": "str"
        },
        "success_level": {
          "type": "str",
          "enum": [
            "Complete Success",
            "Partial Success",
            "Mixed Results",
            "Needs Improvement"
          ]
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    },
    "agent_coordinator": {
      "template_id": "agent_coordinator",
      "name": "Multi-Agent Coordination",
      "type": "agentic",
      "template": "# \ud83c\udfad Multi-Agent Coordination System\n\n## Mission Brief\n**Primary Objective**: {{objective}}\n**Available Agents**: {{agents}}\n**Coordination Mode**: {{coordination_mode | default('collaborative')}}\n\n## \ud83e\udd1d Agent Roles & Responsibilities\n\n{% for agent in agents %}\n### {{agent.name}} Agent\n- **Specialty**: {{agent.specialty}}\n- **Capabilities**: {{agent.capabilities | join(', ')}}\n- **Role in Mission**: {{agent.role}}\n- **Communication Protocol**: {{agent.communication | default('standard')}}\n{% endfor %}\n\n## \ud83d\udcca Coordination Strategy\n\n### Phase 1: Task Distribution\n1. **Analyze** the objective and break into sub-tasks\n2. **Assign** tasks based on agent specialties\n3. **Define** handoff points between agents\n4. **Establish** success criteria for each task\n\n### Phase 2: Execution Coordination\n- **Parallel Tasks**: Tasks that can run simultaneously\n- **Sequential Dependencies**: Tasks that must wait for others\n- **Critical Path**: The longest sequence of dependent tasks\n- **Resource Conflicts**: Shared resources that need coordination\n\n### Phase 3: Integration & Quality Control\n- **Output Integration**: How agent outputs combine\n- **Quality Checkpoints**: Validation at each stage\n- **Error Handling**: Recovery from agent failures\n- **Final Assembly**: Consolidating results\n\n## \ud83d\udd04 Execution Protocol\n\n{% if coordination_mode == 'hierarchical' %}\n**Hierarchical Mode**: Central coordinator assigns and manages tasks\n1. Coordinator analyzes objective\n2. Delegates specific tasks to specialized agents\n3. Monitors progress and adjusts assignments\n4. Integrates outputs into final result\n{% elif coordination_mode == 'collaborative' %}\n**Collaborative Mode**: Agents negotiate and self-organize\n1. All agents review the objective together\n2. Agents negotiate task assignments based on capabilities\n3. Establish peer-to-peer communication channels\n4. Collaborate on integration and quality control\n{% else %}\n**Sequential Mode**: Agents work in defined sequence\n1. Define the processing pipeline\n2. Each agent processes and passes to next\n3. Maintain state and context through handoffs\n4. Final agent produces integrated result\n{% endif %}\n\n## \ud83d\udcdd Communication Protocol\n\n**Status Updates**: {{status_frequency | default('After each major task')}}\n**Conflict Resolution**: {{conflict_resolution | default('Escalate to coordinator')}}\n**Progress Tracking**: {{progress_tracking | default('Shared status board')}}\n\n## \ud83c\udfaf Success Metrics\n\n{% if success_metrics %}\n{% for metric in success_metrics %}\n- **{{metric.name}}**: {{metric.description}} (Target: {{metric.target}})\n{% endfor %}\n{% else %}\n- **Completeness**: All sub-tasks completed successfully\n- **Quality**: Output meets specified criteria\n- **Efficiency**: Completed within time/resource constraints\n- **Coordination**: Smooth handoffs between agents\n{% endif %}\n\n## \ud83d\ude80 **Execute Coordination Plan**\n[Your specific coordination strategy and task assignments here]",
      "input_variables": [
        "objective",
        "agents"
      ],
      "optional_variables": [
        "coordination_mode",
        "success_metrics",
        "status_frequency",
        "conflict_resolution",
        "progress_tracking"
      ],
      "metadata": {
        "use_case": "Coordinating multiple AI agents working together on complex tasks",
        "complexity": "high",
        "domain": "agentic",
        "tags": [
          "multi-agent",
          "coordination",
          "collaboration",
          "workflow",
          "delegation"
        ],
        "author": null,
        "created_at": "2025-07-31 10:06:57.355060",
        "updated_at": "2025-07-31 10:06:57.355061",
        "version": "1.0.0",
        "description": "Template for coordinating multiple AI agents with different specialties",
        "examples": [
          {
            "objective": "Create comprehensive business plan",
            "agents": [
              {
                "name": "Researcher",
                "specialty": "Market Research",
                "capabilities": [
                  "web_search",
                  "data_analysis"
                ],
                "role": "Gather market intelligence"
              },
              {
                "name": "Analyst",
                "specialty": "Financial Analysis",
                "capabilities": [
                  "financial_modeling",
                  "projections"
                ],
                "role": "Create financial projections"
              },
              {
                "name": "Writer",
                "specialty": "Document Creation",
                "capabilities": [
                  "writing",
                  "formatting"
                ],
                "role": "Compile final business plan"
              }
            ]
          }
        ],
        "performance_notes": null
      },
      "global_prompts": [],
      "preprocessing_steps": [],
      "postprocessing_steps": [],
      "validation_rules": {
        "objective": {
          "type": "str"
        },
        "agents": {
          "type": "list"
        },
        "coordination_mode": {
          "type": "str",
          "enum": [
            "hierarchical",
            "collaborative",
            "sequential"
          ]
        }
      },
      "langgraph_config": null,
      "conditions": {},
      "fallback_templates": []
    }
  },
  "strategies": {
    "static_strategy": {
      "strategy_id": "static_strategy",
      "name": "Static Template Selection",
      "type": "static",
      "description": "Always uses the same template",
      "config": {
        "default_template": "qa_basic"
      },
      "rules": [],
      "fallback_template": null,
      "fallback_chain": [],
      "variants": [],
      "traffic_split": {},
      "performance_metrics": [],
      "langgraph_workflow": null,
      "enabled": true,
      "priority": 100,
      "tags": [],
      "created_by": null
    },
    "rule_based_strategy": {
      "strategy_id": "rule_based_strategy",
      "name": "Rule-Based Selection",
      "type": "rule_based",
      "description": "Select templates based on explicit rules",
      "config": {},
      "rules": [
        {
          "rule_id": "medical_domain_rule",
          "name": "Medical Domain Rule",
          "description": null,
          "field": "domain",
          "operator": "==",
          "value": "medical",
          "template_id": "medical_qa",
          "priority": 10,
          "enabled": true,
          "additional_conditions": [],
          "or_conditions": [],
          "tags": [],
          "created_by": null
        },
        {
          "rule_id": "summary_rule",
          "name": "Summary Request Rule",
          "description": null,
          "field": "query_type",
          "operator": "==",
          "value": "summary",
          "template_id": "summarization",
          "priority": 30,
          "enabled": true,
          "additional_conditions": [],
          "or_conditions": [],
          "tags": [],
          "created_by": null
        },
        {
          "rule_id": "complex_analysis_rule",
          "name": "Complex Analysis Rule",
          "description": null,
          "field": "complexity_level",
          "operator": "==",
          "value": "high",
          "template_id": "chain_of_thought",
          "priority": 40,
          "enabled": true,
          "additional_conditions": [],
          "or_conditions": [],
          "tags": [],
          "created_by": null
        }
      ],
      "fallback_template": "qa_basic",
      "fallback_chain": [],
      "variants": [],
      "traffic_split": {},
      "performance_metrics": [],
      "langgraph_workflow": null,
      "enabled": true,
      "priority": 100,
      "tags": [],
      "created_by": null
    },
    "context_aware_strategy": {
      "strategy_id": "context_aware_strategy",
      "name": "Context-Aware Selection",
      "type": "context_aware",
      "description": "Intelligent selection based on multiple context factors",
      "config": {
        "domain_templates": {
          "medical": "medical_qa",
          "software": "code_analysis"
        },
        "complexity_templates": {
          "high": "chain_of_thought",
          "medium": "qa_detailed",
          "low": "qa_basic"
        },
        "intent_templates": {
          "summarize": "summarization",
          "chat": "chat_assistant"
        }
      },
      "rules": [],
      "fallback_template": "qa_basic",
      "fallback_chain": [],
      "variants": [],
      "traffic_split": {},
      "performance_metrics": [],
      "langgraph_workflow": null,
      "enabled": true,
      "priority": 100,
      "tags": [],
      "created_by": null
    }
  },
  "fallback_behavior": {
    "strategy_fallback_chain": [
      "context_aware_strategy",
      "rule_based_strategy",
      "static_strategy"
    ],
    "template_fallback_chain": [
      "qa_basic",
      "chat_assistant"
    ],
    "error_handling": {
      "log_errors": true,
      "return_error_details": false
    }
  },
  "integrations": {
    "rag_system": {
      "enabled": true,
      "context_mapping": {
        "documents": "context",
        "query": "query",
        "domain": "domain"
      }
    },
    "langgraph": {
      "enabled": false,
      "workflow_endpoint": "http://localhost:8000/workflows"
    }
  },
  "monitoring": {
    "enabled": true,
    "metrics": [
      "execution_time",
      "template_usage",
      "error_rate",
      "fallback_rate"
    ],
    "logging_level": "INFO"
  },
  "environments": {
    "development": {
      "monitoring": {
        "logging_level": "DEBUG"
      },
      "fallback_behavior": {
        "return_error_details": true
      }
    },
    "production": {
      "monitoring": {
        "logging_level": "WARNING"
      },
      "global_prompts": [
        {
          "global_id": "production_disclaimer",
          "name": "Production Disclaimer",
          "suffix_prompt": "\n\n---\nNote: This response was generated by an AI system. Please verify important information independently.",
          "applies_to": [
            "*"
          ],
          "priority": 200,
          "enabled": true
        }
      ]
    }
  }
}