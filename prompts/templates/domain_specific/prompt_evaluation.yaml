template_id: prompt_evaluation
name: Prompt Effectiveness Evaluation
type: domain_specific
template: '# Prompt Effectiveness Evaluation


  ## Prompt Being Evaluated

  ```

  {{ prompt_template }}

  ```


  ## Test Cases and Responses

  {{ test_cases }}


  ## Evaluation Context

  {{ evaluation_context }}


  📝 **PROMPT EVALUATION FRAMEWORK**


  🎯 **Clarity & Instructions**

  • **Instruction Clarity**: Are the instructions clear and unambiguous?

  • **Task Definition**: Is the desired task/output clearly defined?

  • **Format Specification**: Are output format requirements clearly stated?

  • **Role Definition**: Is the AI''s role or persona clearly established?


  🧠 **Cognitive Load & Complexity**

  • **Cognitive Demand**: How much reasoning does the prompt require?

  • **Context Length**: Is the prompt appropriately concise yet comprehensive?

  • **Step Breakdown**: Are complex tasks broken into manageable steps?

  • **Mental Model**: Does the prompt help establish the right mental framework?


  ⚡ **Performance Effectiveness**

  • **Consistency**: Does the prompt produce consistent outputs across similar inputs?

  • **Accuracy**: How accurate are the responses generated by this prompt?

  • **Completeness**: Do responses fully address the intended requirements?

  • **Reliability**: How reliably does the prompt work across different scenarios?


  🎨 **Design Quality**

  • **Structure**: Is the prompt well-organized and logically structured?

  • **Examples**: Are examples provided where helpful?

  • **Constraints**: Are appropriate constraints and boundaries set?

  • **Flexibility**: Can the prompt handle variations in input?


  📊 **Evaluation Scores**


  **Clarity Metrics:**

  • Instruction Clarity: [Score 1-5] - [Justification]

  • Task Definition: [Score 1-5] - [Justification]

  • Format Specification: [Score 1-5] - [Justification]

  • Role Definition: [Score 1-5] - [Justification]


  **Effectiveness Metrics:**

  • Consistency: [Score 1-5] - [Justification]

  • Accuracy: [Score 1-5] - [Justification]

  • Completeness: [Score 1-5] - [Justification]

  • Reliability: [Score 1-5] - [Justification]


  **Design Metrics:**

  • Structure: [Score 1-5] - [Justification]

  • Examples Usage: [Score 1-5] - [Justification]

  • Constraint Setting: [Score 1-5] - [Justification]

  • Flexibility: [Score 1-5] - [Justification]


  **Overall Prompt Score**: [Total]/60

  **Grade**: [Excellent (50-60) / Good (40-49) / Fair (30-39) / Poor (20-29) / Very
  Poor (0-19)]


  🔍 **Detailed Analysis**


  **What Works Well:**

  • [Specific strengths of the prompt]


  **Areas for Improvement:**

  • [Specific weaknesses and issues]


  **Common Response Patterns:**

  • [Patterns observed in AI responses]


  **Edge Cases & Failure Modes:**

  • [Scenarios where the prompt fails or underperforms]


  💡 **Optimization Recommendations**


  **High Priority:**

  • [Most critical improvements needed]


  **Medium Priority:**

  • [Secondary improvements that would help]


  **Optimization Techniques:**

  • [Specific prompt engineering techniques to apply]


  **Revised Prompt Suggestion:**

  ```

  [Provide an improved version of the prompt if significant changes are recommended]

  ```'
input_variables:
- prompt_template
- test_cases
- evaluation_context
optional_variables: []
metadata:
  use_case: prompt_optimization
  complexity: high
  domain: evaluation
  description: Comprehensive evaluation template for prompt effectiveness and optimization
  tags:
  - prompt_engineering
  - evaluation
  - optimization
  - testing
  - effectiveness
  author: LlamaFarm Team
  examples:
  - description: Evaluating a summarization prompt
    input:
      prompt_template: 'Summarize the following text in 3 sentences: {{ text }}'
      test_cases: 'Test 1: Long research paper -> Generated a 2-sentence summary missing
        key points

        Test 2: News article -> Generated a 4-sentence summary with good coverage

        Test 3: Technical documentation -> Generated unclear summary with jargon'
      evaluation_context: Testing for general document summarization across different
        text types
    expected_output: '**Overall Prompt Score**: 35/60 - Fair

      **Key Issues**: Inconsistent length adherence, lacks domain adaptation

      **Recommendations**: Add length constraints, specify clarity requirements, include
      examples'
validation_rules:
  prompt_template:
    type: str
    min_length: 10
    max_length: 5000
    required: true
  test_cases:
    type: str
    min_length: 20
    max_length: 10000
    required: true
  evaluation_context:
    type: str
    min_length: 10
    max_length: 1000
    required: true
