{
  "template_id": "prompt_evaluation",
  "name": "Prompt Effectiveness Evaluation",
  "type": "domain_specific",
  "template": "# Prompt Effectiveness Evaluation\n\n## Prompt Being Evaluated\n```\n{{ prompt_template }}\n```\n\n## Test Cases and Responses\n{{ test_cases }}\n\n## Evaluation Context\n{{ evaluation_context }}\n\n📝 **PROMPT EVALUATION FRAMEWORK**\n\n🎯 **Clarity & Instructions**\n• **Instruction Clarity**: Are the instructions clear and unambiguous?\n• **Task Definition**: Is the desired task/output clearly defined?\n• **Format Specification**: Are output format requirements clearly stated?\n• **Role Definition**: Is the AI's role or persona clearly established?\n\n🧠 **Cognitive Load & Complexity**\n• **Cognitive Demand**: How much reasoning does the prompt require?\n• **Context Length**: Is the prompt appropriately concise yet comprehensive?\n• **Step Breakdown**: Are complex tasks broken into manageable steps?\n• **Mental Model**: Does the prompt help establish the right mental framework?\n\n⚡ **Performance Effectiveness**\n• **Consistency**: Does the prompt produce consistent outputs across similar inputs?\n• **Accuracy**: How accurate are the responses generated by this prompt?\n• **Completeness**: Do responses fully address the intended requirements?\n• **Reliability**: How reliably does the prompt work across different scenarios?\n\n🎨 **Design Quality**\n• **Structure**: Is the prompt well-organized and logically structured?\n• **Examples**: Are examples provided where helpful?\n• **Constraints**: Are appropriate constraints and boundaries set?\n• **Flexibility**: Can the prompt handle variations in input?\n\n📊 **Evaluation Scores**\n\n**Clarity Metrics:**\n• Instruction Clarity: [Score 1-5] - [Justification]\n• Task Definition: [Score 1-5] - [Justification]\n• Format Specification: [Score 1-5] - [Justification]\n• Role Definition: [Score 1-5] - [Justification]\n\n**Effectiveness Metrics:**\n• Consistency: [Score 1-5] - [Justification]\n• Accuracy: [Score 1-5] - [Justification]\n• Completeness: [Score 1-5] - [Justification]\n• Reliability: [Score 1-5] - [Justification]\n\n**Design Metrics:**\n• Structure: [Score 1-5] - [Justification]\n• Examples Usage: [Score 1-5] - [Justification]\n• Constraint Setting: [Score 1-5] - [Justification]\n• Flexibility: [Score 1-5] - [Justification]\n\n**Overall Prompt Score**: [Total]/60\n**Grade**: [Excellent (50-60) / Good (40-49) / Fair (30-39) / Poor (20-29) / Very Poor (0-19)]\n\n🔍 **Detailed Analysis**\n\n**What Works Well:**\n• [Specific strengths of the prompt]\n\n**Areas for Improvement:**\n• [Specific weaknesses and issues]\n\n**Common Response Patterns:**\n• [Patterns observed in AI responses]\n\n**Edge Cases & Failure Modes:**\n• [Scenarios where the prompt fails or underperforms]\n\n💡 **Optimization Recommendations**\n\n**High Priority:**\n• [Most critical improvements needed]\n\n**Medium Priority:**\n• [Secondary improvements that would help]\n\n**Optimization Techniques:**\n• [Specific prompt engineering techniques to apply]\n\n**Revised Prompt Suggestion:**\n```\n[Provide an improved version of the prompt if significant changes are recommended]\n```",
  "input_variables": ["prompt_template", "test_cases", "evaluation_context"],
  "optional_variables": [],
  "metadata": {
    "use_case": "prompt_optimization",
    "complexity": "high",
    "domain": "evaluation",
    "description": "Comprehensive evaluation template for prompt effectiveness and optimization",
    "tags": ["prompt_engineering", "evaluation", "optimization", "testing", "effectiveness"],
    "author": "LlamaFarm Team",
    "examples": [
      {
        "description": "Evaluating a summarization prompt",
        "input": {
          "prompt_template": "Summarize the following text in 3 sentences: {{ text }}",
          "test_cases": "Test 1: Long research paper -> Generated a 2-sentence summary missing key points\nTest 2: News article -> Generated a 4-sentence summary with good coverage\nTest 3: Technical documentation -> Generated unclear summary with jargon",
          "evaluation_context": "Testing for general document summarization across different text types"
        },
        "expected_output": "**Overall Prompt Score**: 35/60 - Fair\n**Key Issues**: Inconsistent length adherence, lacks domain adaptation\n**Recommendations**: Add length constraints, specify clarity requirements, include examples"
      }
    ]
  },
  "validation_rules": {
    "prompt_template": {
      "type": "str",
      "min_length": 10,
      "max_length": 5000,
      "required": true
    },
    "test_cases": {
      "type": "str",
      "min_length": 20,
      "max_length": 10000,
      "required": true
    },
    "evaluation_context": {
      "type": "str",
      "min_length": 10,
      "max_length": 1000,
      "required": true
    }
  }
}