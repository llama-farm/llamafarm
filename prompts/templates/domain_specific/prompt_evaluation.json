{
  "template_id": "prompt_evaluation",
  "name": "Prompt Effectiveness Evaluation",
  "type": "domain_specific",
  "template": "# Prompt Effectiveness Evaluation\n\n## Prompt Being Evaluated\n```\n{{ prompt_template }}\n```\n\n## Test Cases and Responses\n{{ test_cases }}\n\n## Evaluation Context\n{{ evaluation_context }}\n\nðŸ“ **PROMPT EVALUATION FRAMEWORK**\n\nðŸŽ¯ **Clarity & Instructions**\nâ€¢ **Instruction Clarity**: Are the instructions clear and unambiguous?\nâ€¢ **Task Definition**: Is the desired task/output clearly defined?\nâ€¢ **Format Specification**: Are output format requirements clearly stated?\nâ€¢ **Role Definition**: Is the AI's role or persona clearly established?\n\nðŸ§  **Cognitive Load & Complexity**\nâ€¢ **Cognitive Demand**: How much reasoning does the prompt require?\nâ€¢ **Context Length**: Is the prompt appropriately concise yet comprehensive?\nâ€¢ **Step Breakdown**: Are complex tasks broken into manageable steps?\nâ€¢ **Mental Model**: Does the prompt help establish the right mental framework?\n\nâš¡ **Performance Effectiveness**\nâ€¢ **Consistency**: Does the prompt produce consistent outputs across similar inputs?\nâ€¢ **Accuracy**: How accurate are the responses generated by this prompt?\nâ€¢ **Completeness**: Do responses fully address the intended requirements?\nâ€¢ **Reliability**: How reliably does the prompt work across different scenarios?\n\nðŸŽ¨ **Design Quality**\nâ€¢ **Structure**: Is the prompt well-organized and logically structured?\nâ€¢ **Examples**: Are examples provided where helpful?\nâ€¢ **Constraints**: Are appropriate constraints and boundaries set?\nâ€¢ **Flexibility**: Can the prompt handle variations in input?\n\nðŸ“Š **Evaluation Scores**\n\n**Clarity Metrics:**\nâ€¢ Instruction Clarity: [Score 1-5] - [Justification]\nâ€¢ Task Definition: [Score 1-5] - [Justification]\nâ€¢ Format Specification: [Score 1-5] - [Justification]\nâ€¢ Role Definition: [Score 1-5] - [Justification]\n\n**Effectiveness Metrics:**\nâ€¢ Consistency: [Score 1-5] - [Justification]\nâ€¢ Accuracy: [Score 1-5] - [Justification]\nâ€¢ Completeness: [Score 1-5] - [Justification]\nâ€¢ Reliability: [Score 1-5] - [Justification]\n\n**Design Metrics:**\nâ€¢ Structure: [Score 1-5] - [Justification]\nâ€¢ Examples Usage: [Score 1-5] - [Justification]\nâ€¢ Constraint Setting: [Score 1-5] - [Justification]\nâ€¢ Flexibility: [Score 1-5] - [Justification]\n\n**Overall Prompt Score**: [Total]/60\n**Grade**: [Excellent (50-60) / Good (40-49) / Fair (30-39) / Poor (20-29) / Very Poor (0-19)]\n\nðŸ” **Detailed Analysis**\n\n**What Works Well:**\nâ€¢ [Specific strengths of the prompt]\n\n**Areas for Improvement:**\nâ€¢ [Specific weaknesses and issues]\n\n**Common Response Patterns:**\nâ€¢ [Patterns observed in AI responses]\n\n**Edge Cases & Failure Modes:**\nâ€¢ [Scenarios where the prompt fails or underperforms]\n\nðŸ’¡ **Optimization Recommendations**\n\n**High Priority:**\nâ€¢ [Most critical improvements needed]\n\n**Medium Priority:**\nâ€¢ [Secondary improvements that would help]\n\n**Optimization Techniques:**\nâ€¢ [Specific prompt engineering techniques to apply]\n\n**Revised Prompt Suggestion:**\n```\n[Provide an improved version of the prompt if significant changes are recommended]\n```",
  "input_variables": ["prompt_template", "test_cases", "evaluation_context"],
  "optional_variables": [],
  "metadata": {
    "use_case": "prompt_optimization",
    "complexity": "high",
    "domain": "evaluation",
    "description": "Comprehensive evaluation template for prompt effectiveness and optimization",
    "tags": ["prompt_engineering", "evaluation", "optimization", "testing", "effectiveness"],
    "author": "LlamaFarm Team",
    "examples": [
      {
        "description": "Evaluating a summarization prompt",
        "input": {
          "prompt_template": "Summarize the following text in 3 sentences: {{ text }}",
          "test_cases": "Test 1: Long research paper -> Generated a 2-sentence summary missing key points\nTest 2: News article -> Generated a 4-sentence summary with good coverage\nTest 3: Technical documentation -> Generated unclear summary with jargon",
          "evaluation_context": "Testing for general document summarization across different text types"
        },
        "expected_output": "**Overall Prompt Score**: 35/60 - Fair\n**Key Issues**: Inconsistent length adherence, lacks domain adaptation\n**Recommendations**: Add length constraints, specify clarity requirements, include examples"
      }
    ]
  },
  "validation_rules": {
    "prompt_template": {
      "type": "str",
      "min_length": 10,
      "max_length": 5000,
      "required": true
    },
    "test_cases": {
      "type": "str",
      "min_length": 20,
      "max_length": 10000,
      "required": true
    },
    "evaluation_context": {
      "type": "str",
      "min_length": 10,
      "max_length": 1000,
      "required": true
    }
  }
}