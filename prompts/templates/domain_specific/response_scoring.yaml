template_id: response_scoring
name: Response Quality Scoring
type: domain_specific
template: "# Response Quality Scoring\n\n## Original Query\n{{ query }}\n\n## AI Response\
  \ to Score\n{{ response }}\n\n## Reference Materials (if available)\n{{ reference_materials\
  \ | format_documents }}\n\n## Scoring Criteria\n{{ scoring_criteria }}\n\n\U0001F3C6\
  \ **RESPONSE QUALITY SCORING**\n\n\U0001F4CB **Core Quality Dimensions**\n\n**1.\
  \ Content Quality**\n• **Accuracy**: Is the information factually correct and reliable?\n\
  • **Relevance**: Does the response directly address the query?\n• **Depth**: How\
  \ thorough and comprehensive is the response?\n• **Currency**: Is the information\
  \ up-to-date and current?\n\n**2. Communication Quality** \n• **Clarity**: Is the\
  \ response clear and easy to understand?\n• **Structure**: Is the response well-organized\
  \ and logical?\n• **Conciseness**: Is the response appropriately concise without\
  \ losing important information?\n• **Tone**: Is the tone appropriate for the context\
  \ and audience?\n\n**3. Technical Quality**\n• **Coherence**: Does the response\
  \ flow logically from point to point?\n• **Completeness**: Are all aspects of the\
  \ query addressed?\n• **Citations**: Are sources properly referenced when applicable?\n\
  • **Formatting**: Is the response well-formatted and readable?\n\n**4. Utility &\
  \ Actionability**\n• **Usefulness**: How helpful is the response to the user?\n\
  • **Actionability**: Does the response provide actionable insights or next steps?\n\
  • **Practical Value**: Can the user apply this information effectively?\n• **Follow-up\
  \ Guidance**: Does it help the user understand next steps?\n\n\U0001F4CA **Detailed\
  \ Scoring Matrix**\n\n**Content Quality Scores:**\n• Accuracy: [Score 1-10] - [Specific\
  \ reasoning]\n• Relevance: [Score 1-10] - [Specific reasoning]\n• Depth: [Score\
  \ 1-10] - [Specific reasoning]\n• Currency: [Score 1-10] - [Specific reasoning]\n\
  \n**Communication Quality Scores:**\n• Clarity: [Score 1-10] - [Specific reasoning]\n\
  • Structure: [Score 1-10] - [Specific reasoning]\n• Conciseness: [Score 1-10] -\
  \ [Specific reasoning]\n• Tone: [Score 1-10] - [Specific reasoning]\n\n**Technical\
  \ Quality Scores:**\n• Coherence: [Score 1-10] - [Specific reasoning]\n• Completeness:\
  \ [Score 1-10] - [Specific reasoning]\n• Citations: [Score 1-10] - [Specific reasoning]\n\
  • Formatting: [Score 1-10] - [Specific reasoning]\n\n**Utility & Actionability Scores:**\n\
  • Usefulness: [Score 1-10] - [Specific reasoning]\n• Actionability: [Score 1-10]\
  \ - [Specific reasoning]\n• Practical Value: [Score 1-10] - [Specific reasoning]\n\
  • Follow-up Guidance: [Score 1-10] - [Specific reasoning]\n\n\U0001F4C8 **Summary\
  \ Scores**\n• **Content Quality**: [Total]/40 ([Percentage]%)\n• **Communication\
  \ Quality**: [Total]/40 ([Percentage]%)\n• **Technical Quality**: [Total]/40 ([Percentage]%)\n\
  • **Utility & Actionability**: [Total]/40 ([Percentage]%)\n\n**OVERALL RESPONSE\
  \ SCORE**: [Total]/160 ([Percentage]%)\n\n**Quality Grade**: \n• 90-100%: Exceptional\
  \ ⭐⭐⭐⭐⭐\n• 80-89%: Excellent ⭐⭐⭐⭐\n• 70-79%: Good ⭐⭐⭐\n• 60-69%: Fair ⭐⭐\n• Below\
  \ 60%: Needs Improvement ⭐\n\n\U0001F4A1 **Qualitative Assessment**\n\n**Key Strengths:**\n\
  • [What the response does exceptionally well]\n\n**Areas for Improvement:**\n• [Specific\
  \ areas that need enhancement]\n\n**Missing Elements:**\n• [Important information\
  \ or aspects not addressed]\n\n**Standout Features:**\n• [Particularly impressive\
  \ or noteworthy aspects]\n\n\U0001F3AF **Improvement Recommendations**\n\n**High\
  \ Priority:**\n• [Most critical improvements needed]\n\n**Medium Priority:**\n•\
  \ [Secondary improvements that would enhance quality]\n\n**Enhancement Suggestions:**\n\
  • [Specific ways to make the response even better]\n\n**Comparative Analysis:**\n\
  • How does this response compare to typical responses for similar queries?\n• What\
  \ would make this response exemplary?"
input_variables:
- query
- response
- scoring_criteria
optional_variables:
- reference_materials
metadata:
  use_case: response_evaluation
  complexity: medium
  domain: evaluation
  description: Comprehensive scoring template for AI response quality assessment
  tags:
  - scoring
  - quality_assessment
  - response_evaluation
  - metrics
  - benchmarking
  author: LlamaFarm Team
  examples:
  - description: Scoring a technical explanation response
    input:
      query: How does machine learning work?
      response: Machine learning is a type of AI where computers learn patterns from
        data without being explicitly programmed. It works by training algorithms
        on large datasets to recognize patterns and make predictions.
      scoring_criteria: Technical accuracy, clarity for general audience, completeness
        of explanation
      reference_materials:
      - title: ML Fundamentals
        content: Machine learning involves training algorithms on data to make predictions
          or decisions without explicit programming.
    expected_output: '**OVERALL RESPONSE SCORE**: 112/160 (70%)

      **Quality Grade**: Good ⭐⭐⭐

      **Key Strengths**: Clear explanation, accurate basics

      **Areas for Improvement**: Could include examples, more depth on algorithm types'
validation_rules:
  query:
    type: str
    min_length: 5
    max_length: 1000
    required: true
  response:
    type: str
    min_length: 10
    max_length: 10000
    required: true
  scoring_criteria:
    type: str
    min_length: 10
    max_length: 1000
    required: true
  reference_materials:
    type: list
    required: false
