{
  "template_id": "response_scoring",
  "name": "Response Quality Scoring",
  "type": "domain_specific",
  "template": "# Response Quality Scoring\n\n## Original Query\n{{ query }}\n\n## AI Response to Score\n{{ response }}\n\n## Reference Materials (if available)\n{{ reference_materials | format_documents }}\n\n## Scoring Criteria\n{{ scoring_criteria }}\n\n沛 **RESPONSE QUALITY SCORING**\n\n沒 **Core Quality Dimensions**\n\n**1. Content Quality**\n窶｢ **Accuracy**: Is the information factually correct and reliable?\n窶｢ **Relevance**: Does the response directly address the query?\n窶｢ **Depth**: How thorough and comprehensive is the response?\n窶｢ **Currency**: Is the information up-to-date and current?\n\n**2. Communication Quality** \n窶｢ **Clarity**: Is the response clear and easy to understand?\n窶｢ **Structure**: Is the response well-organized and logical?\n窶｢ **Conciseness**: Is the response appropriately concise without losing important information?\n窶｢ **Tone**: Is the tone appropriate for the context and audience?\n\n**3. Technical Quality**\n窶｢ **Coherence**: Does the response flow logically from point to point?\n窶｢ **Completeness**: Are all aspects of the query addressed?\n窶｢ **Citations**: Are sources properly referenced when applicable?\n窶｢ **Formatting**: Is the response well-formatted and readable?\n\n**4. Utility & Actionability**\n窶｢ **Usefulness**: How helpful is the response to the user?\n窶｢ **Actionability**: Does the response provide actionable insights or next steps?\n窶｢ **Practical Value**: Can the user apply this information effectively?\n窶｢ **Follow-up Guidance**: Does it help the user understand next steps?\n\n沒 **Detailed Scoring Matrix**\n\n**Content Quality Scores:**\n窶｢ Accuracy: [Score 1-10] - [Specific reasoning]\n窶｢ Relevance: [Score 1-10] - [Specific reasoning]\n窶｢ Depth: [Score 1-10] - [Specific reasoning]\n窶｢ Currency: [Score 1-10] - [Specific reasoning]\n\n**Communication Quality Scores:**\n窶｢ Clarity: [Score 1-10] - [Specific reasoning]\n窶｢ Structure: [Score 1-10] - [Specific reasoning]\n窶｢ Conciseness: [Score 1-10] - [Specific reasoning]\n窶｢ Tone: [Score 1-10] - [Specific reasoning]\n\n**Technical Quality Scores:**\n窶｢ Coherence: [Score 1-10] - [Specific reasoning]\n窶｢ Completeness: [Score 1-10] - [Specific reasoning]\n窶｢ Citations: [Score 1-10] - [Specific reasoning]\n窶｢ Formatting: [Score 1-10] - [Specific reasoning]\n\n**Utility & Actionability Scores:**\n窶｢ Usefulness: [Score 1-10] - [Specific reasoning]\n窶｢ Actionability: [Score 1-10] - [Specific reasoning]\n窶｢ Practical Value: [Score 1-10] - [Specific reasoning]\n窶｢ Follow-up Guidance: [Score 1-10] - [Specific reasoning]\n\n沒 **Summary Scores**\n窶｢ **Content Quality**: [Total]/40 ([Percentage]%)\n窶｢ **Communication Quality**: [Total]/40 ([Percentage]%)\n窶｢ **Technical Quality**: [Total]/40 ([Percentage]%)\n窶｢ **Utility & Actionability**: [Total]/40 ([Percentage]%)\n\n**OVERALL RESPONSE SCORE**: [Total]/160 ([Percentage]%)\n\n**Quality Grade**: \n窶｢ 90-100%: Exceptional 箝絶ｭ絶ｭ絶ｭ絶ｭ申n窶｢ 80-89%: Excellent 箝絶ｭ絶ｭ絶ｭ申n窶｢ 70-79%: Good 箝絶ｭ絶ｭ申n窶｢ 60-69%: Fair 箝絶ｭ申n窶｢ Below 60%: Needs Improvement 箝申n\n汳｡ **Qualitative Assessment**\n\n**Key Strengths:**\n窶｢ [What the response does exceptionally well]\n\n**Areas for Improvement:**\n窶｢ [Specific areas that need enhancement]\n\n**Missing Elements:**\n窶｢ [Important information or aspects not addressed]\n\n**Standout Features:**\n窶｢ [Particularly impressive or noteworthy aspects]\n\n沁ｯ **Improvement Recommendations**\n\n**High Priority:**\n窶｢ [Most critical improvements needed]\n\n**Medium Priority:**\n窶｢ [Secondary improvements that would enhance quality]\n\n**Enhancement Suggestions:**\n窶｢ [Specific ways to make the response even better]\n\n**Comparative Analysis:**\n窶｢ How does this response compare to typical responses for similar queries?\n窶｢ What would make this response exemplary?",
  "input_variables": ["query", "response", "scoring_criteria"],
  "optional_variables": ["reference_materials"],
  "metadata": {
    "use_case": "response_evaluation",
    "complexity": "medium",
    "domain": "evaluation",
    "description": "Comprehensive scoring template for AI response quality assessment",
    "tags": ["scoring", "quality_assessment", "response_evaluation", "metrics", "benchmarking"],
    "author": "LlamaFarm Team",
    "examples": [
      {
        "description": "Scoring a technical explanation response",
        "input": {
          "query": "How does machine learning work?",
          "response": "Machine learning is a type of AI where computers learn patterns from data without being explicitly programmed. It works by training algorithms on large datasets to recognize patterns and make predictions.",
          "scoring_criteria": "Technical accuracy, clarity for general audience, completeness of explanation",
          "reference_materials": [
            {
              "title": "ML Fundamentals",
              "content": "Machine learning involves training algorithms on data to make predictions or decisions without explicit programming."
            }
          ]
        },
        "expected_output": "**OVERALL RESPONSE SCORE**: 112/160 (70%)\n**Quality Grade**: Good 箝絶ｭ絶ｭ申n**Key Strengths**: Clear explanation, accurate basics\n**Areas for Improvement**: Could include examples, more depth on algorithm types"
      }
    ]
  },
  "validation_rules": {
    "query": {
      "type": "str",
      "min_length": 5,
      "max_length": 1000,
      "required": true
    },
    "response": {
      "type": "str",
      "min_length": 10,
      "max_length": 10000,
      "required": true
    },
    "scoring_criteria": {
      "type": "str",
      "min_length": 10,
      "max_length": 1000,
      "required": true
    },
    "reference_materials": {
      "type": "list",
      "required": false
    }
  }
}