# LlamaFarm Models - Use Case Specific Configurations
# Optimized configurations for different use cases and domains

name: "LlamaFarm Models - Use Case Examples"
version: "1.0.0"
description: "Configurations optimized for specific use cases and domains"

# =============================================================================
# CUSTOMER SUPPORT USE CASE
# =============================================================================

customer_support:
  name: "Customer Support Configuration"
  description: "Optimized for customer support with fast, helpful responses"
  
  default_provider: "openai_gpt4o_mini"
  fallback_chain: 
    - "openai_gpt4o_mini"
    - "anthropic_claude_3_haiku"
    - "ollama_llama3_1_8b"
  
  providers:
    openai_gpt4o_mini:
      type: "cloud"
      provider: "openai"
      model: "gpt-4o-mini"
      api_key: "${OPENAI_API_KEY}"
      max_tokens: 1024              # Concise responses
      temperature: 0.3              # Consistent, helpful tone
      description: "Primary support model - fast and reliable"
    
    anthropic_claude_3_haiku:
      type: "cloud"
      provider: "anthropic"
      model: "claude-3-haiku-20240307"
      api_key: "${ANTHROPIC_API_KEY}"
      max_tokens: 1024
      temperature: 0.3
      description: "Alternative for when OpenAI is unavailable"
    
    ollama_llama3_1_8b:
      type: "local"
      provider: "ollama"
      model: "llama3.1:8b"
      host: "${OLLAMA_HOST:localhost}"
      port: 11434
      timeout: 60
      temperature: 0.3
      description: "Local fallback for privacy-sensitive queries"
  
  monitoring:
    track_usage: true
    response_time_alert_ms: 3000    # Alert if responses take >3s
    cost_alert_threshold: 20.0      # Alert at $20/day
    track_satisfaction_metrics: true

# =============================================================================
# CODE GENERATION USE CASE
# =============================================================================

code_generation:
  name: "Code Generation Configuration"
  description: "Optimized for programming and code-related tasks"
  
  default_provider: "openai_gpt4_turbo"
  fallback_chain:
    - "openai_gpt4_turbo"
    - "anthropic_claude_3_sonnet"
    - "ollama_codellama_13b"
  
  providers:
    openai_gpt4_turbo:
      type: "cloud"
      provider: "openai"
      model: "gpt-4-turbo-preview"
      api_key: "${OPENAI_API_KEY}"
      max_tokens: 4096              # Longer for complete code examples
      temperature: 0.1              # Low temp for consistent code
      description: "Primary code generation model"
    
    anthropic_claude_3_sonnet:
      type: "cloud"
      provider: "anthropic"
      model: "claude-3-sonnet-20240229"
      api_key: "${ANTHROPIC_API_KEY}"
      max_tokens: 4096
      temperature: 0.1
      description: "Alternative for complex coding tasks"
    
    ollama_codellama_13b:
      type: "local"
      provider: "ollama"
      model: "codellama:13b"
      host: "${OLLAMA_HOST:localhost}"
      port: 11434
      timeout: 180
      temperature: 0.1
      description: "Specialized local code model"

# =============================================================================
# CONTENT CREATION USE CASE
# =============================================================================

content_creation:
  name: "Content Creation Configuration"
  description: "Optimized for creative writing, marketing, and content generation"
  
  default_provider: "anthropic_claude_3_sonnet"
  fallback_chain:
    - "anthropic_claude_3_sonnet"
    - "openai_gpt4_turbo"
    - "together_mixtral_8x7b"
  
  providers:
    anthropic_claude_3_sonnet:
      type: "cloud"
      provider: "anthropic"
      model: "claude-3-sonnet-20240229"
      api_key: "${ANTHROPIC_API_KEY}"
      max_tokens: 4096
      temperature: 0.8              # Higher creativity
      description: "Primary creative writing model"
    
    openai_gpt4_turbo:
      type: "cloud"
      provider: "openai"
      model: "gpt-4-turbo-preview"
      api_key: "${OPENAI_API_KEY}"
      max_tokens: 4096
      temperature: 0.8
      description: "Alternative for diverse content styles"
    
    together_mixtral_8x7b:
      type: "cloud"
      provider: "together"
      model: "mistralai/Mixtral-8x7B-Instruct-v0.1"
      api_key: "${TOGETHER_API_KEY}"
      max_tokens: 4096
      temperature: 0.8
      description: "Cost-effective alternative for bulk content"

# =============================================================================
# DATA ANALYSIS USE CASE
# =============================================================================

data_analysis:
  name: "Data Analysis Configuration"
  description: "Optimized for data analysis, research, and analytical tasks"
  
  default_provider: "anthropic_claude_3_opus"
  fallback_chain:
    - "anthropic_claude_3_opus"
    - "openai_gpt4_turbo"
    - "together_llama3_70b"
  
  providers:
    anthropic_claude_3_opus:
      type: "cloud"
      provider: "anthropic"
      model: "claude-3-opus-20240229"
      api_key: "${ANTHROPIC_API_KEY}"
      max_tokens: 4096
      temperature: 0.2              # Lower temp for analytical accuracy
      description: "Most capable model for complex analysis"
    
    openai_gpt4_turbo:
      type: "cloud"
      provider: "openai"
      model: "gpt-4-turbo-preview"
      api_key: "${OPENAI_API_KEY}"
      max_tokens: 4096
      temperature: 0.2
      description: "Alternative for data interpretation"
    
    together_llama3_70b:
      type: "cloud"
      provider: "together"
      model: "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
      api_key: "${TOGETHER_API_KEY}"
      max_tokens: 4096
      temperature: 0.2
      description: "Cost-effective for large-scale analysis"

# =============================================================================
# REAL-TIME CHAT USE CASE
# =============================================================================

real_time_chat:
  name: "Real-Time Chat Configuration"
  description: "Optimized for fast, interactive chat applications"
  
  default_provider: "groq_llama3_70b"
  fallback_chain:
    - "groq_llama3_70b"
    - "groq_mixtral_8x7b"
    - "openai_gpt4o_mini"
  
  providers:
    groq_llama3_70b:
      type: "cloud"
      provider: "groq"
      model: "llama3-70b-8192"
      api_key: "${GROQ_API_KEY}"
      max_tokens: 2048
      temperature: 0.7
      timeout: 10                   # Very fast responses required
      description: "Ultra-fast inference for real-time chat"
    
    groq_mixtral_8x7b:
      type: "cloud"
      provider: "groq"
      model: "mixtral-8x7b-32768"
      api_key: "${GROQ_API_KEY}"
      max_tokens: 2048
      temperature: 0.7
      timeout: 10
      description: "Alternative fast model with large context"
    
    openai_gpt4o_mini:
      type: "cloud"
      provider: "openai"
      model: "gpt-4o-mini"
      api_key: "${OPENAI_API_KEY}"
      max_tokens: 2048
      temperature: 0.7
      timeout: 15
      description: "Reliable fallback for chat"

# =============================================================================
# PRIVACY-FIRST USE CASE
# =============================================================================

privacy_first:
  name: "Privacy-First Configuration"
  description: "Only local models for maximum privacy and data security"
  
  default_provider: "ollama_llama3_1_8b"
  fallback_chain:
    - "ollama_llama3_1_8b"
    - "ollama_mistral_7b"
    - "hf_flan_t5_base"
  
  providers:
    ollama_llama3_1_8b:
      type: "local"
      provider: "ollama"
      model: "llama3.1:8b"
      host: "${OLLAMA_HOST:localhost}"
      port: 11434
      timeout: 120
      temperature: 0.7
      description: "Primary local model - no data leaves your system"
    
    ollama_mistral_7b:
      type: "local"
      provider: "ollama"
      model: "mistral:7b"
      host: "${OLLAMA_HOST:localhost}"
      port: 11434
      timeout: 90
      temperature: 0.7
      description: "Alternative local model family"
    
    hf_flan_t5_base:
      type: "local"
      provider: "huggingface"
      model: "google/flan-t5-base"
      cache_dir: "~/.cache/huggingface/hub"
      device: "cpu"
      max_tokens: 512
      temperature: 0.7
      description: "Lightweight local model option"
  
  security:
    encrypt_requests: true          # Encrypt all local communications
    no_telemetry: true             # Disable all telemetry
    audit_logging: true            # Log all requests locally

# =============================================================================
# MULTILINGUAL USE CASE
# =============================================================================

multilingual:
  name: "Multilingual Configuration"
  description: "Optimized for multiple languages and international use"
  
  default_provider: "anthropic_claude_3_sonnet"
  fallback_chain:
    - "anthropic_claude_3_sonnet"
    - "openai_gpt4_turbo"
    - "ollama_qwen2_7b"
  
  providers:
    anthropic_claude_3_sonnet:
      type: "cloud"
      provider: "anthropic"
      model: "claude-3-sonnet-20240229"
      api_key: "${ANTHROPIC_API_KEY}"
      max_tokens: 4096
      temperature: 0.7
      description: "Excellent multilingual capabilities"
    
    openai_gpt4_turbo:
      type: "cloud"
      provider: "openai"
      model: "gpt-4-turbo-preview"
      api_key: "${OPENAI_API_KEY}"
      max_tokens: 4096
      temperature: 0.7
      description: "Strong multilingual support"
    
    ollama_qwen2_7b:
      type: "local"
      provider: "ollama"
      model: "qwen2:7b"
      host: "${OLLAMA_HOST:localhost}"
      port: 11434
      timeout: 90
      temperature: 0.7
      description: "Local multilingual model, strong in Chinese/English"

# =============================================================================
# COST-OPTIMIZED USE CASE
# =============================================================================

cost_optimized:
  name: "Cost-Optimized Configuration"
  description: "Minimize costs while maintaining reasonable quality"
  
  default_provider: "groq_gemma_7b"
  fallback_chain:
    - "groq_gemma_7b"
    - "openai_gpt4o_mini"
    - "ollama_phi3_mini"
  
  providers:
    groq_gemma_7b:
      type: "cloud"
      provider: "groq"
      model: "gemma-7b-it"
      api_key: "${GROQ_API_KEY}"
      max_tokens: 2048
      temperature: 0.7
      cost_per_1k_tokens: 0.00007   # Very low cost
      description: "Extremely affordable model with decent quality"
    
    openai_gpt4o_mini:
      type: "cloud"
      provider: "openai"
      model: "gpt-4o-mini"
      api_key: "${OPENAI_API_KEY}"
      max_tokens: 2048
      temperature: 0.7
      cost_per_1k_tokens: 0.00015
      description: "Cost-effective OpenAI model"
    
    ollama_phi3_mini:
      type: "local"
      provider: "ollama"
      model: "phi3:mini"
      host: "${OLLAMA_HOST:localhost}"
      port: 11434
      timeout: 30
      temperature: 0.7
      description: "Free local model - no API costs"
  
  monitoring:
    cost_alert_threshold: 5.0       # Alert at $5/day
    track_cost_per_request: true
    auto_switch_on_budget: true     # Switch to cheaper models when approaching budget

# =============================================================================
# USAGE EXAMPLES IN COMMENTS
# =============================================================================

# Example usage for different use cases:
#
# Customer Support:
#   uv run python cli.py query "How do I reset my password?" --config config/use_case_examples.yaml --use-case customer_support
#
# Code Generation:
#   uv run python cli.py query "Write a Python function to sort a list" --config config/use_case_examples.yaml --use-case code_generation
#
# Content Creation:
#   uv run python cli.py query "Write a blog post about AI" --config config/use_case_examples.yaml --use-case content_creation
#
# Data Analysis:
#   uv run python cli.py query "Analyze this sales data" --config config/use_case_examples.yaml --use-case data_analysis
#
# Real-time Chat:
#   uv run python cli.py chat --config config/use_case_examples.yaml --use-case real_time_chat
#
# Privacy-First:
#   uv run python cli.py query "Sensitive question" --config config/use_case_examples.yaml --use-case privacy_first
#
# Multilingual:
#   uv run python cli.py query "¿Cómo estás?" --config config/use_case_examples.yaml --use-case multilingual
#
# Cost-Optimized:
#   uv run python cli.py batch questions.txt --config config/use_case_examples.yaml --use-case cost_optimized