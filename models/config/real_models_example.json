{
  "name": "LlamaFarm Production Models Configuration",
  "version": "1.0.0",
  "description": "Comprehensive configuration with real cloud and local models",
  "default_provider": "openai_gpt4o_mini",
  "fallback_chain": ["openai_gpt4o_mini", "anthropic_claude_3_haiku", "ollama_llama3_1"],
  
  "providers": {
    "openai_gpt4o_mini": {
      "type": "cloud",
      "provider": "openai",
      "model": "gpt-4o-mini",
      "api_key": "${OPENAI_API_KEY}",
      "base_url": "https://api.openai.com/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "top_p": 0.9,
      "frequency_penalty": 0,
      "presence_penalty": 0,
      "cost_per_1k_tokens": 0.00015,
      "description": "Fast, cost-effective model for general tasks"
    },
    
    "openai_gpt4_turbo": {
      "type": "cloud",
      "provider": "openai",
      "model": "gpt-4-turbo-preview",
      "api_key": "${OPENAI_API_KEY}",
      "base_url": "https://api.openai.com/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.01,
      "description": "Advanced reasoning and complex tasks"
    },
    
    "openai_gpt35_turbo": {
      "type": "cloud",
      "provider": "openai",
      "model": "gpt-3.5-turbo",
      "api_key": "${OPENAI_API_KEY}",
      "base_url": "https://api.openai.com/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.0005,
      "description": "Legacy fast model, good for simple tasks"
    },
    
    "anthropic_claude_3_opus": {
      "type": "cloud",
      "provider": "anthropic",
      "model": "claude-3-opus-20240229",
      "api_key": "${ANTHROPIC_API_KEY}",
      "base_url": "https://api.anthropic.com/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.015,
      "description": "Most capable Claude model"
    },
    
    "anthropic_claude_3_sonnet": {
      "type": "cloud",
      "provider": "anthropic",
      "model": "claude-3-sonnet-20240229",
      "api_key": "${ANTHROPIC_API_KEY}",
      "base_url": "https://api.anthropic.com/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.003,
      "description": "Balanced performance and cost"
    },
    
    "anthropic_claude_3_haiku": {
      "type": "cloud",
      "provider": "anthropic",
      "model": "claude-3-haiku-20240307",
      "api_key": "${ANTHROPIC_API_KEY}",
      "base_url": "https://api.anthropic.com/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.00025,
      "description": "Fast and affordable Claude model"
    },
    
    "together_llama3_70b": {
      "type": "cloud",
      "provider": "together",
      "model": "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
      "api_key": "${TOGETHER_API_KEY}",
      "base_url": "https://api.together.xyz/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.00088,
      "description": "Llama 3.1 70B via Together AI"
    },
    
    "together_mixtral_8x7b": {
      "type": "cloud",
      "provider": "together",
      "model": "mistralai/Mixtral-8x7B-Instruct-v0.1",
      "api_key": "${TOGETHER_API_KEY}",
      "base_url": "https://api.together.xyz/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.0006,
      "description": "Mixtral MoE model via Together AI"
    },
    
    "groq_llama3_70b": {
      "type": "cloud",
      "provider": "groq",
      "model": "llama3-70b-8192",
      "api_key": "${GROQ_API_KEY}",
      "base_url": "https://api.groq.com/openai/v1",
      "max_tokens": 8192,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.00064,
      "description": "Llama 3 70B on Groq LPU (very fast)"
    },
    
    "groq_mixtral_8x7b": {
      "type": "cloud",
      "provider": "groq",
      "model": "mixtral-8x7b-32768",
      "api_key": "${GROQ_API_KEY}",
      "base_url": "https://api.groq.com/openai/v1",
      "max_tokens": 32768,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.00027,
      "description": "Mixtral on Groq LPU (32k context)"
    },
    
    "cohere_command_r_plus": {
      "type": "cloud",
      "provider": "cohere",
      "model": "command-r-plus",
      "api_key": "${COHERE_API_KEY}",
      "base_url": "https://api.cohere.ai/v1",
      "max_tokens": 4096,
      "temperature": 0.7,
      "cost_per_1k_tokens": 0.003,
      "description": "Cohere's most capable model"
    },
    
    "ollama_llama3_1": {
      "type": "local",
      "provider": "ollama",
      "model": "llama3.1:8b",
      "host": "${OLLAMA_HOST:localhost}",
      "port": 11434,
      "timeout": 120,
      "temperature": 0.7,
      "description": "Llama 3.1 8B running locally"
    },
    
    "ollama_llama3_2": {
      "type": "local",
      "provider": "ollama",
      "model": "llama3.2:3b",
      "host": "${OLLAMA_HOST:localhost}",
      "port": 11434,
      "timeout": 60,
      "temperature": 0.7,
      "description": "Llama 3.2 3B - smaller, faster local model"
    },
    
    "ollama_mistral": {
      "type": "local",
      "provider": "ollama",
      "model": "mistral:7b",
      "host": "${OLLAMA_HOST:localhost}",
      "port": 11434,
      "timeout": 90,
      "temperature": 0.7,
      "description": "Mistral 7B running locally"
    },
    
    "ollama_phi3": {
      "type": "local",
      "provider": "ollama",
      "model": "phi3:mini",
      "host": "${OLLAMA_HOST:localhost}",
      "port": 11434,
      "timeout": 30,
      "temperature": 0.7,
      "description": "Microsoft Phi-3 Mini - very fast local model"
    },
    
    "ollama_codellama": {
      "type": "local",
      "provider": "ollama",
      "model": "codellama:13b",
      "host": "${OLLAMA_HOST:localhost}",
      "port": 11434,
      "timeout": 120,
      "temperature": 0.3,
      "description": "Code Llama 13B for programming tasks"
    },
    
    "hf_gpt2": {
      "type": "local",
      "provider": "huggingface",
      "model": "gpt2",
      "cache_dir": "~/.cache/huggingface/hub",
      "device": "cpu",
      "token": "${HF_TOKEN}",
      "max_tokens": 100,
      "temperature": 0.7,
      "description": "GPT-2 base model from Hugging Face"
    },
    
    "hf_distilgpt2": {
      "type": "local",
      "provider": "huggingface",
      "model": "distilgpt2",
      "cache_dir": "~/.cache/huggingface/hub",
      "device": "cpu",
      "token": "${HF_TOKEN}",
      "max_tokens": 100,
      "temperature": 0.7,
      "description": "DistilGPT-2 - smaller, faster GPT-2"
    },
    
    "vllm_llama2_7b": {
      "type": "local",
      "provider": "vllm",
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 2048,
      "gpu_memory_utilization": 0.8,
      "trust_remote_code": true,
      "temperature": 0.7,
      "max_tokens": 512,
      "description": "Llama 2 7B Chat via vLLM (requires GPU)"
    },
    
    "tgi_endpoint": {
      "type": "local",
      "provider": "tgi",
      "endpoint": "http://localhost:8080",
      "timeout": 30,
      "temperature": 0.7,
      "max_tokens": 512,
      "description": "Text Generation Inference server endpoint"
    }
  },
  
  "model_selection": {
    "rules": [
      {
        "condition": "task_type == 'code'",
        "preferred_providers": ["openai_gpt4_turbo", "anthropic_claude_3_opus", "ollama_codellama"]
      },
      {
        "condition": "task_type == 'chat'",
        "preferred_providers": ["openai_gpt4o_mini", "anthropic_claude_3_haiku", "ollama_llama3_1"]
      },
      {
        "condition": "task_type == 'analysis'",
        "preferred_providers": ["anthropic_claude_3_sonnet", "openai_gpt4_turbo", "together_llama3_70b"]
      },
      {
        "condition": "cost_sensitive == true",
        "preferred_providers": ["openai_gpt4o_mini", "anthropic_claude_3_haiku", "groq_mixtral_8x7b", "ollama_llama3_2"]
      },
      {
        "condition": "speed_critical == true",
        "preferred_providers": ["groq_llama3_70b", "groq_mixtral_8x7b", "ollama_phi3", "openai_gpt35_turbo"]
      }
    ]
  },
  
  "rate_limits": {
    "openai": {
      "requests_per_minute": 500,
      "tokens_per_minute": 200000
    },
    "anthropic": {
      "requests_per_minute": 50,
      "tokens_per_minute": 100000
    },
    "together": {
      "requests_per_minute": 600,
      "tokens_per_minute": 1000000
    },
    "groq": {
      "requests_per_minute": 30,
      "tokens_per_minute": 20000
    }
  },
  
  "monitoring": {
    "track_usage": true,
    "log_requests": true,
    "alert_on_errors": true,
    "cost_alert_threshold": 10.0,
    "latency_alert_threshold_ms": 5000
  }
}