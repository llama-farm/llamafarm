# LlamaFarm Demo Strategies
# All demos are driven by these strategy definitions
# NO HARDCODED VALUES - everything comes from here

version: "2.0"

strategies:
  # =============================================================================
  # DEMO 1: CLOUD API WITH FALLBACK STRATEGY
  # =============================================================================
  
  demo1_cloud_fallback:
    name: "Cloud API with Automatic Fallback"
    description: "Production-grade API management with automatic fallback to local models"
    
    components:
      # Primary cloud provider
      cloud_api:
        type: openai_compatible
        config:
          provider: openai
          api_key: ${OPENAI_API_KEY}
          default_model: gpt-4o-mini
          timeout: 30
          max_retries: 3
          
      # Local fallback model
      model_app:
        type: ollama
        config:
          base_url: http://localhost:11434
          default_model: llama3.2:3b
          timeout: 60
          
    # Fallback chain - automatically tries each in order
    fallback_chain:
      - provider: cloud_api
        model: gpt-4o-mini
        conditions:
          - api_healthy
          - within_rate_limit
          
      - provider: cloud_api
        model: gpt-4o-mini  # Fast and efficient
        conditions:
          - api_healthy
          
      - provider: model_app
        model: llama3.2:3b
        conditions:
          - service_running
          
      - provider: model_app
        model: mistral:7b
        conditions:
          - service_running
          
      - provider: model_app
        model: tinyllama:1.1b
        conditions:
          - service_running
          
    # Demo-specific settings
    test_queries:
      - "What is the capital of France?"
      - "Explain quantum computing in simple terms"
      - "Write a haiku about programming"
      
    monitoring:
      track_latency: true
      track_costs: true
      track_provider: true
      log_level: INFO

  # =============================================================================
  # DEMO 2: MULTI-MODEL OPTIMIZATION STRATEGY
  # =============================================================================
  
  demo2_multi_model:
    name: "Multi-Model Task Optimization"
    description: "Intelligent routing to different models based on task complexity"
    
    components:
      # Multiple cloud providers for different tasks
      cloud_api:
        type: openai_compatible
        config:
          provider: openai
          api_key: ${OPENAI_API_KEY}
          timeout: 30
          
      # Local models for specific tasks
      model_app:
        type: ollama
        config:
          base_url: http://localhost:11434
          timeout: 60
          
    # Task-based routing rules
    routing_rules:
      simple:
        description: "Simple factual queries"
        provider: cloud_api
        model: gpt-4o-mini  # Fast and efficient
        max_tokens: 100
        temperature: 0.1
        cost_per_1k_tokens: 0.002
        
      complex:
        description: "Complex reasoning tasks"
        provider: cloud_api
        model: gpt-4o-mini
        max_tokens: 500
        temperature: 0.7
        cost_per_1k_tokens: 0.015
        
      creative:
        description: "Creative writing tasks"
        provider: cloud_api
        model: gpt-4o
        max_tokens: 1000
        temperature: 0.9
        cost_per_1k_tokens: 0.03
        
      code:
        description: "Code generation and analysis"
        provider: model_app
        model: mistral:7b
        max_tokens: 2000
        temperature: 0.3
        cost_per_1k_tokens: 0.0  # Local model
        
    # Test tasks for demonstration
    test_tasks:
      simple:
        - "What is 2+2?"
        - "What is the capital of Japan?"
        - "Define photosynthesis"
        
      complex:
        - "Explain the theory of relativity"
        - "What are the implications of quantum entanglement?"
        - "Analyze the causes of World War I"
        
      creative:
        - "Write a haiku about programming"
        - "Create a short story about AI"
        - "Compose a limerick about debugging"
        
      code:
        - "Write a Python function to calculate fibonacci"
        - "Explain this code: lambda x: x**2"
        - "Create a sorting algorithm"
        
    monitoring:
      track_model_selection: true
      track_task_routing: true
      compare_performance: true
      log_level: INFO

  # =============================================================================
  # DEMO 3: COMPLETE TRAINING PIPELINE
  # =============================================================================
  
  demo3_training:
    name: "Medical Model Fine-Tuning Pipeline"
    description: "Complete training workflow with before/after comparison and Ollama export"
    
    components:
      # Local Ollama model for base testing
      ollama:
        type: ollama
        config:
          base_url: http://localhost:11434
          model: tinyllama
          timeout: 60
          auto_pull: true  # Automatically download if not present
          
      # Fine-tuner for training
      fine_tuner:
        type: pytorch
        config:
          # ========================================
          # HARDWARE CONFIGURATION
          # ========================================
          # This section is CRITICAL for cross-platform compatibility
          # Adjust based on your hardware:
          
          hardware:
            # Device selection (auto-detects by default)
            device: auto  # Options: auto, cuda, mps, cpu
            
            # Your available memory (affects batch size and optimizations)
            memory_gb: 8  # Adjust to your system's available memory
            
            # Precision settings per device type
            precision:
              cuda:
                use_fp16: true   # Faster on NVIDIA GPUs
                use_bf16: false  # Only for RTX 30xx/40xx, A100
                use_tf32: true   # Ampere and newer GPUs
              mps:
                use_fp16: false  # Not stable on Apple Silicon
                use_bf16: false  # Not supported on MPS
              cpu:
                use_fp16: false  # Keep full precision on CPU
            
            # Memory optimizations
            optimization:
              gradient_checkpointing: false  # Disabled due to MPS compatibility
              gradient_accumulation_steps: 1  # Increase if OOM errors
              mixed_precision: no  # Set to 'fp16' for CUDA, 'no' for others
          
          # Model configuration
          base_model:
            name: TinyLlama/TinyLlama-1.1B-Chat-v1.0
            huggingface_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
            cache_dir: ./model_cache
            torch_dtype: auto  # Will be set based on hardware config
            
          method:
            type: lora
            r: 16
            alpha: 32
            dropout: 0.1
            target_modules:
              - q_proj
              - v_proj
              - k_proj
              - o_proj
              
          training_args:
            output_dir: ./fine_tuned_models/medical
            num_train_epochs: 3
            per_device_train_batch_size: 4
            per_device_eval_batch_size: 4
            gradient_accumulation_steps: 4
            learning_rate: 0.0002
            warmup_ratio: 0.1
            logging_steps: 10
            save_steps: 100
            save_total_limit: 3
            # Disable evaluation since we don't have an eval dataset
            eval_strategy: "no"
            save_strategy: "steps"
            load_best_model_at_end: false
            
      # Model app for testing
      model_app:
        type: ollama
        config:
          base_url: http://localhost:11434
          default_model: medical-model:finetuned
          
    # Export configuration
    export:
      to_ollama: true
      to_gguf: true
      quantization: q4_0  # Options: q4_0 (smallest), q5_0, q8_0, f16 (best quality)
      model_name: medical-model
      system_prompt: "You are a knowledgeable medical assistant. Provide accurate, evidence-based medical information."
      
    # Conversion settings (for fine-tuned models)
    conversion:
      # Automatic llama.cpp installation
      auto_install_llama_cpp: true
      
      # Platform-specific build options
      build_options:
        darwin:  # macOS
          metal: true  # Enable Metal acceleration for Apple Silicon
        linux:
          cuda: true   # Enable CUDA if available
          rocm: false  # Enable ROCm for AMD GPUs
        windows:
          cmake: true  # Use CMake build system
      
      # Conversion workflow
      workflow:
        - merge_lora      # Merge LoRA adapter with base model
        - convert_gguf    # Convert to GGUF format
        - quantize        # Apply quantization
        - create_ollama   # Create Ollama model
      
      # Quantization options for different use cases
      quantization_profiles:
        mobile:
          method: q4_0
          description: "Smallest size, suitable for mobile/edge devices"
        balanced:
          method: q5_0
          description: "Good balance of size and quality"
        quality:
          method: q8_0
          description: "Higher quality, larger size"
        maximum:
          method: f16
          description: "Best quality, largest size"
      
    # Prompt template for training
    prompt_template:
      system: "You are a knowledgeable medical assistant."
      format: |
        ### System:
        {system}
        
        ### User:
        {input}
        
        ### Assistant:
        {output}
        
    monitoring:
      track_training_metrics: true
      save_checkpoints: true
      log_tensorboard: true
      log_level: INFO

  # =============================================================================
  # HARDWARE-SPECIFIC TRAINING CONFIGURATIONS
  # =============================================================================
  # Example configurations optimized for different hardware setups
  
  # For NVIDIA GPU (RTX 3060/3070/3080/4060/4070/4080/4090)
  training_cuda_consumer:
    name: "NVIDIA Consumer GPU Training"
    description: "Optimized for RTX 30xx/40xx series with 8-24GB VRAM"
    components:
      fine_tuner:
        type: pytorch
        config:
          hardware:
            device: cuda
            memory_gb: 12  # Adjust to your GPU's VRAM
            precision:
              cuda:
                use_fp16: true   # Fast and memory efficient
                use_bf16: false  # Set true for RTX 30xx/40xx
                use_tf32: true   # Ampere optimization
            optimization:
              gradient_checkpointing: true  # Enable if <12GB VRAM
              gradient_accumulation_steps: 2
              mixed_precision: fp16
          base_model:
            huggingface_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
            torch_dtype: float16
          training_args:
            per_device_train_batch_size: 4  # Increase if memory allows
            
  # For Apple Silicon (M1/M2/M3 Mac)
  training_mps_apple:
    name: "Apple Silicon Training"
    description: "Optimized for M1/M2/M3 Macs with 8-64GB unified memory"
    components:
      fine_tuner:
        type: pytorch
        config:
          hardware:
            device: mps
            memory_gb: 16  # Your Mac's RAM
            precision:
              mps:
                use_fp16: false  # Not stable on MPS
                use_bf16: false  # Not supported
            optimization:
              gradient_checkpointing: false  # Usually not needed with unified memory
              gradient_accumulation_steps: 1
              mixed_precision: no  # MPS doesn't support well
          base_model:
            huggingface_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
            torch_dtype: float32  # Full precision for stability
          training_args:
            per_device_train_batch_size: 2  # Conservative for stability
            
  # For CPU-only systems (no GPU)
  training_cpu_only:
    name: "CPU-Only Training"
    description: "For systems without GPU - slow but works everywhere"
    components:
      fine_tuner:
        type: pytorch
        config:
          hardware:
            device: cpu
            memory_gb: 16  # System RAM
            precision:
              cpu:
                use_fp16: false  # CPU needs full precision
            optimization:
              gradient_checkpointing: true  # Save memory
              gradient_accumulation_steps: 4  # Simulate larger batches
              mixed_precision: no
          base_model:
            huggingface_id: TinyLlama/TinyLlama-1.1B-Chat-v1.0
            torch_dtype: float32
          training_args:
            per_device_train_batch_size: 1  # Minimal batch size
            num_train_epochs: 1  # Fewer epochs for CPU
            
  # For High-End GPUs (A100, H100, RTX 4090)
  training_cuda_datacenter:
    name: "Datacenter GPU Training"
    description: "For A100/H100 or high-end RTX 4090 with 24-80GB VRAM"
    components:
      fine_tuner:
        type: pytorch
        config:
          hardware:
            device: cuda
            memory_gb: 40  # Adjust to your GPU
            precision:
              cuda:
                use_fp16: false  # Use bf16 instead
                use_bf16: true   # Better for large models
                use_tf32: true   # Tensor Core optimization
            optimization:
              gradient_checkpointing: false  # Not needed with large VRAM
              gradient_accumulation_steps: 1
              mixed_precision: bf16  # Best for modern datacenter GPUs
          base_model:
            huggingface_id: meta-llama/Llama-2-7b-hf  # Can handle larger models
            torch_dtype: bfloat16
          training_args:
            per_device_train_batch_size: 8  # Large batch size
            
  # =============================================================================
  # ADDITIONAL DEMO STRATEGIES
  # =============================================================================
  
  local_development:
    name: "Local Development Environment"
    description: "Pure local setup for development and testing"
    
    components:
      model_app:
        type: ollama
        config:
          base_url: http://localhost:11434
          default_model: llama3.2:3b
          timeout: 120
          
    monitoring:
      log_level: DEBUG
      
  production_hybrid:
    name: "Production Hybrid Setup"
    description: "Production configuration with cloud and local models"
    
    components:
      cloud_api:
        type: openai_compatible
        config:
          provider: openai
          api_key: ${OPENAI_API_KEY}
          default_model: gpt-4o
          rate_limit:
            requests_per_minute: 500
            tokens_per_minute: 90000
            
      backup_api:
        type: openai_compatible
        config:
          provider: anthropic
          api_key: ${ANTHROPIC_API_KEY}
          default_model: claude-3-opus-20240229
          
      model_app:
        type: ollama
        config:
          base_url: http://localhost:11434
          default_model: llama3.2:3b  # Smaller model for average machines
          
    fallback_chain:
      - provider: cloud_api
        model: gpt-4o
        priority: 1
        
      - provider: backup_api
        model: claude-3-opus-20240229
        priority: 2
        
      - provider: model_app
        model: llama3.2:3b
        priority: 3
        
    constraints:
      max_daily_cost_usd: 100
      max_tokens_per_request: 4096
      require_ssl: true
      
    monitoring:
      track_usage: true
      track_costs: true
      alert_thresholds:
        daily_cost_usd: 100
        error_rate_percent: 5
      log_level: WARNING

  # =============================================================================
  # MOCK MODEL STRATEGY - Fast Development and Testing
  # =============================================================================
  
  mock_development:
    name: "Mock Model for Fast Development"
    description: "Uses mock models for rapid prototyping and testing without API costs or setup"
    
    components:
      # Mock model server
      model_app:
        type: mock_model
        config:
          base_url: "builtin://mock"
          default_model: "mock-gpt-4"
    
    test_queries:
      - "What is machine learning?"
      - "Write a Python function to sort a list"
      - "Explain quantum computing"
      - "Create a haiku about programming"
      - "What are the benefits of mock testing?"
          response_delay: 0.2
          templates:
            default: |
              🤖 Mock Response [{timestamp}]
              
              Query: "{query}"
              
              This is a mock response for development/testing. In a real implementation, 
              this would be processed by: {model}
              
              Estimated tokens: ~{estimated_tokens}
              Response time: {response_time}ms
              
              💡 Tip: Switch to real models when ready for production!
            error: "❌ Mock Error [{timestamp}]: {error_type} - This is simulated for testing"
          models:
            - name: "mock-gpt-4"
              capabilities: ["chat", "completion", "function_calling"]
            - name: "mock-claude-3" 
              capabilities: ["chat", "completion", "vision"]
            - name: "mock-llama-70b"
              capabilities: ["chat", "completion"]
            - name: "mock-tiny"
              capabilities: ["chat"]
          mock_config:
            behavior:
              echo_query: true
              add_timestamp: true
              simulate_thinking: false
            responses:
              include_metadata: true
              show_model_name: true
              estimate_tokens: true
            development:
              log_requests: true
              validate_schemas: true
              
    # Test queries for mock demonstration
    test_queries:
      - "What is machine learning?"
      - "Write a Python function to sort a list"
      - "Explain quantum computing"
      - "Create a haiku about programming"
      - "What are the benefits of mock testing?"
      
    # Mock response customization
    response_config:
      base_delay: 0.2
      tokens_per_second: 100
      max_response_length: 1000
      include_suggestions: true
      
    monitoring:
      track_requests: true
      track_response_times: true
      log_level: DEBUG