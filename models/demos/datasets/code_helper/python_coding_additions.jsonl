{"instruction": "How do I work with machine learning using scikit-learn?", "input": "", "output": "Scikit-learn provides a comprehensive machine learning library:\n\n```python\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.datasets import make_classification, make_regression, load_iris\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 1. Classification Example\ndef classification_example():\n    \"\"\"Complete classification workflow\"\"\"\n    print(\"=== Classification Example ===\")\n    \n    # Load sample data\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    \n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train multiple models\n    models = {\n        'Logistic Regression': LogisticRegression(random_state=42),\n        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n        'SVM': SVC(random_state=42)\n    }\n    \n    results = {}\n    \n    for name, model in models.items():\n        # Train model\n        if name == 'SVM':\n            model.fit(X_train_scaled, y_train)\n            y_pred = model.predict(X_test_scaled)\n        else:\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n        \n        # Evaluate\n        accuracy = accuracy_score(y_test, y_pred)\n        results[name] = accuracy\n        \n        print(f\"\\n{name}:\")\n        print(f\"Accuracy: {accuracy:.4f}\")\n        print(\"Classification Report:\")\n        print(classification_report(y_test, y_pred, target_names=iris.target_names))\n    \n    # Cross-validation\n    print(\"\\nCross-validation scores:\")\n    for name, model in models.items():\n        if name == 'SVM':\n            scores = cross_val_score(model, X_train_scaled, y_train, cv=5)\n        else:\n            scores = cross_val_score(model, X_train, y_train, cv=5)\n        print(f\"{name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n    \n    return results\n\n# 2. Regression Example\ndef regression_example():\n    \"\"\"Complete regression workflow\"\"\"\n    print(\"\\n=== Regression Example ===\")\n    \n    # Generate sample data\n    X, y = make_regression(n_samples=1000, n_features=5, noise=0.1, random_state=42)\n    \n    # Split the data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Scale features\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_test_scaled = scaler.transform(X_test)\n    \n    # Train models\n    models = {\n        'Linear Regression': LinearRegression(),\n        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42)\n    }\n    \n    for name, model in models.items():\n        # Use scaled data for Linear Regression, original for Random Forest\n        if name == 'Linear Regression':\n            model.fit(X_train_scaled, y_train)\n            y_pred = model.predict(X_test_scaled)\n        else:\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n        \n        # Evaluate\n        mse = mean_squared_error(y_test, y_pred)\n        r2 = r2_score(y_test, y_pred)\n        \n        print(f\"\\n{name}:\")\n        print(f\"MSE: {mse:.4f}\")\n        print(f\"RÂ² Score: {r2:.4f}\")\n        \n        # Feature importance for Random Forest\n        if hasattr(model, 'feature_importances_'):\n            importance = model.feature_importances_\n            print(f\"Feature Importances: {importance}\")\n\n# 3. Hyperparameter Tuning\ndef hyperparameter_tuning_example():\n    \"\"\"Hyperparameter tuning with GridSearchCV\"\"\"\n    print(\"\\n=== Hyperparameter Tuning ===\")\n    \n    # Load data\n    iris = load_iris()\n    X, y = iris.data, iris.target\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Define parameter grid for Random Forest\n    param_grid = {\n        'n_estimators': [50, 100, 200],\n        'max_depth': [3, 5, 7, None],\n        'min_samples_split': [2, 5, 10]\n    }\n    \n    # Grid search\n    rf = RandomForestClassifier(random_state=42)\n    grid_search = GridSearchCV(\n        rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1\n    )\n    \n    grid_search.fit(X_train, y_train)\n    \n    print(f\"Best parameters: {grid_search.best_params_}\")\n    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n    \n    # Evaluate best model on test set\n    best_model = grid_search.best_estimator_\n    y_pred = best_model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    print(f\"Test accuracy: {accuracy:.4f}\")\n\n# 4. Data Preprocessing Pipeline\ndef preprocessing_pipeline_example():\n    \"\"\"Complete preprocessing pipeline\"\"\"\n    print(\"\\n=== Data Preprocessing Pipeline ===\")\n    \n    # Create sample data with mixed types\n    data = pd.DataFrame({\n        'numeric1': np.random.randn(1000),\n        'numeric2': np.random.randn(1000) * 10 + 5,\n        'categorical': np.random.choice(['A', 'B', 'C'], 1000),\n        'binary': np.random.choice([0, 1], 1000)\n    })\n    \n    # Add some missing values\n    data.loc[np.random.choice(data.index, 50), 'numeric1'] = np.nan\n    data.loc[np.random.choice(data.index, 30), 'categorical'] = np.nan\n    \n    print(f\"Data shape: {data.shape}\")\n    print(f\"Missing values:\\n{data.isnull().sum()}\")\n    \n    # Handle missing values\n    from sklearn.impute import SimpleImputer\n    \n    # Numeric imputation\n    numeric_imputer = SimpleImputer(strategy='mean')\n    data['numeric1'] = numeric_imputer.fit_transform(data[['numeric1']]).ravel()\n    \n    # Categorical imputation\n    categorical_imputer = SimpleImputer(strategy='most_frequent')\n    data['categorical'] = categorical_imputer.fit_transform(data[['categorical']]).ravel()\n    \n    # Encode categorical variables\n    from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n    \n    # Label encoding for binary\n    le = LabelEncoder()\n    data['categorical_encoded'] = le.fit_transform(data['categorical'])\n    \n    # One-hot encoding\n    ohe = OneHotEncoder(sparse_output=False, drop='first')\n    categorical_encoded = ohe.fit_transform(data[['categorical']])\n    \n    print(f\"\\nAfter preprocessing:\")\n    print(f\"No missing values: {data.isnull().sum().sum() == 0}\")\n    print(f\"Categorical classes: {le.classes_}\")\n    \n    return data\n\n# 5. Model Evaluation and Visualization\ndef model_evaluation_example():\n    \"\"\"Comprehensive model evaluation\"\"\"\n    print(\"\\n=== Model Evaluation ===\")\n    \n    # Generate binary classification data\n    X, y = make_classification(\n        n_samples=1000, n_features=20, n_informative=10,\n        n_redundant=10, n_clusters_per_class=1, random_state=42\n    )\n    \n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42\n    )\n    \n    # Train model\n    model = RandomForestClassifier(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    y_pred_proba = model.predict_proba(X_test)[:, 1]\n    \n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    print(f\"\\nConfusion Matrix:\\n{cm}\")\n    \n    # Classification metrics\n    from sklearn.metrics import precision_score, recall_score, f1_score\n    \n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    \n    print(f\"\\nMetrics:\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1-Score: {f1:.4f}\")\n    \n    # ROC AUC\n    from sklearn.metrics import roc_auc_score, roc_curve\n    \n    auc = roc_auc_score(y_test, y_pred_proba)\n    print(f\"ROC AUC: {auc:.4f}\")\n    \n    # Learning curves\n    from sklearn.model_selection import learning_curve\n    \n    train_sizes, train_scores, val_scores = learning_curve(\n        model, X, y, cv=5, n_jobs=-1,\n        train_sizes=np.linspace(0.1, 1.0, 10)\n    )\n    \n    print(f\"\\nLearning curve computed with {len(train_sizes)} points\")\n\n# 6. Feature Selection\ndef feature_selection_example():\n    \"\"\"Feature selection techniques\"\"\"\n    print(\"\\n=== Feature Selection ===\")\n    \n    # Generate data with irrelevant features\n    X, y = make_classification(\n        n_samples=1000, n_features=50, n_informative=10,\n        n_redundant=5, n_clusters_per_class=1, random_state=42\n    )\n    \n    print(f\"Original feature count: {X.shape[1]}\")\n    \n    # Univariate feature selection\n    from sklearn.feature_selection import SelectKBest, f_classif\n    \n    selector = SelectKBest(score_func=f_classif, k=20)\n    X_selected = selector.fit_transform(X, y)\n    \n    print(f\"After SelectKBest: {X_selected.shape[1]}\")\n    \n    # Recursive Feature Elimination\n    from sklearn.feature_selection import RFE\n    \n    estimator = RandomForestClassifier(n_estimators=50, random_state=42)\n    rfe = RFE(estimator, n_features_to_select=15)\n    X_rfe = rfe.fit_transform(X, y)\n    \n    print(f\"After RFE: {X_rfe.shape[1]}\")\n    \n    # Feature importance from tree-based models\n    rf = RandomForestClassifier(n_estimators=100, random_state=42)\n    rf.fit(X, y)\n    \n    # Get feature importance\n    importance = rf.feature_importances_\n    indices = np.argsort(importance)[::-1]\n    \n    print(f\"\\nTop 10 most important features:\")\n    for i in range(10):\n        print(f\"Feature {indices[i]}: {importance[indices[i]]:.4f}\")\n\n# Main execution\nif __name__ == \"__main__\":\n    # Run all examples\n    classification_example()\n    regression_example()\n    hyperparameter_tuning_example()\n    preprocessing_pipeline_example()\n    model_evaluation_example()\n    feature_selection_example()\n    \n    print(\"\\n=== Machine Learning Pipeline Complete ===\")\n```\n\n**Key Concepts**:\n- **Data Preprocessing**: Scaling, encoding, handling missing values\n- **Model Selection**: Different algorithms for different problems\n- **Cross-Validation**: Robust model evaluation\n- **Hyperparameter Tuning**: Optimizing model performance\n- **Feature Selection**: Choosing relevant features\n- **Model Evaluation**: Comprehensive metrics and visualization\n\n**Best Practices**:\n- Always split data before preprocessing\n- Use cross-validation for model selection\n- Scale features for distance-based algorithms\n- Handle missing data appropriately\n- Evaluate models with multiple metrics\n- Consider feature importance and selection"}