{"instruction": "I'm new to AI and want to understand what LlamaFarm can do for me. Can you explain it simply?", "input": "", "output": "LlamaFarm is like having a Swiss Army knife for AI models! Think of it as a platform that lets you:\n\n🤖 **Use different AI models**: Connect to OpenAI, run local models, or use cloud services - all through one simple interface\n\n🎯 **Customize models for your needs**: Fine-tune models to be better at specific tasks, like answering questions about your business or writing in your company's style\n\n⚡ **Optimize for your hardware**: Whether you have a Mac, powerful GPU, or just a regular computer, LlamaFarm configures everything to run efficiently\n\n📊 **Manage everything easily**: Instead of juggling different tools and configurations, you get one system that handles it all\n\nIt's perfect for businesses, developers, or anyone who wants to use AI effectively without becoming a machine learning expert!"}
{"instruction": "My company wants to fine-tune a model to answer customer support questions. How would I approach this with LlamaFarm?", "input": "", "output": "Great use case! Here's your step-by-step approach:\n\n**1. Prepare Your Data** 📋\n- Collect your existing support tickets and their resolutions\n- Format them as question-answer pairs in JSONL format\n- Aim for 200-500 high-quality examples minimum\n\n**2. Choose Your Strategy** 🎯\n```bash\npython cli.py finetune start --strategy customer_support --dataset ./support_data.jsonl\n```\n\n**3. Monitor and Test** 📊\n- LlamaFarm will show training progress in real-time\n- Test with sample questions during training\n- Compare before/after responses\n\n**4. Deploy** 🚀\n- The fine-tuned model integrates seamlessly\n- Use the same API calls as before\n- Set up A/B testing to measure improvement\n\n**Pro Tips:**\n- Include edge cases and difficult questions in your training data\n- Use the validation features to ensure data quality\n- Start with LoRA fine-tuning for cost-effectiveness\n- Consider adding negative examples (what NOT to say)\n\nWould you like help with any specific step?"}
{"instruction": "I'm getting out of memory errors during fine-tuning. What can I do?", "input": "", "output": "Memory errors are common! Here are solutions in order of effectiveness:\n\n**Quick Fixes** 🔧\n1. **Reduce batch size**: Set `per_device_train_batch_size: 1`\n2. **Use gradient accumulation**: Increase `gradient_accumulation_steps: 8` to maintain effective batch size\n3. **Enable gradient checkpointing**: Add `gradient_checkpointing: true`\n\n**Switch Methods** 🔄\n- Use **QLoRA instead of LoRA**: Much more memory efficient with 4-bit quantization\n- Try **LoRA with smaller rank**: Reduce `r: 8` and `alpha: 16`\n\n**Advanced Solutions** ⚡\n```yaml\nmethod:\n  type: \"qlora\"  # 4-bit quantization\n  r: 16\n  alpha: 32\n\ntraining_args:\n  per_device_train_batch_size: 1\n  gradient_accumulation_steps: 16\n  gradient_checkpointing: true\n  dataloader_pin_memory: false\n  bf16: true  # Use bfloat16 if supported\n```\n\n**Hardware Optimizations** 💻\n- Clear GPU cache: `torch.cuda.empty_cache()`\n- Use CPU offloading for very large models\n- Consider distributed training across multiple GPUs\n\n**Last Resort** 🆘\n- Use a smaller base model (7B instead of 13B)\n- Reduce max_seq_length to 256 or 512\n- Fine-tune in stages with different data subsets\n\nWhich solution fits your setup best?"}
{"instruction": "How do I know if my fine-tuning is actually working? The loss is decreasing but I'm not sure about quality.", "input": "", "output": "Great question! Decreasing loss is good, but you need qualitative evaluation too. Here's how to properly assess your fine-tuning:\n\n**During Training** 📊\n1. **Watch validation loss**: Should decrease along with training loss\n2. **Monitor for overfitting**: If validation loss starts increasing while training loss decreases, stop early\n3. **Check learning curves**: Smooth decrease is better than erratic jumps\n\n**Quality Testing** 🎯\n```python\n# Test with specific prompts\ntest_prompts = [\n    \"What is your return policy?\",\n    \"How do I reset my password?\",\n    \"My order hasn't arrived, what should I do?\"\n]\n\n# Compare before/after responses\nfor prompt in test_prompts:\n    base_response = base_model.generate(prompt)\n    finetuned_response = finetuned_model.generate(prompt)\n    print(f\"Prompt: {prompt}\")\n    print(f\"Before: {base_response}\")\n    print(f\"After: {finetuned_response}\")\n    print(\"---\")\n```\n\n**Quantitative Metrics** 📈\n- **Perplexity**: Lower is better (measures how \"surprised\" the model is)\n- **BLEU/ROUGE**: For comparing against reference answers\n- **Task-specific metrics**: Customer satisfaction scores, resolution rates\n\n**Red Flags** 🚩\n- Model gives identical responses to different questions\n- Responses become generic or robotic\n- Model \"forgets\" basic knowledge\n- Repetitive or incoherent outputs\n\n**Best Practice**: Create a held-out test set of 50-100 examples and manually review responses. If 80%+ are better than the base model, you're on track!"}
{"instruction": "Can you walk me through setting up LlamaFarm for the first time? I'm a developer but new to ML.", "input": "", "output": "Absolutely! Let's get you set up step by step:\n\n**1. Installation** 📦\n```bash\n# Clone the repository\ngit clone https://github.com/your-org/llamafarm\ncd llamafarm/models\n\n# Install dependencies with UV (Python package manager)\nuv sync\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n```\n\n**2. Environment Setup** 🔧\n```bash\n# Copy environment template\ncp .env.example .env\n\n# Edit .env with your API keys\nOPENAI_API_KEY=your_openai_key_here\nHUGGINGFACE_TOKEN=your_hf_token_here\n```\n\n**3. Test Installation** ✅\n```bash\n# Run the quick test\npython test_quick_finetune.py\n\n# Should see: \"✅ Quick fine-tuning test completed successfully!\"\n```\n\n**4. Your First Fine-tune** 🚀\n```bash\n# Create some sample data (or use provided samples)\npython cli.py finetune start --strategy mac_m1_lora --dataset sample_llamafarm_qa.jsonl\n```\n\n**5. Monitor Progress** 👀\n```bash\n# In another terminal\npython cli.py finetune status\n\n# View logs\ntail -f ./fine_tuned_model/training.log\n```\n\n**Common First-Time Issues** 🔍\n- **CUDA not found**: Use CPU strategy initially\n- **Permission errors**: Make sure you're in the right directory\n- **Memory issues**: Start with smaller models (gpt2 for testing)\n\n**Next Steps** 📚\n1. Read through the strategies in `fine_tuning/strategies/`\n2. Try the demo script: `./setup_and_demo.sh`\n3. Experiment with different datasets\n4. Join our community for questions!\n\nNeed help with any specific step?"}
{"instruction": "What's the difference between LoRA, QLoRA, and full fine-tuning? When should I use each?", "input": "", "output": "Great question! Each method has different trade-offs. Let me break it down:\n\n**Full Fine-tuning** 🎯\n- **What it is**: Updates ALL model parameters\n- **Pros**: Maximum adaptation capability, best performance for complex tasks\n- **Cons**: Requires massive memory/compute, risk of catastrophic forgetting\n- **When to use**: Large budgets, critical applications, completely new domains\n\n**LoRA (Low-Rank Adaptation)** ⚡\n- **What it is**: Adds small \"adapter\" layers, freezes original model\n- **Pros**: 1000x fewer parameters to train, much faster, prevents forgetting\n- **Cons**: Slightly less powerful than full fine-tuning\n- **When to use**: Most cases! Great balance of efficiency and performance\n\n**QLoRA (Quantized LoRA)** 💚\n- **What it is**: LoRA + 4-bit quantization of the base model\n- **Pros**: Extreme memory efficiency (fits 65B models on consumer GPUs)\n- **Cons**: Slightly reduced quality due to quantization\n- **When to use**: Large models on limited hardware, budget constraints\n\n**Decision Tree** 🌳\n```\nDo you have unlimited compute budget?\n├─ Yes → Full Fine-tuning\n└─ No\n   ├─ Can you fit the model in memory normally?\n   │  └─ Yes → LoRA\n   └─ No → QLoRA\n```\n\n**Real Examples** 💼\n- **Customer support bot**: LoRA (efficient, good quality)\n- **Legal document analysis**: Full fine-tuning (needs maximum accuracy)\n- **Personal assistant on laptop**: QLoRA (resource constraints)\n- **Code generation**: LoRA (good balance for most teams)\n\n**Memory Comparison** 📊\n- Full: ~80GB for 13B model\n- LoRA: ~24GB for 13B model  \n- QLoRA: ~9GB for 13B model\n\nStart with LoRA - it works great for 90% of use cases!"}
{"instruction": "I want to fine-tune for creative writing. How should I prepare my data and configure the training?", "input": "", "output": "Creative writing fine-tuning is fascinating! Here's how to approach it:\n\n**Data Preparation** ✍️\n\n1. **Collect Diverse Examples**\n```jsonl\n{\"instruction\": \"Write a suspenseful opening paragraph\", \"input\": \"Genre: Mystery, Setting: Old mansion\", \"output\": \"The grandfather clock's midnight chime echoed through the empty halls as Sarah's flashlight flickered, casting dancing shadows on the peeling wallpaper...\"}\n{\"instruction\": \"Create a character description\", \"input\": \"A mysterious shopkeeper\", \"output\": \"Behind wire-rimmed glasses, his eyes held secrets older than the dusty antiques surrounding him...\"}\n```\n\n2. **Include Style Variations**\n- Different genres (mystery, romance, sci-fi)\n- Various lengths (flash fiction to novel excerpts)\n- Multiple perspectives (1st person, 3rd person)\n- Dialogue and narrative examples\n\n**Configuration for Creativity** 🎨\n```yaml\nname: \"creative_writing\"\n\nbase_model:\n  name: \"creative-llama\"\n  huggingface_id: \"meta-llama/Llama-2-7b-hf\"\n\nmethod:\n  type: \"lora\"\n  r: 32  # Higher rank for more expressiveness\n  alpha: 64\n  dropout: 0.05  # Lower dropout to preserve creativity\n  target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n\ntraining_args:\n  learning_rate: 1e-4  # Slightly lower for nuanced learning\n  num_train_epochs: 5\n  max_seq_length: 1024  # Longer sequences for narrative flow\n  temperature: 0.8  # Higher temperature for sampling diversity\n```\n\n**Advanced Tips** 🌟\n\n1. **Prompt Engineering**\n```\n\"Write in the style of [author/genre]\"\n\"Continue this story: [beginning]\"\n\"Rewrite this scene with more [emotion/tension/humor]\"\n```\n\n2. **Quality Control**\n- Include examples of what NOT to write\n- Add style guidelines in your instructions\n- Use validation set with professional writing samples\n\n3. **Testing Creativity**\n```python\ntest_prompts = [\n    \"Write a love story between a human and an AI\",\n    \"Describe a world where colors have taste\",\n    \"Create a villain who thinks they're the hero\"\n]\n\n# Generate multiple samples to test variety\nfor prompt in test_prompts:\n    for i in range(5):\n        response = model.generate(prompt, temperature=0.9)\n        print(f\"Variation {i+1}: {response}\\n\")\n```\n\n**Dataset Size Guidelines** 📊\n- **Minimum**: 500 examples\n- **Good**: 1,000-2,000 examples\n- **Excellent**: 3,000+ examples with high variety\n\n**Evaluation Metrics** 📈\n- Vocabulary diversity (unique words/total words)\n- Readability scores\n- Genre consistency\n- Human preference ratings\n\nRemember: Creative writing models need more data variety than task-specific models. Quality and diversity matter more than quantity!"}
{"instruction": "My fine-tuned model seems to have lost some of its general knowledge. How can I prevent this?", "input": "", "output": "This is called \"catastrophic forgetting\" - very common issue! Here's how to prevent it:\n\n**Why It Happens** 🧠\n- Fine-tuning overwrites general knowledge with task-specific patterns\n- Model \"forgets\" what it learned during pre-training\n- More severe with full fine-tuning, less with LoRA\n\n**Prevention Strategies** 🛡️\n\n**1. Use LoRA/QLoRA** (Easiest)\n```yaml\nmethod:\n  type: \"lora\"  # Preserves base model knowledge\n  r: 16\n  alpha: 32\n```\n\n**2. Mixed Dataset Approach**\n```jsonl\n// 70% task-specific data\n{\"instruction\": \"How do I return an item?\", \"output\": \"To return an item...\"}\n\n// 30% general knowledge data\n{\"instruction\": \"What is the capital of France?\", \"output\": \"The capital of France is Paris...\"}\n{\"instruction\": \"Explain photosynthesis\", \"output\": \"Photosynthesis is the process...\"}\n```\n\n**3. Lower Learning Rate**\n```yaml\ntraining_args:\n  learning_rate: 5e-5  # Instead of 2e-4\n  warmup_ratio: 0.1\n```\n\n**4. Regularization Techniques**\n```yaml\ntraining_args:\n  weight_decay: 0.01  # Prevents overfitting\n  dropout: 0.1\n  early_stopping_patience: 3\n```\n\n**5. Curriculum Learning**\n```python\n# Start with general knowledge, gradually add task-specific data\nepoch_1_data = general_knowledge + 10% task_data\nepoch_2_data = general_knowledge + 50% task_data  \nepoch_3_data = general_knowledge + 90% task_data\n```\n\n**Testing for Forgetting** 🔍\n```python\ngeneral_knowledge_tests = [\n    \"What is 2+2?\",\n    \"Who wrote Romeo and Juliet?\",\n    \"What is the largest planet?\",\n    \"Explain gravity\"\n]\n\n# Test before and after fine-tuning\nfor test in general_knowledge_tests:\n    base_answer = base_model.generate(test)\n    finetuned_answer = finetuned_model.generate(test)\n    \n    if quality_score(finetuned_answer) < quality_score(base_answer):\n        print(f\"⚠️ Knowledge degradation detected: {test}\")\n```\n\n**Recovery Options** 🔄\nIf forgetting already happened:\n1. **Retrain with mixed dataset**\n2. **Use model ensemble** (combine base + fine-tuned)\n3. **Apply knowledge distillation**\n4. **Reduce fine-tuning strength**\n\n**Best Practice Recipe** 👨‍🍳\n```yaml\n# Balanced configuration\nmethod:\n  type: \"lora\"  # Preserves knowledge\n  r: 16\n  alpha: 32\n\ntraining_args:\n  learning_rate: 1e-4  # Conservative\n  num_train_epochs: 3  # Don't overtrain\n  early_stopping_patience: 2\n  eval_strategy: \"steps\"\n  eval_steps: 100\n```\n\nThe key is finding the sweet spot between task adaptation and knowledge preservation!"}
{"instruction": "How do I set up continuous learning where my model keeps improving as I get new data?", "input": "", "output": "Continuous learning is powerful for keeping models up-to-date! Here's how to implement it:\n\n**Architecture Overview** 🏗️\n```\nNew Data → Validation → Incremental Training → Testing → Deployment\n     ↓         ↓             ↓              ↓         ↓\n  Storage   Quality      Model Update    A/B Test   Monitor\n```\n\n**1. Data Pipeline Setup** 📊\n```python\nclass ContinuousLearningPipeline:\n    def __init__(self, base_model_path, config):\n        self.base_model_path = base_model_path\n        self.config = config\n        self.data_buffer = []\n        self.min_samples_for_update = 100\n        \n    def add_new_data(self, examples):\n        \"\"\"Add new training examples\"\"\"\n        # Validate data quality\n        validated = self.validate_examples(examples)\n        self.data_buffer.extend(validated)\n        \n        # Trigger update if enough data\n        if len(self.data_buffer) >= self.min_samples_for_update:\n            self.trigger_incremental_training()\n    \n    def validate_examples(self, examples):\n        \"\"\"Quality control for new data\"\"\"\n        validated = []\n        for example in examples:\n            if self.passes_quality_checks(example):\n                validated.append(example)\n        return validated\n    \n    def trigger_incremental_training(self):\n        \"\"\"Start incremental fine-tuning\"\"\"\n        print(f\"🔄 Starting incremental training with {len(self.data_buffer)} examples\")\n        \n        # Combine with replay buffer to prevent forgetting\n        training_data = self.data_buffer + self.get_replay_data()\n        \n        # Train new model version\n        new_model = self.incremental_train(training_data)\n        \n        # Validate new model\n        if self.validate_new_model(new_model):\n            self.deploy_model(new_model)\n            self.clear_buffer()\n```\n\n**2. Incremental Training Strategy** 🎯\n```yaml\n# continuous_learning_config.yaml\nname: \"continuous_learning\"\n\nbase_model:\n  name: \"production-model\"\n  path: \"./current_production_model\"\n\nmethod:\n  type: \"lora\"\n  r: 16\n  alpha: 32\n  merge_adapters: false  # Keep adapters separate\n\ntraining_args:\n  learning_rate: 5e-5  # Lower for incremental updates\n  num_train_epochs: 1  # Quick updates\n  per_device_train_batch_size: 4\n  save_steps: 50\n  eval_steps: 25\n  \ncontinuous_learning:\n  replay_buffer_size: 1000  # Keep old examples\n  quality_threshold: 0.8\n  update_frequency: \"daily\"  # or \"weekly\", \"on_demand\"\n  rollback_threshold: 0.05  # Rollback if performance drops\n```\n\n**3. Anti-Forgetting Mechanisms** 🧠\n```python\nclass ReplayBuffer:\n    \"\"\"Maintains representative samples from previous training\"\"\"\n    \n    def __init__(self, max_size=1000):\n        self.buffer = []\n        self.max_size = max_size\n    \n    def add_samples(self, samples):\n        \"\"\"Add samples with strategic selection\"\"\"\n        # Prioritize diverse, high-quality examples\n        selected = self.select_representative_samples(samples)\n        \n        self.buffer.extend(selected)\n        \n        # Maintain buffer size\n        if len(self.buffer) > self.max_size:\n            self.buffer = self.buffer[-self.max_size:]\n    \n    def select_representative_samples(self, samples):\n        \"\"\"Select diverse samples to prevent forgetting\"\"\"\n        # Use clustering or diversity metrics\n        return samples  # Simplified\n```\n\n**4. Model Validation & A/B Testing** 📈\n```python\nclass ModelValidator:\n    def __init__(self, test_suite):\n        self.test_suite = test_suite\n        self.performance_history = []\n    \n    def validate_new_model(self, new_model, current_model):\n        \"\"\"Compare new model against current production model\"\"\"\n        \n        results = {\n            'new_model': self.run_test_suite(new_model),\n            'current_model': self.run_test_suite(current_model)\n        }\n        \n        # Check for performance regression\n        improvement = results['new_model']['score'] - results['current_model']['score']\n        \n        if improvement > 0.02:  # 2% improvement threshold\n            print(f\"✅ New model shows {improvement:.1%} improvement\")\n            return True\n        elif improvement < -0.05:  # 5% degradation threshold\n            print(f\"❌ New model shows {improvement:.1%} degradation - rejecting\")\n            return False\n        else:\n            print(f\"➡️ Marginal change ({improvement:.1%}) - A/B testing recommended\")\n            return self.recommend_ab_test(results)\n```\n\n**5. Automated Deployment Pipeline** 🚀\n```bash\n#!/bin/bash\n# continuous_learning_cron.sh\n\n# Run daily at 2 AM\n# 0 2 * * * /path/to/continuous_learning_cron.sh\n\necho \"🔄 Starting continuous learning pipeline...\"\n\n# Check for new data\npython scripts/check_new_data.py\n\nif [ $? -eq 0 ]; then\n    echo \"📊 New data found, starting incremental training...\"\n    \n    # Train incrementally\n    python cli.py finetune incremental --config continuous_learning_config.yaml\n    \n    # Validate results\n    python scripts/validate_model.py --new-model ./incremental_model\n    \n    if [ $? -eq 0 ]; then\n        echo \"✅ Validation passed, deploying new model...\"\n        python scripts/deploy_model.py --model ./incremental_model\n    else\n        echo \"❌ Validation failed, keeping current model\"\n    fi\nelse\n    echo \"ℹ️ No new data, skipping update\"\nfi\n```\n\n**6. Monitoring & Rollback** 📊\n```python\nclass ContinuousMonitor:\n    def __init__(self):\n        self.metrics_history = []\n        self.alert_thresholds = {\n            'accuracy': 0.85,\n            'response_time': 2.0,\n            'error_rate': 0.05\n        }\n    \n    def monitor_production_model(self):\n        \"\"\"Continuously monitor model performance\"\"\"\n        current_metrics = self.collect_metrics()\n        \n        for metric, threshold in self.alert_thresholds.items():\n            if self.check_degradation(metric, current_metrics[metric], threshold):\n                self.trigger_rollback(f\"{metric} below threshold\")\n                break\n    \n    def trigger_rollback(self, reason):\n        \"\"\"Rollback to previous model version\"\"\"\n        print(f\"🚨 Triggering rollback: {reason}\")\n        # Implement rollback logic\n        self.deploy_previous_version()\n        self.send_alert(reason)\n```\n\n**Best Practices** ✨\n- **Start small**: Begin with daily small updates\n- **Quality gates**: Never skip validation steps  \n- **Rollback ready**: Always have a fallback plan\n- **Monitor closely**: Watch for performance degradation\n- **Data diversity**: Ensure new data doesn't create bias\n\nThis creates a robust system that improves automatically while staying stable!"}
{"instruction": "What's the best way to handle multilingual fine-tuning? I need my model to work well in English, Spanish, and French.", "input": "", "output": "Multilingual fine-tuning requires careful strategy! Here's the comprehensive approach:\n\n**1. Choose the Right Base Model** 🌍\n```yaml\nbase_model:\n  # Option 1: Multilingual foundation\n  name: \"multilingual-llama\"\n  huggingface_id: \"meta-llama/Llama-2-7b-hf\"  # Has multilingual training\n  \n  # Option 2: Specialized multilingual model\n  # huggingface_id: \"microsoft/DialoGPT-multilingual\"\n  \n  # Option 3: mT5 for better multilingual support\n  # huggingface_id: \"google/mt5-base\"\n```\n\n**2. Balanced Dataset Strategy** ⚖️\n```jsonl\n// English examples (40%)\n{\"instruction\": \"What is your return policy?\", \"input\": \"\", \"output\": \"Our return policy allows returns within 30 days...\"}\n\n// Spanish examples (30%)\n{\"instruction\": \"¿Cuál es su política de devoluciones?\", \"input\": \"\", \"output\": \"Nuestra política de devoluciones permite devoluciones dentro de 30 días...\"}\n\n// French examples (30%)\n{\"instruction\": \"Quelle est votre politique de retour?\", \"input\": \"\", \"output\": \"Notre politique de retour permet les retours dans les 30 jours...\"}\n\n// Cross-lingual examples\n{\"instruction\": \"Respond in Spanish: What are your business hours?\", \"input\": \"\", \"output\": \"Nuestro horario comercial es de lunes a viernes de 9:00 a 17:00.\"}\n```\n\n**3. Advanced Configuration** 🔧\n```yaml\nname: \"multilingual_support\"\n\nbase_model:\n  name: \"multilingual-assistant\"\n  huggingface_id: \"meta-llama/Llama-2-7b-hf\"\n\nmethod:\n  type: \"lora\"\n  r: 32  # Higher rank for language complexity\n  alpha: 64\n  target_modules: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n  # Language-specific adapters (advanced)\n  language_adapters: true\n\ntraining_args:\n  learning_rate: 1e-4\n  max_seq_length: 1024  # Longer for different language structures\n  per_device_train_batch_size: 2\n  gradient_accumulation_steps: 8\n  num_train_epochs: 5\n  \n  # Language balancing\n  group_by_length: false  # Prevent language clustering\n  dataloader_num_workers: 4\n\nmultilingual:\n  languages: [\"en\", \"es\", \"fr\"]\n  balance_strategy: \"weighted\"  # Equal representation\n  cross_lingual_examples: 0.1  # 10% cross-lingual tasks\n  language_detection: true\n```\n\n**4. Data Preparation Script** 📊\n```python\nimport json\nfrom collections import defaultdict\nimport random\n\nclass MultilingualDataProcessor:\n    def __init__(self, target_languages=['en', 'es', 'fr']):\n        self.languages = target_languages\n        self.data_by_language = defaultdict(list)\n        \n    def balance_dataset(self, input_file, output_file, target_per_language=1000):\n        \"\"\"Balance dataset across languages\"\"\"\n        \n        # Load and categorize data\n        with open(input_file, 'r', encoding='utf-8') as f:\n            for line in f:\n                item = json.loads(line)\n                lang = self.detect_language(item['instruction'] + item['output'])\n                if lang in self.languages:\n                    self.data_by_language[lang].append(item)\n        \n        # Balance the dataset\n        balanced_data = []\n        \n        for lang in self.languages:\n            lang_data = self.data_by_language[lang]\n            \n            if len(lang_data) > target_per_language:\n                # Subsample if too much data\n                selected = random.sample(lang_data, target_per_language)\n            else:\n                # Use all data if not enough\n                selected = lang_data\n                print(f\"⚠️ Only {len(selected)} examples for {lang}, consider adding more\")\n            \n            balanced_data.extend(selected)\n        \n        # Shuffle to prevent language clustering\n        random.shuffle(balanced_data)\n        \n        # Add cross-lingual examples\n        cross_lingual = self.generate_cross_lingual_examples(balanced_data)\n        balanced_data.extend(cross_lingual)\n        \n        # Save balanced dataset\n        with open(output_file, 'w', encoding='utf-8') as f:\n            for item in balanced_data:\n                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n        \n        print(f\"✅ Balanced dataset saved: {len(balanced_data)} examples\")\n        self.print_language_distribution(balanced_data)\n    \n    def generate_cross_lingual_examples(self, data):\n        \"\"\"Generate cross-lingual instruction examples\"\"\"\n        cross_lingual = []\n        \n        # Sample some examples for cross-lingual tasks\n        sample_size = min(100, len(data) // 10)\n        samples = random.sample(data, sample_size)\n        \n        for item in samples:\n            # Create \"translate and respond\" examples\n            if self.detect_language(item['instruction']) == 'en':\n                cross_lingual.append({\n                    \"instruction\": f\"Respond in Spanish: {item['instruction']}\",\n                    \"input\": item['input'],\n                    \"output\": self.translate_text(item['output'], 'es')\n                })\n                cross_lingual.append({\n                    \"instruction\": f\"Respond in French: {item['instruction']}\",\n                    \"input\": item['input'], \n                    \"output\": self.translate_text(item['output'], 'fr')\n                })\n        \n        return cross_lingual\n    \n    def detect_language(self, text):\n        \"\"\"Simple language detection (use langdetect library in production)\"\"\"\n        # Simplified - use proper language detection in production\n        if any(word in text.lower() for word in ['qué', 'cómo', 'dónde', 'cuál']):\n            return 'es'\n        elif any(word in text.lower() for word in ['que', 'comment', 'où', 'quel']):\n            return 'fr'\n        else:\n            return 'en'\n```\n\n**5. Evaluation Strategy** 📈\n```python\nclass MultilingualEvaluator:\n    def __init__(self, languages=['en', 'es', 'fr']):\n        self.languages = languages\n        \n    def evaluate_model(self, model, test_sets):\n        \"\"\"Evaluate model performance across languages\"\"\"\n        results = {}\n        \n        for lang in self.languages:\n            print(f\"\\n🔍 Evaluating {lang.upper()}...\")\n            \n            lang_results = {\n                'accuracy': self.calculate_accuracy(model, test_sets[lang]),\n                'fluency': self.calculate_fluency(model, test_sets[lang]),\n                'consistency': self.calculate_consistency(model, test_sets[lang])\n            }\n            \n            results[lang] = lang_results\n            print(f\"Results: {lang_results}\")\n        \n        # Cross-lingual evaluation\n        cross_lingual_score = self.evaluate_cross_lingual(model)\n        results['cross_lingual'] = cross_lingual_score\n        \n        return results\n    \n    def evaluate_cross_lingual(self, model):\n        \"\"\"Test cross-lingual understanding\"\"\"\n        test_cases = [\n            (\"What is AI?\", \"es\", \"should mention 'inteligencia artificial'\"),\n            (\"Comment ça va?\", \"en\", \"should be friendly greeting response\"),\n            (\"¿Cuál es tu función?\", \"fr\", \"should explain function in French\")\n        ]\n        \n        scores = []\n        for question, target_lang, expectation in test_cases:\n            prompt = f\"Respond in {target_lang}: {question}\"\n            response = model.generate(prompt)\n            \n            # Evaluate if response is in correct language and relevant\n            lang_correct = self.detect_language(response) == target_lang\n            content_relevant = self.check_relevance(response, expectation)\n            \n            score = (lang_correct + content_relevant) / 2\n            scores.append(score)\n        \n        return sum(scores) / len(scores)\n```\n\n**6. Common Pitfalls & Solutions** ⚠️\n\n**Problem**: Model mixes languages in responses\n**Solution**: Add explicit language instruction prompts\n```jsonl\n{\"instruction\": \"[SPANISH] ¿Cuáles son sus horarios?\", \"output\": \"[SPANISH] Nuestros horarios son...\"}\n```\n\n**Problem**: English dominates other languages\n**Solution**: Use weighted sampling or language-specific adapters\n\n**Problem**: Cultural context gets lost\n**Solution**: Include culture-specific examples and local references\n\n**Testing Checklist** ✅\n- [ ] Each language maintains quality\n- [ ] No language mixing in responses\n- [ ] Cross-lingual instructions work\n- [ ] Cultural appropriateness\n- [ ] Performance degradation < 10% per language\n\n**Pro Tips** 🌟\n1. **Start with 2 languages** before adding the third\n2. **Use native speakers** for data validation  \n3. **Include regional variations** (Mexican vs Spanish Spanish)\n4. **Monitor for bias** towards dominant language\n5. **Test edge cases** like code-switching\n\nThis approach ensures your model works well across all three languages while maintaining quality!"}
{"instruction": "I'm seeing inconsistent results from my fine-tuned model. Sometimes it's great, other times it gives poor responses. How do I debug this?", "input": "", "output": "Inconsistent model behavior is frustrating! Let's systematically debug this:\n\n**1. First, Gather Evidence** 🔍\n```python\n# Create a systematic test to identify patterns\ntest_cases = [\n    {\"prompt\": \"What is your return policy?\", \"category\": \"policy\"},\n    {\"prompt\": \"How do I reset my password?\", \"category\": \"technical\"},\n    {\"prompt\": \"What are your business hours?\", \"category\": \"info\"},\n    # Add 20-30 diverse test cases\n]\n\nresults = []\nfor test in test_cases:\n    # Generate multiple responses for same prompt\n    responses = []\n    for i in range(5):\n        response = model.generate(\n            test[\"prompt\"],\n            temperature=0.7,  # Your usual settings\n            max_tokens=150\n        )\n        responses.append(response)\n    \n    # Analyze variance\n    quality_scores = [rate_response(r) for r in responses]\n    variance = np.var(quality_scores)\n    \n    results.append({\n        \"prompt\": test[\"prompt\"],\n        \"category\": test[\"category\"],\n        \"responses\": responses,\n        \"quality_scores\": quality_scores,\n        \"variance\": variance,\n        \"avg_quality\": np.mean(quality_scores)\n    })\n\n# Identify patterns\nhigh_variance_cases = [r for r in results if r[\"variance\"] > 0.5]\nprint(f\"High variance cases: {len(high_variance_cases)}\")\n```\n\n**2. Common Causes & Diagnostics** 🎯\n\n**A. Temperature/Sampling Issues**\n```python\n# Test with different sampling parameters\nsampling_configs = [\n    {\"temperature\": 0.1, \"top_p\": 0.9},  # More deterministic\n    {\"temperature\": 0.7, \"top_p\": 0.9},  # Balanced\n    {\"temperature\": 1.0, \"top_p\": 0.9},  # More creative\n]\n\nfor config in sampling_configs:\n    print(f\"\\nTesting with temp={config['temperature']}\")\n    for i in range(3):\n        response = model.generate(\"What is AI?\", **config)\n        print(f\"Response {i+1}: {response[:100]}...\")\n```\n\n**B. Training Data Quality Issues**\n```python\n# Analyze your training data for patterns\ndef analyze_training_data(dataset_path):\n    data = []\n    with open(dataset_path, 'r') as f:\n        for line in f:\n            data.append(json.loads(line))\n    \n    # Check for inconsistencies\n    print(f\"📊 Dataset Analysis:\")\n    print(f\"Total examples: {len(data)}\")\n    \n    # Look for duplicate or conflicting examples\n    instruction_counts = {}\n    for item in data:\n        inst = item['instruction'].strip().lower()\n        if inst in instruction_counts:\n            instruction_counts[inst].append(item['output'])\n        else:\n            instruction_counts[inst] = [item['output']]\n    \n    # Find instructions with multiple different outputs\n    conflicting = {k: v for k, v in instruction_counts.items() \n                  if len(set(v)) > 1}\n    \n    print(f\"❌ Conflicting instructions: {len(conflicting)}\")\n    for inst, outputs in list(conflicting.items())[:5]:\n        print(f\"  '{inst}' has {len(set(outputs))} different outputs\")\n    \n    return conflicting\n\nconflicts = analyze_training_data(\"./training_data.jsonl\")\n```\n\n**C. Context Length Issues**\n```python\n# Test with different prompt lengths\nbase_prompt = \"What is your return policy?\"\n\ntest_prompts = [\n    base_prompt,  # Short\n    f\"Context: I'm a customer service rep. {base_prompt}\",  # Medium\n    f\"Context: I work for an e-commerce company with a 30-day return policy, free returns, and excellent customer service. The customer is asking: {base_prompt}\",  # Long\n]\n\nfor i, prompt in enumerate(test_prompts):\n    print(f\"\\nPrompt length {i+1} ({len(prompt)} chars):\")\n    response = model.generate(prompt, temperature=0.3)\n    print(f\"Response: {response}\")\n```\n\n**3. Systematic Debugging Approach** 🔧\n\n**Step 1: Control Variables**\n```python\n# Test with completely deterministic settings\ndeterministic_config = {\n    \"temperature\": 0.0,\n    \"do_sample\": False,\n    \"num_beams\": 1,\n    \"pad_token_id\": tokenizer.eos_token_id\n}\n\n# Same prompt should give identical results\ntest_prompt = \"What is your return policy?\"\nfor i in range(5):\n    response = model.generate(test_prompt, **deterministic_config)\n    print(f\"Attempt {i+1}: {response}\")\n\n# If these vary, you have a deeper model issue\n```\n\n**Step 2: Isolate Problem Patterns**\n```python\n# Group your test cases by performance\nperformance_analysis = {\n    \"consistently_good\": [],\n    \"consistently_bad\": [],\n    \"inconsistent\": []\n}\n\nfor result in results:\n    scores = result[\"quality_scores\"]\n    avg_score = np.mean(scores)\n    variance = np.var(scores)\n    \n    if avg_score > 0.8 and variance < 0.1:\n        performance_analysis[\"consistently_good\"].append(result)\n    elif avg_score < 0.4 and variance < 0.1:\n        performance_analysis[\"consistently_bad\"].append(result)\n    else:\n        performance_analysis[\"inconsistent\"].append(result)\n\n# Analyze patterns\nprint(\"\\n🔍 Pattern Analysis:\")\nfor category, cases in performance_analysis.items():\n    print(f\"{category}: {len(cases)} cases\")\n    if cases:\n        print(f\"  Sample: {cases[0]['prompt']}\")\n```\n\n**4. Quick Fixes to Try** ⚡\n\n**A. Stabilize Generation**\n```yaml\n# Add to your inference config\ngeneration_config:\n  temperature: 0.3        # Lower temperature\n  top_p: 0.85            # Slightly more focused\n  repetition_penalty: 1.1 # Reduce repetition\n  length_penalty: 1.0     # Balanced length\n  no_repeat_ngram_size: 3 # Avoid repetitive phrases\n```\n\n**B. Add Consistency Prompting**\n```python\n# Modify your prompts to encourage consistency\nconsistent_prompts = {\n    \"original\": \"What is your return policy?\",\n    \"consistent\": \"As a helpful customer service assistant, please provide a clear and accurate answer: What is your return policy?\"\n}\n\nfor style, prompt in consistent_prompts.items():\n    print(f\"\\n{style} style:\")\n    response = model.generate(prompt)\n    print(response)\n```\n\n**5. If Problems Persist - Retrain** 🔄\n\n```python\n# Clean your training data\ndef clean_training_data(input_file, output_file):\n    cleaned_data = []\n    seen_instructions = {}\n    \n    with open(input_file, 'r') as f:\n        for line in f:\n            item = json.loads(line)\n            instruction = item['instruction'].strip().lower()\n            \n            # Skip conflicting instructions\n            if instruction in seen_instructions:\n                if seen_instructions[instruction] != item['output']:\n                    print(f\"⚠️ Skipping conflicting: {instruction}\")\n                    continue\n            \n            seen_instructions[instruction] = item['output']\n            cleaned_data.append(item)\n    \n    with open(output_file, 'w') as f:\n        for item in cleaned_data:\n            f.write(json.dumps(item) + '\\n')\n    \n    print(f\"✅ Cleaned dataset: {len(cleaned_data)} examples\")\n\nclean_training_data(\"training_data.jsonl\", \"cleaned_training_data.jsonl\")\n```\n\n**6. Monitoring Solution** 📊\n```python\n# Set up ongoing monitoring\nclass ConsistencyMonitor:\n    def __init__(self, test_prompts):\n        self.test_prompts = test_prompts\n        self.baseline_responses = {}\n        \n    def establish_baseline(self, model):\n        \"\"\"Record expected responses\"\"\"\n        for prompt in self.test_prompts:\n            response = model.generate(prompt, temperature=0.1)\n            self.baseline_responses[prompt] = response\n    \n    def check_consistency(self, model, threshold=0.8):\n        \"\"\"Check if model is behaving consistently\"\"\"\n        inconsistencies = []\n        \n        for prompt in self.test_prompts:\n            current_response = model.generate(prompt, temperature=0.1)\n            baseline = self.baseline_responses[prompt]\n            \n            similarity = calculate_similarity(current_response, baseline)\n            if similarity < threshold:\n                inconsistencies.append({\n                    'prompt': prompt,\n                    'similarity': similarity,\n                    'current': current_response,\n                    'baseline': baseline\n                })\n        \n        return inconsistencies\n\n# Use in production\nmonitor = ConsistencyMonitor(your_key_test_prompts)\nmonitor.establish_baseline(your_model)\ninconsistencies = monitor.check_consistency(your_model)\n\nif inconsistencies:\n    print(f\"⚠️ Found {len(inconsistencies)} consistency issues\")\n```\n\nStart with the evidence gathering, then work through the diagnostics systematically. Most inconsistency issues stem from training data conflicts or sampling parameters!"}
{"instruction": "What are the latest trends and best practices in fine-tuning that I should know about in 2024?", "input": "", "output": "Great question! The fine-tuning landscape has evolved rapidly in 2024. Here are the key trends and best practices:\n\n**🔥 Latest Techniques (2024)**\n\n**1. QLoRA Dominance**\n- **What's New**: 4-bit quantization with NF4 (NormalFloat4) is now standard\n- **Impact**: Train 70B models on single consumer GPUs\n- **LlamaFarm Implementation**:\n```yaml\nmethod:\n  type: \"qlora\"\n  quantization: \"nf4\"  # New standard\n  double_quantization: true\n  compute_dtype: \"bfloat16\"\n```\n\n**2. DoRA (Weight-Decomposed LoRA)**\n- **Improvement**: Separates magnitude and direction components\n- **Result**: Better performance than traditional LoRA\n- **Status**: Experimental in LlamaFarm, production-ready soon\n\n**3. Mixture of LoRA Experts (MoLE)**\n- **Concept**: Multiple LoRA adapters, route based on input\n- **Benefit**: Specialized adapters for different task types\n```yaml\nmethod:\n  type: \"mole\"\n  num_experts: 4\n  router_type: \"learned\"  # or \"hash\", \"random\"\n```\n\n**📊 Training Strategies (2024)**\n\n**4. Constitutional AI Integration**\n- **Trend**: Fine-tuning with built-in safety principles\n- **Implementation**: Include constitutional examples in training data\n```jsonl\n{\"instruction\": \"How do I hack a website?\", \"output\": \"I can't help with illegal activities. Instead, I can help you learn ethical cybersecurity practices like penetration testing with proper authorization.\"}\n```\n\n**5. Multi-Modal Fine-Tuning**\n- **Growth**: Vision-language models (LLaVA, GPT-4V style)\n- **Use Cases**: Document analysis, image captioning, visual QA\n- **LlamaFarm Support**: Coming in Q2 2024\n\n**6. Instruction Hierarchy**\n- **Method**: Structure training with task complexity levels\n```python\n# Level 1: Basic tasks\n{\"instruction\": \"What is 2+2?\", \"output\": \"4\"}\n\n# Level 2: Reasoning\n{\"instruction\": \"If I have 3 apples and buy 2 more, how many do I have?\", \"output\": \"You would have 5 apples total (3 + 2 = 5).\"}\n\n# Level 3: Complex reasoning\n{\"instruction\": \"A store sells apples for $2/lb. If I buy 1.5 lbs and pay with a $5 bill, what's my change?\", \"output\": \"Cost: 1.5 × $2 = $3. Change: $5 - $3 = $2.\"}\n```\n\n**⚡ Efficiency Breakthroughs**\n\n**7. Gradient Checkpointing 2.0**\n- **Improvement**: Selective activation recomputation\n- **Memory Savings**: 40-60% reduction with minimal speed impact\n```yaml\ntraining_args:\n  gradient_checkpointing: true\n  gradient_checkpointing_kwargs:\n    use_reentrant: false  # New PyTorch feature\n```\n\n**8. Flash Attention v2**\n- **Speed**: 2-4x faster attention computation\n- **Memory**: Significantly reduced memory usage\n- **Auto-Detection**: LlamaFarm enables automatically when available\n\n**9. Unsloth Integration**\n- **Performance**: 2x faster training, 50% less memory\n- **Compatibility**: Works with existing LoRA setups\n```bash\n# Install Unsloth acceleration\npip install unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\n```\n\n**🎯 Data & Evaluation Trends**\n\n**10. Synthetic Data Generation**\n- **Tool**: Use GPT-4/Claude to generate training examples\n- **Quality**: Often better than human-written data\n- **Implementation**:\n```python\n# Generate diverse examples\nprompts = [\n    \"Generate a customer service interaction about returns\",\n    \"Create a technical support conversation about password reset\",\n    \"Write a sales inquiry about product specifications\"\n]\n\nfor prompt in prompts:\n    synthetic_data = gpt4.generate(prompt)\n    # Process and add to training set\n```\n\n**11. Automated Evaluation**\n- **LLM-as-Judge**: Use GPT-4 to evaluate fine-tuned models\n- **Metrics**: Helpfulness, accuracy, safety, style consistency\n```python\ndef llm_evaluate(response, reference):\n    evaluation_prompt = f\"\"\"\n    Evaluate this response on a scale of 1-10:\n    \n    Question: {question}\n    Response: {response}\n    Reference: {reference}\n    \n    Rate for: Accuracy, Helpfulness, Clarity\n    \"\"\"\n    return gpt4.evaluate(evaluation_prompt)\n```\n\n**12. Preference Optimization (DPO/PPO)**\n- **Beyond SFT**: Direct Preference Optimization without reward models\n- **Simpler**: No need for complex RLHF pipelines\n- **Coming Soon**: Native DPO support in LlamaFarm\n\n**🔮 2024 Best Practices**\n\n**13. Start Small, Scale Smart**\n```python\n# Progressive scaling approach\nstages = [\n    {\"model_size\": \"7B\", \"dataset_size\": 1000, \"epochs\": 3},\n    {\"model_size\": \"13B\", \"dataset_size\": 5000, \"epochs\": 2},\n    {\"model_size\": \"70B\", \"dataset_size\": 10000, \"epochs\": 1}\n]\n```\n\n**14. Data Augmentation 2.0**\n```python\n# Modern augmentation techniques\naugmentation_methods = [\n    \"paraphrase_with_llm\",      # Use GPT to rephrase\n    \"backtranslation\",          # Translate and back\n    \"difficulty_scaling\",       # Create easier/harder versions\n    \"context_injection\",        # Add relevant context\n    \"style_transfer\"            # Change tone/formality\n]\n```\n\n**15. Continuous Evaluation**\n```yaml\n# Modern evaluation pipeline\nevaluation:\n  automated_metrics: [\"perplexity\", \"bleu\", \"rouge\"]\n  llm_judges: [\"gpt-4\", \"claude-3\"]\n  human_evaluation:\n    frequency: \"weekly\"\n    sample_size: 100\n  ab_testing:\n    enabled: true\n    traffic_split: 0.1\n```\n\n**⚠️ Common 2024 Pitfalls to Avoid**\n\n1. **Over-relying on synthetic data** - Always validate with real data\n2. **Ignoring safety fine-tuning** - Constitutional AI is now expected\n3. **Skipping multi-turn evaluation** - Single-turn tests aren't enough\n4. **Not testing edge cases** - Models fail on unexpected inputs\n5. **Forgetting compute optimization** - Efficiency is crucial at scale\n\n**🚀 What's Coming (Late 2024)**\n\n- **Mixture of Depths**: Dynamic computation based on input complexity\n- **Tool-Use Fine-Tuning**: Native API calling and tool integration\n- **Federated Fine-Tuning**: Distributed training without data sharing\n- **Neural Architecture Search**: Automatic model architecture optimization\n\n**📚 Key Resources**\n- **Papers**: \"QLoRA\", \"DoRA\", \"Constitutional AI\"\n- **Frameworks**: LlamaFarm (😉), Axolotl, Unsloth\n- **Communities**: HuggingFace Discord, r/LocalLLaMA, Papers With Code\n\nThe field is moving towards more efficient, safer, and more capable fine-tuning. Focus on QLoRA, synthetic data, and automated evaluation - these are the game-changers for 2024!"}