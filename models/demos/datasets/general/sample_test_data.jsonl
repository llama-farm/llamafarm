{"instruction": "What is LlamaFarm?", "input": "", "output": "LlamaFarm is a comprehensive AI model management system that provides unified access to multiple AI providers, fine-tuning capabilities, and advanced model optimization features."}
{"instruction": "How do I start fine-tuning a model?", "input": "", "output": "Use the command 'python cli.py finetune start --strategy <strategy_name>' or 'python cli.py finetune start --config <config_file>' with your dataset and configuration."}
{"instruction": "What fine-tuning methods does LlamaFarm support?", "input": "", "output": "LlamaFarm supports LoRA (Low-Rank Adaptation), QLoRA (Quantized LoRA), and full fine-tuning methods through PyTorch and LlamaFactory frameworks."}
{"instruction": "How do I configure a model in LlamaFarm?", "input": "", "output": "You can configure models using YAML configuration files with the 'base_model', 'method', 'framework', and 'training_args' sections. Use strategy-based configurations for common setups."}
{"instruction": "What hardware does LlamaFarm optimize for?", "input": "", "output": "LlamaFarm provides optimization strategies for Mac M1/M2 with Metal acceleration, NVIDIA GPUs with CUDA, cloud environments, and CPU-only setups."}
{"instruction": "What datasets formats are supported?", "input": "", "output": "LlamaFarm supports JSONL, Alpaca format, and custom dataset formats. The system includes processors for automatic format conversion and validation."}
{"instruction": "How do I monitor fine-tuning progress?", "input": "", "output": "LlamaFarm provides real-time logging, metrics tracking, and checkpoint saving. You can monitor progress through the CLI output and saved logs."}
{"instruction": "What is LoRA fine-tuning?", "input": "", "output": "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method that only trains a small number of additional parameters, making it faster and requiring less memory."}
{"instruction": "How does QLoRA differ from LoRA?", "input": "", "output": "QLoRA combines LoRA with quantization techniques, further reducing memory requirements by using 4-bit precision for the base model while training LoRA adapters in full precision."}
{"instruction": "When should I use full fine-tuning vs LoRA?", "input": "", "output": "Use LoRA for most cases as it's efficient and effective. Use full fine-tuning when you need maximum adaptation capability and have sufficient computational resources."}
{"instruction": "How do I validate my dataset before fine-tuning?", "input": "", "output": "Use the dataset validation features in LlamaFarm which check format compliance, required fields, data quality, and provide statistics about your dataset."}
{"instruction": "Can I use OpenAI models with LlamaFarm?", "input": "", "output": "Yes, LlamaFarm integrates with OpenAI's API and supports using OpenAI models alongside local models for comparison and ensemble approaches."}
{"instruction": "How do I set up Ollama integration?", "input": "", "output": "LlamaFarm automatically detects Ollama installations and can pull, manage, and use Ollama models through the unified interface."}
{"instruction": "What is a strategy-based configuration?", "input": "", "output": "Strategy-based configurations are pre-defined setups optimized for specific use cases like 'mac_m1_lora', 'gpu_full_finetune', or 'cpu_efficient' that automatically configure all parameters."}
{"instruction": "How do I create a custom fine-tuning strategy?", "input": "", "output": "Create a new YAML file in the strategies directory with your custom configuration parameters for base_model, method, framework, and training_args sections."}
{"instruction": "How do I test a fine-tuned model?", "input": "", "output": "LlamaFarm provides testing utilities to compare before/after performance, run validation datasets, and generate sample outputs to evaluate fine-tuning effectiveness."}
{"instruction": "Can I continue training from a checkpoint?", "input": "", "output": "Yes, LlamaFarm supports resuming training from checkpoints. Specify the checkpoint path in your configuration to continue from where you left off."}
{"instruction": "How do I optimize training for my hardware?", "input": "", "output": "Use the appropriate strategy for your hardware (mac_m1, gpu, cpu) or customize training_args like batch_size, gradient_accumulation_steps, and mixed_precision settings."}
{"instruction": "What is gradient accumulation?", "input": "", "output": "Gradient accumulation allows training with larger effective batch sizes by accumulating gradients over multiple forward passes before performing a backward pass."}
{"instruction": "How do I handle out-of-memory errors?", "input": "", "output": "Reduce batch_size, enable gradient_checkpointing, use gradient_accumulation_steps, or switch to a more memory-efficient method like QLoRA."}
{"instruction": "What learning rates work best?", "input": "", "output": "For LoRA, typical learning rates are 1e-4 to 5e-4. For full fine-tuning, use lower rates like 1e-5 to 1e-4. LlamaFarm strategies include optimized defaults."}
{"instruction": "How many epochs should I train for?", "input": "", "output": "Start with 1-3 epochs for most tasks. Monitor validation loss to avoid overfitting. LlamaFarm provides early stopping and validation monitoring."}
{"instruction": "How do I prepare my training data?", "input": "", "output": "Format your data in JSONL or Alpaca format with instruction/input/output fields. Ensure quality, diversity, and sufficient examples (100+ recommended for meaningful fine-tuning)."}
{"instruction": "What makes good training examples?", "input": "", "output": "Good examples are diverse, high-quality, relevant to your use case, and follow consistent formatting. Include edge cases and various difficulty levels."}
{"instruction": "How do I evaluate fine-tuning results?", "input": "", "output": "Compare model outputs before and after fine-tuning, run evaluation datasets, measure metrics like perplexity or task-specific scores, and test real-world scenarios."}
{"instruction": "Can I fine-tune multiple models simultaneously?", "input": "", "output": "Yes, LlamaFarm supports parallel fine-tuning jobs. Configure different strategies or models and run them concurrently based on your hardware capacity."}
{"instruction": "How do I deploy a fine-tuned model?", "input": "", "output": "LlamaFarm fine-tuned models can be deployed through the same interface as base models. The system handles loading adapters and provides unified inference APIs."}