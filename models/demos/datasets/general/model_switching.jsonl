{"instruction": "What's the difference between GPT-4o-mini and GPT-4o?", "input": "", "output": "GPT-4o-mini is a smaller, faster, and more cost-effective version of GPT-4o designed for simpler tasks. GPT-4o is the full model with superior reasoning capabilities for complex tasks. GPT-4o-mini is ideal for chatbots, content moderation, and simple Q&A, while GPT-4o excels at research, analysis, and complex problem-solving."}
{"instruction": "Explain the concept of model temperature in AI", "input": "", "output": "Temperature is a parameter that controls randomness in AI model outputs. Lower temperatures (0.1-0.3) produce more focused, deterministic responses, while higher temperatures (0.7-1.0) increase creativity and variation. Temperature 0 gives the most likely response every time, while temperature 1.0 provides maximum diversity. Choose based on your use case: low for factual Q&A, medium for balanced responses, high for creative writing."}
{"instruction": "How do I choose the right OpenAI model for my application?", "input": "", "output": "Consider these factors:\n\n1. **Task complexity**: GPT-4o for reasoning, GPT-4o-mini for simple tasks\n2. **Cost**: GPT-4o-mini is ~60x cheaper than GPT-4o\n3. **Speed**: GPT-4o-mini responds faster\n4. **Context length**: Both support 128K tokens\n5. **Use cases**:\n   - GPT-4o-mini: Customer support, content moderation, simple coding\n   - GPT-4o: Research, complex analysis, advanced reasoning\n   - GPT-4-turbo: Balance of capability and cost\n\nStart with GPT-4o-mini and upgrade only if you need the extra capability."}
{"instruction": "Write a Python function to switch between OpenAI models dynamically", "input": "", "output": "```python\nfrom openai import OpenAI\nimport os\n\nclass ModelSwitcher:\n    def __init__(self, api_key=None):\n        self.client = OpenAI(api_key=api_key or os.getenv('OPENAI_API_KEY'))\n        self.models = {\n            'fast': 'gpt-4o-mini',\n            'balanced': 'gpt-4-turbo',\n            'powerful': 'gpt-4o'\n        }\n    \n    def generate(self, prompt, complexity='balanced', temperature=0.7):\n        model = self.models.get(complexity, 'gpt-4o-mini')\n        \n        response = self.client.chat.completions.create(\n            model=model,\n            messages=[{'role': 'user', 'content': prompt}],\n            temperature=temperature,\n            max_tokens=1000\n        )\n        \n        return {\n            'response': response.choices[0].message.content,\n            'model_used': model,\n            'tokens': response.usage.total_tokens\n        }\n\n# Usage example\nswitcher = ModelSwitcher()\n\n# For simple tasks - use fast model\nresult = switcher.generate('What is 2+2?', complexity='fast')\n\n# For complex reasoning - use powerful model\nresult = switcher.generate('Explain quantum computing', complexity='powerful')\n```"}
{"instruction": "What are the cost implications of using different OpenAI models?", "input": "", "output": "OpenAI model pricing (per 1M tokens):\n\n**GPT-4o-mini**:\n- Input: $0.15\n- Output: $0.60\n- Best for: High-volume, simple tasks\n\n**GPT-4o**:\n- Input: $5.00\n- Output: $15.00\n- Best for: Complex reasoning when quality matters\n\n**GPT-4-turbo**:\n- Input: $10.00\n- Output: $30.00\n- Best for: Balanced performance/cost\n\n**Cost optimization strategies**:\n1. Use GPT-4o-mini for 80% of tasks\n2. Reserve GPT-4o for complex analysis\n3. Implement smart routing based on query complexity\n4. Use prompt engineering to reduce token usage\n5. Cache common responses to avoid repeated API calls"}
{"instruction": "Explain how to implement a smart model routing system", "input": "", "output": "A smart routing system automatically selects the optimal model based on query characteristics:\n\n```python\nimport re\nfrom typing import Dict, List\n\nclass SmartRouter:\n    def __init__(self):\n        self.complexity_indicators = {\n            'simple': [\n                r'what is\\b', r'define\\b', r'who is\\b',\n                r'when was\\b', r'where is\\b', r'how many\\b'\n            ],\n            'complex': [\n                r'analyze\\b', r'compare\\b', r'evaluate\\b',\n                r'reasoning\\b', r'strategy\\b', r'algorithm\\b'\n            ]\n        }\n    \n    def classify_query(self, query: str) -> str:\n        query_lower = query.lower()\n        \n        # Check for complex indicators first\n        for pattern in self.complexity_indicators['complex']:\n            if re.search(pattern, query_lower):\n                return 'powerful'\n        \n        # Check for simple indicators\n        for pattern in self.complexity_indicators['simple']:\n            if re.search(pattern, query_lower):\n                return 'fast'\n        \n        # Default to balanced for medium complexity\n        return 'balanced' if len(query.split()) > 10 else 'fast'\n    \n    def route_query(self, query: str) -> Dict:\n        complexity = self.classify_query(query)\n        \n        model_map = {\n            'fast': 'gpt-4o-mini',\n            'balanced': 'gpt-4-turbo', \n            'powerful': 'gpt-4o'\n        }\n        \n        return {\n            'model': model_map[complexity],\n            'complexity': complexity,\n            'reasoning': f'Query classified as {complexity} based on content analysis'\n        }\n```\n\nThis approach can reduce costs by 40-60% while maintaining quality."}
{"instruction": "How do I handle rate limits across different OpenAI models?", "input": "", "output": "Different OpenAI models have different rate limits. Here's how to handle them:\n\n```python\nimport asyncio\nimport time\nfrom typing import Dict, Any\nfrom openai import OpenAI, RateLimitError\n\nclass RateLimitedClient:\n    def __init__(self):\n        self.client = OpenAI()\n        self.rate_limits = {\n            'gpt-4o-mini': {'rpm': 30000, 'tpm': 200000},\n            'gpt-4o': {'rpm': 10000, 'tpm': 30000},\n            'gpt-4-turbo': {'rpm': 10000, 'tpm': 30000}\n        }\n        self.request_history = {}\n    \n    async def generate_with_backoff(self, model: str, messages: list, \n                                  max_retries: int = 3) -> Dict[str, Any]:\n        for attempt in range(max_retries):\n            try:\n                response = await self.client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    timeout=30\n                )\n                return {'success': True, 'response': response}\n                \n            except RateLimitError as e:\n                if attempt == max_retries - 1:\n                    return {'success': False, 'error': str(e)}\n                \n                # Exponential backoff\n                wait_time = (2 ** attempt) + random.uniform(0, 1)\n                await asyncio.sleep(wait_time)\n                \n            except Exception as e:\n                return {'success': False, 'error': str(e)}\n    \n    def get_optimal_model(self, current_load: Dict[str, int]) -> str:\n        \"\"\"Choose model based on current rate limit usage\"\"\"\n        for model in ['gpt-4o-mini', 'gpt-4-turbo', 'gpt-4o']:\n            limits = self.rate_limits[model]\n            current_rpm = current_load.get(model, 0)\n            \n            if current_rpm < limits['rpm'] * 0.8:  # 80% threshold\n                return model\n        \n        return 'gpt-4o-mini'  # Fallback to cheapest\n```"}
{"instruction": "Create a configuration system for managing multiple OpenAI models", "input": "", "output": "Here's a flexible configuration system for OpenAI model management:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Dict, Optional, List\nimport yaml\nimport os\n\n@dataclass\nclass ModelConfig:\n    name: str\n    api_name: str\n    max_tokens: int\n    temperature: float\n    cost_per_1k_input: float\n    cost_per_1k_output: float\n    rpm_limit: int\n    use_cases: List[str]\n    \nclass OpenAIModelManager:\n    def __init__(self, config_path: str = 'models_config.yaml'):\n        self.config_path = config_path\n        self.models = self._load_config()\n        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n    \n    def _load_config(self) -> Dict[str, ModelConfig]:\n        \"\"\"Load model configurations from YAML\"\"\"\n        default_config = {\n            'models': {\n                'gpt-4o-mini': {\n                    'api_name': 'gpt-4o-mini',\n                    'max_tokens': 4096,\n                    'temperature': 0.7,\n                    'cost_per_1k_input': 0.15,\n                    'cost_per_1k_output': 0.60,\n                    'rpm_limit': 30000,\n                    'use_cases': ['chat', 'simple_qa', 'content_moderation']\n                },\n                'gpt-4o': {\n                    'api_name': 'gpt-4o',\n                    'max_tokens': 4096,\n                    'temperature': 0.7,\n                    'cost_per_1k_input': 5.00,\n                    'cost_per_1k_output': 15.00,\n                    'rpm_limit': 10000,\n                    'use_cases': ['analysis', 'reasoning', 'complex_tasks']\n                }\n            }\n        }\n        \n        if os.path.exists(self.config_path):\n            with open(self.config_path, 'r') as f:\n                config = yaml.safe_load(f)\n        else:\n            config = default_config\n            self._save_config(config)\n        \n        models = {}\n        for name, model_data in config['models'].items():\n            models[name] = ModelConfig(name=name, **model_data)\n        \n        return models\n    \n    def _save_config(self, config: Dict):\n        \"\"\"Save configuration to YAML file\"\"\"\n        with open(self.config_path, 'w') as f:\n            yaml.dump(config, f, default_flow_style=False)\n    \n    def get_model_for_task(self, task_type: str) -> Optional[ModelConfig]:\n        \"\"\"Get the best model for a specific task type\"\"\"\n        for model in self.models.values():\n            if task_type in model.use_cases:\n                return model\n        return self.models.get('gpt-4o-mini')  # Default fallback\n    \n    def estimate_cost(self, model_name: str, input_tokens: int, \n                     output_tokens: int) -> float:\n        \"\"\"Estimate cost for a request\"\"\"\n        model = self.models.get(model_name)\n        if not model:\n            return 0.0\n        \n        input_cost = (input_tokens / 1000) * model.cost_per_1k_input\n        output_cost = (output_tokens / 1000) * model.cost_per_1k_output\n        return input_cost + output_cost\n```"}
{"instruction": "Demonstrate A/B testing different OpenAI models for the same task", "input": "", "output": "Here's a system for A/B testing different OpenAI models:\n\n```python\nimport asyncio\nimport random\nimport time\nfrom typing import Dict, List, Tuple\nfrom dataclasses import dataclass\nfrom openai import OpenAI\n\n@dataclass\nclass TestResult:\n    model: str\n    response: str\n    response_time: float\n    tokens_used: int\n    cost: float\n    quality_score: float = 0.0\n\nclass ModelABTester:\n    def __init__(self, models: List[str]):\n        self.client = OpenAI()\n        self.models = models\n        self.results = []\n    \n    async def test_models(self, prompt: str, num_tests: int = 10) -> Dict[str, List[TestResult]]:\n        \"\"\"Run A/B tests across multiple models\"\"\"\n        results = {model: [] for model in self.models}\n        \n        tasks = []\n        for _ in range(num_tests):\n            for model in self.models:\n                tasks.append(self._single_test(model, prompt))\n        \n        # Run all tests concurrently\n        test_results = await asyncio.gather(*tasks)\n        \n        # Group results by model\n        for i, result in enumerate(test_results):\n            model_index = i % len(self.models)\n            model = self.models[model_index]\n            results[model].append(result)\n        \n        return results\n    \n    async def _single_test(self, model: str, prompt: str) -> TestResult:\n        \"\"\"Run a single test for one model\"\"\"\n        start_time = time.time()\n        \n        try:\n            response = await self.client.chat.completions.create(\n                model=model,\n                messages=[{'role': 'user', 'content': prompt}],\n                temperature=0.7,\n                max_tokens=500\n            )\n            \n            response_time = time.time() - start_time\n            tokens_used = response.usage.total_tokens\n            cost = self._calculate_cost(model, tokens_used)\n            \n            return TestResult(\n                model=model,\n                response=response.choices[0].message.content,\n                response_time=response_time,\n                tokens_used=tokens_used,\n                cost=cost\n            )\n            \n        except Exception as e:\n            return TestResult(\n                model=model,\n                response=f\"Error: {str(e)}\",\n                response_time=time.time() - start_time,\n                tokens_used=0,\n                cost=0.0\n            )\n    \n    def _calculate_cost(self, model: str, tokens: int) -> float:\n        \"\"\"Calculate approximate cost\"\"\"\n        cost_per_1k = {\n            'gpt-4o-mini': 0.375,  # Average of input/output\n            'gpt-4o': 10.0,\n            'gpt-4-turbo': 20.0\n        }\n        return (tokens / 1000) * cost_per_1k.get(model, 1.0)\n    \n    def analyze_results(self, results: Dict[str, List[TestResult]]) -> Dict:\n        \"\"\"Analyze A/B test results\"\"\"\n        analysis = {}\n        \n        for model, test_results in results.items():\n            if not test_results:\n                continue\n                \n            successful_tests = [r for r in test_results if not r.response.startswith('Error')]\n            \n            if successful_tests:\n                analysis[model] = {\n                    'avg_response_time': sum(r.response_time for r in successful_tests) / len(successful_tests),\n                    'avg_tokens': sum(r.tokens_used for r in successful_tests) / len(successful_tests),\n                    'avg_cost': sum(r.cost for r in successful_tests) / len(successful_tests),\n                    'success_rate': len(successful_tests) / len(test_results),\n                    'total_tests': len(test_results)\n                }\n            else:\n                analysis[model] = {\n                    'avg_response_time': 0,\n                    'avg_tokens': 0,\n                    'avg_cost': 0,\n                    'success_rate': 0,\n                    'total_tests': len(test_results)\n                }\n        \n        return analysis\n\n# Usage example\nasync def run_ab_test():\n    tester = ModelABTester(['gpt-4o-mini', 'gpt-4o'])\n    \n    test_prompt = \"Explain the benefits of renewable energy in 2 paragraphs.\"\n    \n    results = await tester.test_models(test_prompt, num_tests=5)\n    analysis = tester.analyze_results(results)\n    \n    print(\"A/B Test Results:\")\n    for model, stats in analysis.items():\n        print(f\"\\n{model}:\")\n        print(f\"  Success Rate: {stats['success_rate']:.2%}\")\n        print(f\"  Avg Response Time: {stats['avg_response_time']:.2f}s\")\n        print(f\"  Avg Cost: ${stats['avg_cost']:.4f}\")\n        print(f\"  Avg Tokens: {stats['avg_tokens']:.0f}\")\n```"}
{"instruction": "How do I implement failover between OpenAI models?", "input": "", "output": "Here's a robust failover system for OpenAI models:\n\n```python\nimport asyncio\nimport logging\nfrom typing import List, Dict, Optional, Any\nfrom enum import Enum\nfrom openai import OpenAI, RateLimitError, APIError\nimport time\n\nclass ModelPriority(Enum):\n    PRIMARY = 1\n    SECONDARY = 2\n    FALLBACK = 3\n\nclass FailoverClient:\n    def __init__(self):\n        self.client = OpenAI()\n        self.model_hierarchy = [\n            {'model': 'gpt-4o', 'priority': ModelPriority.PRIMARY, 'max_retries': 2},\n            {'model': 'gpt-4-turbo', 'priority': ModelPriority.SECONDARY, 'max_retries': 2},\n            {'model': 'gpt-4o-mini', 'priority': ModelPriority.FALLBACK, 'max_retries': 3}\n        ]\n        self.model_health = {}\n        self.circuit_breaker = {}\n        \n    async def generate_with_failover(self, messages: List[Dict], \n                                   **kwargs) -> Dict[str, Any]:\n        \"\"\"Generate response with automatic failover\"\"\"\n        last_error = None\n        \n        for model_config in self.model_hierarchy:\n            model = model_config['model']\n            \n            # Check circuit breaker\n            if self._is_circuit_open(model):\n                logging.warning(f\"Circuit breaker open for {model}, skipping\")\n                continue\n            \n            try:\n                response = await self._try_model(\n                    model, messages, model_config['max_retries'], **kwargs\n                )\n                \n                if response['success']:\n                    self._record_success(model)\n                    return {\n                        'response': response['data'],\n                        'model_used': model,\n                        'failover_occurred': model != self.model_hierarchy[0]['model']\n                    }\n                    \n                last_error = response['error']\n                self._record_failure(model, last_error)\n                \n            except Exception as e:\n                last_error = str(e)\n                self._record_failure(model, last_error)\n                logging.error(f\"Model {model} failed: {e}\")\n        \n        # All models failed\n        return {\n            'response': None,\n            'model_used': None,\n            'error': f\"All models failed. Last error: {last_error}\",\n            'failover_occurred': True\n        }\n    \n    async def _try_model(self, model: str, messages: List[Dict], \n                        max_retries: int, **kwargs) -> Dict[str, Any]:\n        \"\"\"Try a specific model with retries\"\"\"\n        for attempt in range(max_retries):\n            try:\n                response = await self.client.chat.completions.create(\n                    model=model,\n                    messages=messages,\n                    timeout=30,\n                    **kwargs\n                )\n                \n                return {\n                    'success': True,\n                    'data': response,\n                    'error': None\n                }\n                \n            except RateLimitError as e:\n                if attempt == max_retries - 1:\n                    return {'success': False, 'data': None, 'error': f\"Rate limit: {e}\"}\n                \n                # Exponential backoff for rate limits\n                wait_time = (2 ** attempt) * 1.0\n                await asyncio.sleep(wait_time)\n                \n            except APIError as e:\n                if e.status_code >= 500:  # Server errors are retryable\n                    if attempt == max_retries - 1:\n                        return {'success': False, 'data': None, 'error': f\"API error: {e}\"}\n                    await asyncio.sleep(1)\n                else:\n                    # Client errors (4xx) are not retryable\n                    return {'success': False, 'data': None, 'error': f\"Client error: {e}\"}\n                    \n            except Exception as e:\n                return {'success': False, 'data': None, 'error': str(e)}\n        \n        return {'success': False, 'data': None, 'error': 'Max retries exceeded'}\n    \n    def _record_success(self, model: str):\n        \"\"\"Record successful request\"\"\"\n        if model not in self.model_health:\n            self.model_health[model] = {'successes': 0, 'failures': 0, 'last_success': time.time()}\n        \n        self.model_health[model]['successes'] += 1\n        self.model_health[model]['last_success'] = time.time()\n        \n        # Reset circuit breaker on success\n        if model in self.circuit_breaker:\n            self.circuit_breaker[model]['failure_count'] = 0\n    \n    def _record_failure(self, model: str, error: str):\n        \"\"\"Record failed request\"\"\"\n        if model not in self.model_health:\n            self.model_health[model] = {'successes': 0, 'failures': 0, 'last_failure': time.time()}\n        \n        self.model_health[model]['failures'] += 1\n        self.model_health[model]['last_failure'] = time.time()\n        \n        # Update circuit breaker\n        if model not in self.circuit_breaker:\n            self.circuit_breaker[model] = {'failure_count': 0, 'last_failure': time.time()}\n        \n        self.circuit_breaker[model]['failure_count'] += 1\n        self.circuit_breaker[model]['last_failure'] = time.time()\n    \n    def _is_circuit_open(self, model: str) -> bool:\n        \"\"\"Check if circuit breaker is open for model\"\"\"\n        if model not in self.circuit_breaker:\n            return False\n        \n        cb = self.circuit_breaker[model]\n        \n        # Open circuit if too many failures\n        if cb['failure_count'] >= 5:\n            # Keep circuit open for 5 minutes\n            if time.time() - cb['last_failure'] < 300:\n                return True\n            else:\n                # Reset circuit breaker after timeout\n                cb['failure_count'] = 0\n                return False\n        \n        return False\n    \n    def get_health_status(self) -> Dict[str, Dict]:\n        \"\"\"Get health status of all models\"\"\"\n        status = {}\n        \n        for model_config in self.model_hierarchy:\n            model = model_config['model']\n            health = self.model_health.get(model, {'successes': 0, 'failures': 0})\n            \n            total_requests = health['successes'] + health['failures']\n            success_rate = health['successes'] / total_requests if total_requests > 0 else 0\n            \n            status[model] = {\n                'success_rate': success_rate,\n                'total_requests': total_requests,\n                'circuit_open': self._is_circuit_open(model),\n                'priority': model_config['priority'].name\n            }\n        \n        return status\n\n# Usage example\nasync def main():\n    client = FailoverClient()\n    \n    messages = [\n        {'role': 'user', 'content': 'Explain machine learning in simple terms'}\n    ]\n    \n    result = await client.generate_with_failover(messages, temperature=0.7)\n    \n    if result['response']:\n        print(f\"Success with {result['model_used']}\")\n        print(f\"Failover occurred: {result['failover_occurred']}\")\n        print(f\"Response: {result['response'].choices[0].message.content}\")\n    else:\n        print(f\"All models failed: {result['error']}\")\n    \n    # Check health status\n    health = client.get_health_status()\n    print(\"\\nModel Health Status:\")\n    for model, stats in health.items():\n        print(f\"{model}: {stats['success_rate']:.2%} success rate, Circuit: {'OPEN' if stats['circuit_open'] else 'CLOSED'}\")\n```"}
{"instruction": "What are best practices for production deployment of OpenAI model switching?", "input": "", "output": "Here are production best practices for OpenAI model switching:\n\n## 1. **Monitoring & Observability**\n```python\nimport structlog\nfrom prometheus_client import Counter, Histogram, Gauge\n\n# Metrics\nmodel_requests = Counter('openai_requests_total', 'Total requests', ['model', 'status'])\nmodel_latency = Histogram('openai_request_duration_seconds', 'Request duration', ['model'])\nmodel_cost = Counter('openai_cost_total', 'Total cost in USD', ['model'])\nactive_models = Gauge('openai_active_models', 'Number of active models')\n\nclass ProductionModelClient:\n    def __init__(self):\n        self.logger = structlog.get_logger()\n        \n    async def generate(self, model: str, messages: list):\n        with model_latency.labels(model=model).time():\n            try:\n                response = await self.client.chat.completions.create(\n                    model=model, messages=messages\n                )\n                \n                # Log success metrics\n                model_requests.labels(model=model, status='success').inc()\n                cost = self._calculate_cost(model, response.usage.total_tokens)\n                model_cost.labels(model=model).inc(cost)\n                \n                self.logger.info(\"request_completed\", \n                    model=model,\n                    tokens=response.usage.total_tokens,\n                    cost=cost\n                )\n                \n                return response\n                \n            except Exception as e:\n                model_requests.labels(model=model, status='error').inc()\n                self.logger.error(\"request_failed\", model=model, error=str(e))\n                raise\n```\n\n## 2. **Configuration Management**\n```yaml\n# config/production.yaml\nmodels:\n  routing:\n    strategy: \"cost_optimized\"  # or \"performance_optimized\"\n    fallback_enabled: true\n    \n  gpt-4o-mini:\n    enabled: true\n    max_rpm: 30000\n    cost_per_1k_tokens: 0.375\n    use_cases: [\"simple_qa\", \"chat\", \"content_moderation\"]\n    \n  gpt-4o:\n    enabled: true\n    max_rpm: 10000\n    cost_per_1k_tokens: 10.0\n    use_cases: [\"analysis\", \"reasoning\", \"complex_tasks\"]\n    \nlimits:\n  daily_cost_limit: 1000.0  # USD\n  hourly_request_limit: 50000\n  \nfallback:\n  max_retries: 3\n  backoff_multiplier: 2\n  circuit_breaker_threshold: 10\n```\n\n## 3. **Cost Control & Budgets**\n```python\nclass CostController:\n    def __init__(self, redis_client):\n        self.redis = redis_client\n        self.daily_limit = 1000.0  # USD\n        \n    async def check_budget(self, estimated_cost: float) -> bool:\n        \"\"\"Check if request fits within budget\"\"\"\n        today = datetime.now().strftime('%Y-%m-%d')\n        current_spend = float(self.redis.get(f'cost:{today}') or 0)\n        \n        if current_spend + estimated_cost > self.daily_limit:\n            self.logger.warning(\"budget_exceeded\", \n                current=current_spend, \n                estimated=estimated_cost,\n                limit=self.daily_limit\n            )\n            return False\n        return True\n    \n    async def record_cost(self, cost: float):\n        \"\"\"Record actual cost\"\"\"\n        today = datetime.now().strftime('%Y-%m-%d')\n        self.redis.incrbyfloat(f'cost:{today}', cost)\n        self.redis.expire(f'cost:{today}', 86400 * 7)  # Keep 7 days\n```\n\n## 4. **Smart Caching**\n```python\nimport hashlib\nfrom typing import Optional\n\nclass ResponseCache:\n    def __init__(self, redis_client):\n        self.redis = redis_client\n        self.ttl = 3600  # 1 hour\n        \n    def _get_cache_key(self, model: str, messages: list, **kwargs) -> str:\n        \"\"\"Generate cache key for request\"\"\"\n        content = json.dumps({\n            'model': model,\n            'messages': messages,\n            'params': kwargs\n        }, sort_keys=True)\n        return f\"openai_cache:{hashlib.md5(content.encode()).hexdigest()}\"\n    \n    async def get(self, model: str, messages: list, **kwargs) -> Optional[dict]:\n        \"\"\"Get cached response\"\"\"\n        key = self._get_cache_key(model, messages, **kwargs)\n        cached = self.redis.get(key)\n        \n        if cached:\n            self.logger.info(\"cache_hit\", model=model, key=key)\n            return json.loads(cached)\n        return None\n    \n    async def set(self, model: str, messages: list, response: dict, **kwargs):\n        \"\"\"Cache response\"\"\"\n        key = self._get_cache_key(model, messages, **kwargs)\n        self.redis.setex(key, self.ttl, json.dumps(response))\n```\n\n## 5. **Health Checks & Alerting**\n```python\nclass HealthChecker:\n    def __init__(self):\n        self.models = ['gpt-4o-mini', 'gpt-4o', 'gpt-4-turbo']\n        self.health_status = {}\n        \n    async def check_model_health(self, model: str) -> dict:\n        \"\"\"Check if model is healthy\"\"\"\n        try:\n            start_time = time.time()\n            \n            response = await self.client.chat.completions.create(\n                model=model,\n                messages=[{'role': 'user', 'content': 'Health check'}],\n                max_tokens=10,\n                timeout=5\n            )\n            \n            latency = time.time() - start_time\n            \n            return {\n                'status': 'healthy',\n                'latency': latency,\n                'timestamp': time.time()\n            }\n            \n        except Exception as e:\n            return {\n                'status': 'unhealthy',\n                'error': str(e),\n                'timestamp': time.time()\n            }\n    \n    async def run_health_checks(self):\n        \"\"\"Run health checks for all models\"\"\"\n        tasks = [self.check_model_health(model) for model in self.models]\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        \n        for model, result in zip(self.models, results):\n            self.health_status[model] = result\n            \n            if result['status'] == 'unhealthy':\n                await self.send_alert(model, result['error'])\n```\n\n## 6. **Environment-Specific Deployment**\n```python\n# production.py\nclass ProductionConfig:\n    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n    REDIS_URL = os.getenv('REDIS_URL')\n    \n    # Rate limiting\n    RATE_LIMIT_ENABLED = True\n    MAX_REQUESTS_PER_MINUTE = 1000\n    \n    # Monitoring\n    PROMETHEUS_ENABLED = True\n    STRUCTURED_LOGGING = True\n    \n    # Caching\n    CACHE_ENABLED = True\n    CACHE_TTL = 3600\n    \n    # Model selection\n    DEFAULT_MODEL = 'gpt-4o-mini'\n    ENABLE_SMART_ROUTING = True\n    \n# staging.py\nclass StagingConfig:\n    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY_STAGING')\n    \n    # More aggressive caching for cost savings\n    CACHE_TTL = 7200\n    \n    # Lower rate limits\n    MAX_REQUESTS_PER_MINUTE = 100\n```\n\n## 7. **Key Production Recommendations**\n\n1. **Cost Management**:\n   - Set daily/monthly budget limits\n   - Use GPT-4o-mini for 70-80% of requests\n   - Implement request deduplication\n   - Cache common responses\n\n2. **Reliability**:\n   - Implement circuit breakers\n   - Use multiple fallback models\n   - Set appropriate timeouts\n   - Monitor error rates and latency\n\n3. **Security**:\n   - Store API keys in secure vaults\n   - Implement request validation\n   - Add rate limiting per user/API key\n   - Log all requests for audit\n\n4. **Performance**:\n   - Use async/await for concurrent requests\n   - Implement connection pooling\n   - Cache frequently requested data\n   - Monitor response times\n\n5. **Monitoring**:\n   - Track cost per model/endpoint\n   - Monitor success rates\n   - Alert on budget overruns\n   - Dashboard for real-time metrics"}
{"instruction": "How do I implement request queuing and batch processing for OpenAI models?", "input": "", "output": "Here's a comprehensive system for request queuing and batch processing:\n\n```python\nimport asyncio\nimport json\nfrom typing import List, Dict, Any, Optional\nfrom dataclasses import dataclass, asdict\nfrom enum import Enum\nimport time\nimport uuid\nfrom collections import defaultdict\nfrom openai import OpenAI\n\nclass RequestPriority(Enum):\n    LOW = 3\n    NORMAL = 2\n    HIGH = 1\n    URGENT = 0\n\n@dataclass\nclass QueuedRequest:\n    id: str\n    model: str\n    messages: List[Dict]\n    priority: RequestPriority\n    timestamp: float\n    user_id: Optional[str] = None\n    callback_url: Optional[str] = None\n    max_tokens: int = 1000\n    temperature: float = 0.7\n    timeout: float = 30.0\n    metadata: Dict[str, Any] = None\n\nclass BatchProcessor:\n    def __init__(self, max_batch_size: int = 10, max_wait_time: float = 5.0):\n        self.client = OpenAI()\n        self.max_batch_size = max_batch_size\n        self.max_wait_time = max_wait_time\n        self.request_queues = defaultdict(list)  # model -> list of requests\n        self.processing = False\n        self.results = {}  # request_id -> result\n        self.rate_limits = {\n            'gpt-4o-mini': {'rpm': 30000, 'tpm': 200000, 'current_rpm': 0},\n            'gpt-4o': {'rpm': 10000, 'tpm': 30000, 'current_rpm': 0},\n            'gpt-4-turbo': {'rpm': 10000, 'tpm': 30000, 'current_rpm': 0}\n        }\n        \n    def add_request(self, request: QueuedRequest) -> str:\n        \"\"\"Add request to appropriate queue\"\"\"\n        # Sort by priority and timestamp\n        queue = self.request_queues[request.model]\n        \n        # Insert in priority order\n        inserted = False\n        for i, existing in enumerate(queue):\n            if (request.priority.value < existing.priority.value or \n                (request.priority.value == existing.priority.value and \n                 request.timestamp < existing.timestamp)):\n                queue.insert(i, request)\n                inserted = True\n                break\n        \n        if not inserted:\n            queue.append(request)\n        \n        print(f\"📥 Queued request {request.id} for {request.model} (Priority: {request.priority.name})\")\n        return request.id\n    \n    async def start_processing(self):\n        \"\"\"Start the batch processing loop\"\"\"\n        if self.processing:\n            return\n        \n        self.processing = True\n        print(\"🚀 Starting batch processor...\")\n        \n        try:\n            while self.processing:\n                await self._process_batches()\n                await asyncio.sleep(0.1)  # Small delay to prevent busy waiting\n        finally:\n            self.processing = False\n    \n    def stop_processing(self):\n        \"\"\"Stop the batch processor\"\"\"\n        self.processing = False\n        print(\"⏹️  Stopping batch processor...\")\n    \n    async def _process_batches(self):\n        \"\"\"Process batches for all models\"\"\"\n        tasks = []\n        \n        for model, queue in self.request_queues.items():\n            if queue and self._can_process_model(model):\n                batch = self._create_batch(model, queue)\n                if batch:\n                    tasks.append(self._process_batch(model, batch))\n        \n        if tasks:\n            await asyncio.gather(*tasks, return_exceptions=True)\n    \n    def _can_process_model(self, model: str) -> bool:\n        \"\"\"Check if we can process requests for this model\"\"\"\n        limits = self.rate_limits.get(model, {'rpm': 1000, 'current_rpm': 0})\n        return limits['current_rpm'] < limits['rpm'] * 0.8  # 80% of rate limit\n    \n    def _create_batch(self, model: str, queue: List[QueuedRequest]) -> List[QueuedRequest]:\n        \"\"\"Create a batch from the queue\"\"\"\n        if not queue:\n            return []\n        \n        # Check if we should wait for more requests or process now\n        oldest_request_age = time.time() - queue[0].timestamp\n        \n        if len(queue) >= self.max_batch_size or oldest_request_age >= self.max_wait_time:\n            # Take up to max_batch_size requests\n            batch = queue[:self.max_batch_size]\n            # Remove from queue\n            del queue[:len(batch)]\n            return batch\n        \n        return []\n    \n    async def _process_batch(self, model: str, batch: List[QueuedRequest]):\n        \"\"\"Process a batch of requests\"\"\"\n        print(f\"🔄 Processing batch of {len(batch)} requests for {model}\")\n        \n        # Group by similar parameters for better batching\n        param_groups = defaultdict(list)\n        for request in batch:\n            key = (request.max_tokens, request.temperature)\n            param_groups[key].append(request)\n        \n        # Process each parameter group\n        tasks = []\n        for (max_tokens, temperature), requests in param_groups.items():\n            tasks.append(self._process_param_group(model, requests, max_tokens, temperature))\n        \n        await asyncio.gather(*tasks, return_exceptions=True)\n    \n    async def _process_param_group(self, model: str, requests: List[QueuedRequest], \n                                 max_tokens: int, temperature: float):\n        \"\"\"Process requests with the same parameters\"\"\"\n        # For OpenAI, we still need to make individual requests\n        # But we can do them concurrently within rate limits\n        \n        semaphore = asyncio.Semaphore(5)  # Limit concurrent requests per batch\n        \n        async def process_single_request(request: QueuedRequest):\n            async with semaphore:\n                try:\n                    start_time = time.time()\n                    \n                    response = await self.client.chat.completions.create(\n                        model=model,\n                        messages=request.messages,\n                        max_tokens=max_tokens,\n                        temperature=temperature,\n                        timeout=request.timeout\n                    )\n                    \n                    processing_time = time.time() - start_time\n                    \n                    # Store result\n                    self.results[request.id] = {\n                        'success': True,\n                        'response': response,\n                        'processing_time': processing_time,\n                        'timestamp': time.time()\n                    }\n                    \n                    # Update rate limit tracking\n                    self.rate_limits[model]['current_rpm'] += 1\n                    \n                    print(f\"✅ Completed request {request.id} in {processing_time:.2f}s\")\n                    \n                    # Send callback if provided\n                    if request.callback_url:\n                        await self._send_callback(request.callback_url, request.id, \n                                                self.results[request.id])\n                    \n                except Exception as e:\n                    print(f\"❌ Failed request {request.id}: {e}\")\n                    self.results[request.id] = {\n                        'success': False,\n                        'error': str(e),\n                        'timestamp': time.time()\n                    }\n                    \n                    if request.callback_url:\n                        await self._send_callback(request.callback_url, request.id, \n                                                self.results[request.id])\n        \n        # Process all requests in the group concurrently\n        tasks = [process_single_request(req) for req in requests]\n        await asyncio.gather(*tasks)\n    \n    async def _send_callback(self, callback_url: str, request_id: str, result: Dict):\n        \"\"\"Send callback notification\"\"\"\n        # Implementation would depend on your callback system\n        # Could be webhook, Redis pub/sub, message queue, etc.\n        print(f\"📞 Sending callback for {request_id} to {callback_url}\")\n    \n    def get_result(self, request_id: str) -> Optional[Dict]:\n        \"\"\"Get result for a specific request\"\"\"\n        return self.results.get(request_id)\n    \n    def get_queue_status(self) -> Dict[str, Any]:\n        \"\"\"Get current queue status\"\"\"\n        status = {\n            'processing': self.processing,\n            'queues': {},\n            'completed_requests': len(self.results),\n            'rate_limits': self.rate_limits\n        }\n        \n        for model, queue in self.request_queues.items():\n            priority_counts = defaultdict(int)\n            for request in queue:\n                priority_counts[request.priority.name] += 1\n            \n            status['queues'][model] = {\n                'total': len(queue),\n                'by_priority': dict(priority_counts),\n                'oldest_request_age': time.time() - queue[0].timestamp if queue else 0\n            }\n        \n        return status\n\n# Usage example and API wrapper\nclass OpenAIQueueAPI:\n    def __init__(self):\n        self.processor = BatchProcessor(max_batch_size=8, max_wait_time=3.0)\n        self._processing_task = None\n    \n    async def start(self):\n        \"\"\"Start the processing system\"\"\"\n        self._processing_task = asyncio.create_task(self.processor.start_processing())\n    \n    def stop(self):\n        \"\"\"Stop the processing system\"\"\"\n        self.processor.stop_processing()\n        if self._processing_task:\n            self._processing_task.cancel()\n    \n    def submit_request(self, model: str, messages: List[Dict], \n                      priority: RequestPriority = RequestPriority.NORMAL,\n                      **kwargs) -> str:\n        \"\"\"Submit a request to the queue\"\"\"\n        request = QueuedRequest(\n            id=str(uuid.uuid4()),\n            model=model,\n            messages=messages,\n            priority=priority,\n            timestamp=time.time(),\n            **kwargs\n        )\n        \n        return self.processor.add_request(request)\n    \n    async def get_result(self, request_id: str, timeout: float = 30.0) -> Optional[Dict]:\n        \"\"\"Wait for and get result\"\"\"\n        start_time = time.time()\n        \n        while time.time() - start_time < timeout:\n            result = self.processor.get_result(request_id)\n            if result:\n                return result\n            await asyncio.sleep(0.1)\n        \n        return None  # Timeout\n    \n    def get_status(self) -> Dict:\n        \"\"\"Get queue status\"\"\"\n        return self.processor.get_queue_status()\n\n# Example usage\nasync def main():\n    api = OpenAIQueueAPI()\n    await api.start()\n    \n    try:\n        # Submit various requests\n        request_ids = []\n        \n        # High priority request\n        req_id = api.submit_request(\n            model='gpt-4o-mini',\n            messages=[{'role': 'user', 'content': 'Urgent: What is 2+2?'}],\n            priority=RequestPriority.HIGH\n        )\n        request_ids.append(req_id)\n        \n        # Normal priority requests\n        for i in range(5):\n            req_id = api.submit_request(\n                model='gpt-4o-mini',\n                messages=[{'role': 'user', 'content': f'Question {i}: Explain AI'}],\n                priority=RequestPriority.NORMAL\n            )\n            request_ids.append(req_id)\n        \n        # Wait for results\n        print(\"\\n⏳ Waiting for results...\")\n        \n        for req_id in request_ids:\n            result = await api.get_result(req_id, timeout=60)\n            if result and result['success']:\n                response = result['response']\n                print(f\"✅ Request {req_id}: {response.choices[0].message.content[:100]}...\")\n            else:\n                print(f\"❌ Request {req_id} failed or timed out\")\n        \n        # Check final status\n        status = api.get_status()\n        print(f\"\\n📊 Final Status: {json.dumps(status, indent=2)}\")\n        \n    finally:\n        api.stop()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n```\n\nThis system provides:\n1. **Priority-based queuing** with multiple priority levels\n2. **Batch processing** with configurable batch sizes and wait times\n3. **Rate limit awareness** to avoid hitting API limits\n4. **Concurrent processing** within batches\n5. **Result storage** and retrieval\n6. **Status monitoring** and queue visibility\n7. **Callback support** for async notifications\n8. **Graceful error handling** and timeouts"}
{"instruction": "Create a comprehensive strategy switching demo for OpenAI models", "input": "", "output": "Here's a comprehensive strategy switching system for OpenAI models:\n\n```python\nfrom enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Any, Callable\nimport yaml\nimport json\nfrom openai import OpenAI\n\nclass TaskComplexity(Enum):\n    SIMPLE = \"simple\"\n    MODERATE = \"moderate\"\n    COMPLEX = \"complex\"\n    EXPERT = \"expert\"\n\nclass CostStrategy(Enum):\n    MINIMIZE_COST = \"minimize_cost\"\n    BALANCED = \"balanced\"\n    MAXIMIZE_QUALITY = \"maximize_quality\"\n    PERFORMANCE_FIRST = \"performance_first\"\n\n@dataclass\nclass ModelStrategy:\n    name: str\n    description: str\n    model: str\n    temperature: float\n    max_tokens: int\n    use_cases: List[str]\n    cost_per_1k_tokens: float\n    expected_quality: float  # 0-1 scale\n    response_time_ms: int\n    complexity_threshold: TaskComplexity\n\nclass StrategyManager:\n    def __init__(self, config_path: str = None):\n        self.client = OpenAI()\n        self.strategies = self._load_strategies(config_path)\n        self.usage_stats = {}\n        \n    def _load_strategies(self, config_path: str) -> Dict[str, ModelStrategy]:\n        \"\"\"Load strategy configurations\"\"\"\n        default_strategies = {\n            \"lightning_fast\": ModelStrategy(\n                name=\"lightning_fast\",\n                description=\"Ultra-fast responses for simple queries\",\n                model=\"gpt-4o-mini\",\n                temperature=0.3,\n                max_tokens=500,\n                use_cases=[\"simple_qa\", \"definitions\", \"basic_math\"],\n                cost_per_1k_tokens=0.375,\n                expected_quality=0.75,\n                response_time_ms=800,\n                complexity_threshold=TaskComplexity.SIMPLE\n            ),\n            \"balanced_performer\": ModelStrategy(\n                name=\"balanced_performer\",\n                description=\"Good balance of speed, cost, and quality\",\n                model=\"gpt-4o-mini\",\n                temperature=0.7,\n                max_tokens=1500,\n                use_cases=[\"general_chat\", \"content_writing\", \"code_help\"],\n                cost_per_1k_tokens=0.375,\n                expected_quality=0.85,\n                response_time_ms=1200,\n                complexity_threshold=TaskComplexity.MODERATE\n            ),\n            \"quality_focused\": ModelStrategy(\n                name=\"quality_focused\",\n                description=\"High-quality responses for complex tasks\",\n                model=\"gpt-4o\",\n                temperature=0.7,\n                max_tokens=2000,\n                use_cases=[\"analysis\", \"research\", \"complex_reasoning\"],\n                cost_per_1k_tokens=10.0,\n                expected_quality=0.95,\n                response_time_ms=2500,\n                complexity_threshold=TaskComplexity.COMPLEX\n            ),\n            \"expert_level\": ModelStrategy(\n                name=\"expert_level\",\n                description=\"Maximum capability for expert-level tasks\",\n                model=\"gpt-4o\",\n                temperature=0.2,\n                max_tokens=4000,\n                use_cases=[\"expert_analysis\", \"scientific_reasoning\", \"complex_coding\"],\n                cost_per_1k_tokens=10.0,\n                expected_quality=0.98,\n                response_time_ms=4000,\n                complexity_threshold=TaskComplexity.EXPERT\n            ),\n            \"creative_writer\": ModelStrategy(\n                name=\"creative_writer\",\n                description=\"Optimized for creative and diverse outputs\",\n                model=\"gpt-4o\",\n                temperature=1.0,\n                max_tokens=3000,\n                use_cases=[\"creative_writing\", \"brainstorming\", \"storytelling\"],\n                cost_per_1k_tokens=10.0,\n                expected_quality=0.88,\n                response_time_ms=3000,\n                complexity_threshold=TaskComplexity.MODERATE\n            )\n        }\n        \n        if config_path and os.path.exists(config_path):\n            with open(config_path, 'r') as f:\n                config = yaml.safe_load(f)\n                # Override defaults with config file\n                # Implementation would merge config with defaults\n        \n        return default_strategies\n    \n    def analyze_query_complexity(self, query: str) -> TaskComplexity:\n        \"\"\"Analyze query to determine complexity level\"\"\"\n        query_lower = query.lower()\n        \n        # Expert level indicators\n        expert_keywords = [\n            'analyze in depth', 'comprehensive analysis', 'research paper',\n            'scientific method', 'statistical analysis', 'peer review',\n            'mathematical proof', 'algorithm optimization', 'architectural design'\n        ]\n        \n        # Complex level indicators\n        complex_keywords = [\n            'explain the relationship', 'compare and contrast', 'analyze',\n            'evaluate', 'synthesize', 'critique', 'reasoning', 'implications',\n            'strategy', 'methodology', 'framework'\n        ]\n        \n        # Moderate level indicators\n        moderate_keywords = [\n            'how to', 'what are the steps', 'explain', 'describe',\n            'give examples', 'list', 'summarize', 'overview'\n        ]\n        \n        # Simple level indicators\n        simple_keywords = [\n            'what is', 'define', 'who is', 'when was', 'where is',\n            'yes or no', 'true or false', 'basic', 'simple'\n        ]\n        \n        # Check for expert indicators\n        for keyword in expert_keywords:\n            if keyword in query_lower:\n                return TaskComplexity.EXPERT\n        \n        # Check for complex indicators\n        for keyword in complex_keywords:\n            if keyword in query_lower:\n                return TaskComplexity.COMPLEX\n        \n        # Check for moderate indicators\n        for keyword in moderate_keywords:\n            if keyword in query_lower:\n                return TaskComplexity.MODERATE\n        \n        # Check for simple indicators\n        for keyword in simple_keywords:\n            if keyword in query_lower:\n                return TaskComplexity.SIMPLE\n        \n        # Default based on length and structure\n        word_count = len(query.split())\n        if word_count > 50:\n            return TaskComplexity.COMPLEX\n        elif word_count > 20:\n            return TaskComplexity.MODERATE\n        elif word_count > 5:\n            return TaskComplexity.MODERATE\n        else:\n            return TaskComplexity.SIMPLE\n    \n    def select_strategy(self, query: str, cost_preference: CostStrategy = CostStrategy.BALANCED,\n                       force_strategy: str = None) -> ModelStrategy:\n        \"\"\"Select optimal strategy based on query and preferences\"\"\"\n        \n        if force_strategy and force_strategy in self.strategies:\n            return self.strategies[force_strategy]\n        \n        complexity = self.analyze_query_complexity(query)\n        \n        # Filter strategies by complexity threshold\n        suitable_strategies = [\n            strategy for strategy in self.strategies.values()\n            if strategy.complexity_threshold.value == complexity.value or\n               self._complexity_order(strategy.complexity_threshold) >= self._complexity_order(complexity)\n        ]\n        \n        if not suitable_strategies:\n            return self.strategies[\"balanced_performer\"]\n        \n        # Apply cost preference\n        if cost_preference == CostStrategy.MINIMIZE_COST:\n            return min(suitable_strategies, key=lambda s: s.cost_per_1k_tokens)\n        elif cost_preference == CostStrategy.MAXIMIZE_QUALITY:\n            return max(suitable_strategies, key=lambda s: s.expected_quality)\n        elif cost_preference == CostStrategy.PERFORMANCE_FIRST:\n            return min(suitable_strategies, key=lambda s: s.response_time_ms)\n        else:  # BALANCED\n            # Calculate composite score: quality/cost ratio with time penalty\n            def composite_score(strategy):\n                quality_cost_ratio = strategy.expected_quality / strategy.cost_per_1k_tokens\n                time_penalty = 1000 / strategy.response_time_ms  # Favor faster responses\n                return quality_cost_ratio * time_penalty\n            \n            return max(suitable_strategies, key=composite_score)\n    \n    def _complexity_order(self, complexity: TaskComplexity) -> int:\n        \"\"\"Get numeric order of complexity\"\"\"\n        order = {\n            TaskComplexity.SIMPLE: 1,\n            TaskComplexity.MODERATE: 2,\n            TaskComplexity.COMPLEX: 3,\n            TaskComplexity.EXPERT: 4\n        }\n        return order[complexity]\n    \n    async def generate_with_strategy(self, query: str, \n                                   cost_preference: CostStrategy = CostStrategy.BALANCED,\n                                   force_strategy: str = None) -> Dict[str, Any]:\n        \"\"\"Generate response using selected strategy\"\"\"\n        strategy = self.select_strategy(query, cost_preference, force_strategy)\n        \n        print(f\"🎯 Selected strategy: {strategy.name}\")\n        print(f\"📊 Model: {strategy.model}, Quality: {strategy.expected_quality:.0%}, Cost: ${strategy.cost_per_1k_tokens:.3f}/1k\")\n        \n        try:\n            start_time = time.time()\n            \n            response = await self.client.chat.completions.create(\n                model=strategy.model,\n                messages=[{'role': 'user', 'content': query}],\n                temperature=strategy.temperature,\n                max_tokens=strategy.max_tokens\n            )\n            \n            response_time = (time.time() - start_time) * 1000  # Convert to ms\n            \n            # Calculate actual cost\n            tokens_used = response.usage.total_tokens\n            actual_cost = (tokens_used / 1000) * strategy.cost_per_1k_tokens\n            \n            # Update usage stats\n            if strategy.name not in self.usage_stats:\n                self.usage_stats[strategy.name] = {\n                    'uses': 0, 'total_cost': 0, 'total_tokens': 0, 'avg_response_time': 0\n                }\n            \n            stats = self.usage_stats[strategy.name]\n            stats['uses'] += 1\n            stats['total_cost'] += actual_cost\n            stats['total_tokens'] += tokens_used\n            stats['avg_response_time'] = (stats['avg_response_time'] * (stats['uses'] - 1) + response_time) / stats['uses']\n            \n            return {\n                'response': response.choices[0].message.content,\n                'strategy_used': strategy.name,\n                'model': strategy.model,\n                'tokens_used': tokens_used,\n                'cost': actual_cost,\n                'response_time_ms': response_time,\n                'expected_quality': strategy.expected_quality,\n                'complexity_detected': self.analyze_query_complexity(query).value\n            }\n            \n        except Exception as e:\n            return {\n                'error': str(e),\n                'strategy_used': strategy.name,\n                'model': strategy.model\n            }\n    \n    def get_strategy_comparison(self, query: str) -> Dict[str, Any]:\n        \"\"\"Compare all strategies for a given query\"\"\"\n        complexity = self.analyze_query_complexity(query)\n        \n        comparison = {\n            'query': query,\n            'detected_complexity': complexity.value,\n            'strategies': {}\n        }\n        \n        for name, strategy in self.strategies.items():\n            estimated_tokens = min(len(query.split()) * 3 + 200, strategy.max_tokens)\n            estimated_cost = (estimated_tokens / 1000) * strategy.cost_per_1k_tokens\n            \n            comparison['strategies'][name] = {\n                'model': strategy.model,\n                'suitability_score': self._calculate_suitability(strategy, complexity),\n                'estimated_cost': estimated_cost,\n                'expected_quality': strategy.expected_quality,\n                'estimated_response_time': strategy.response_time_ms,\n                'description': strategy.description\n            }\n        \n        return comparison\n    \n    def _calculate_suitability(self, strategy: ModelStrategy, complexity: TaskComplexity) -> float:\n        \"\"\"Calculate how suitable a strategy is for given complexity\"\"\"\n        strategy_complexity_order = self._complexity_order(strategy.complexity_threshold)\n        query_complexity_order = self._complexity_order(complexity)\n        \n        if strategy_complexity_order == query_complexity_order:\n            return 1.0\n        elif strategy_complexity_order > query_complexity_order:\n            # Overkill - reduce score but still suitable\n            return 0.8 - (strategy_complexity_order - query_complexity_order) * 0.1\n        else:\n            # Insufficient capability\n            return 0.3 - (query_complexity_order - strategy_complexity_order) * 0.1\n    \n    def get_usage_analytics(self) -> Dict[str, Any]:\n        \"\"\"Get analytics on strategy usage\"\"\"\n        total_uses = sum(stats['uses'] for stats in self.usage_stats.values())\n        total_cost = sum(stats['total_cost'] for stats in self.usage_stats.values())\n        \n        analytics = {\n            'total_requests': total_uses,\n            'total_cost': total_cost,\n            'strategy_breakdown': {}\n        }\n        \n        for strategy_name, stats in self.usage_stats.items():\n            analytics['strategy_breakdown'][strategy_name] = {\n                'usage_percentage': (stats['uses'] / total_uses * 100) if total_uses > 0 else 0,\n                'cost_percentage': (stats['total_cost'] / total_cost * 100) if total_cost > 0 else 0,\n                'average_cost_per_request': stats['total_cost'] / stats['uses'] if stats['uses'] > 0 else 0,\n                'average_response_time_ms': stats['avg_response_time'],\n                'total_requests': stats['uses']\n            }\n        \n        return analytics\n\n# Demo script\nclass StrategyDemo:\n    def __init__(self):\n        self.manager = StrategyManager()\n        \n    async def run_comprehensive_demo(self):\n        \"\"\"Run comprehensive strategy switching demo\"\"\"\n        print(\"🚀 OpenAI Model Strategy Switching Demo\\n\")\n        \n        # Demo queries of different complexities\n        demo_queries = [\n            {\n                'query': 'What is Python?',\n                'description': 'Simple definition query'\n            },\n            {\n                'query': 'How do I create a REST API in Python using FastAPI?',\n                'description': 'Moderate complexity coding question'\n            },\n            {\n                'query': 'Analyze the architectural trade-offs between microservices and monolithic applications, considering scalability, maintainability, and operational complexity.',\n                'description': 'Complex analysis question'\n            },\n            {\n                'query': 'Design a comprehensive machine learning pipeline for real-time fraud detection in financial transactions, including feature engineering, model selection, deployment strategies, and monitoring approaches.',\n                'description': 'Expert-level system design'\n            }\n        ]\n        \n        for i, demo in enumerate(demo_queries, 1):\n            print(f\"\\n{'='*60}\")\n            print(f\"Demo {i}: {demo['description']}\")\n            print(f\"Query: {demo['query']}\")\n            print(f\"{'='*60}\")\n            \n            # Show strategy comparison\n            comparison = self.manager.get_strategy_comparison(demo['query'])\n            print(f\"\\n🔍 Detected Complexity: {comparison['detected_complexity'].upper()}\")\n            \n            print(\"\\n📊 Strategy Comparison:\")\n            for strategy_name, details in comparison['strategies'].items():\n                print(f\"  {strategy_name}:\")\n                print(f\"    Suitability: {details['suitability_score']:.2f}\")\n                print(f\"    Est. Cost: ${details['estimated_cost']:.4f}\")\n                print(f\"    Quality: {details['expected_quality']:.0%}\")\n                print(f\"    Model: {details['model']}\")\n            \n            # Test different cost strategies\n            cost_strategies = [CostStrategy.MINIMIZE_COST, CostStrategy.BALANCED, CostStrategy.MAXIMIZE_QUALITY]\n            \n            for cost_strategy in cost_strategies:\n                print(f\"\\n🎯 Testing {cost_strategy.value} strategy:\")\n                \n                result = await self.manager.generate_with_strategy(\n                    demo['query'], \n                    cost_preference=cost_strategy\n                )\n                \n                if 'error' not in result:\n                    print(f\"  ✅ Strategy: {result['strategy_used']}\")\n                    print(f\"  📱 Model: {result['model']}\")\n                    print(f\"  💰 Cost: ${result['cost']:.4f}\")\n                    print(f\"  ⏱️  Time: {result['response_time_ms']:.0f}ms\")\n                    print(f\"  🎲 Tokens: {result['tokens_used']}\")\n                    print(f\"  📝 Response: {result['response'][:150]}...\")\n                else:\n                    print(f\"  ❌ Error: {result['error']}\")\n            \n            # Small delay between demos\n            await asyncio.sleep(1)\n        \n        # Show final analytics\n        print(f\"\\n{'='*60}\")\n        print(\"📈 Final Usage Analytics\")\n        print(f\"{'='*60}\")\n        \n        analytics = self.manager.get_usage_analytics()\n        print(f\"Total Requests: {analytics['total_requests']}\")\n        print(f\"Total Cost: ${analytics['total_cost']:.4f}\")\n        \n        print(\"\\nStrategy Breakdown:\")\n        for strategy, stats in analytics['strategy_breakdown'].items():\n            print(f\"  {strategy}:\")\n            print(f\"    Usage: {stats['usage_percentage']:.1f}% ({stats['total_requests']} requests)\")\n            print(f\"    Cost Share: {stats['cost_percentage']:.1f}%\")\n            print(f\"    Avg Cost/Request: ${stats['average_cost_per_request']:.4f}\")\n            print(f\"    Avg Response Time: {stats['average_response_time_ms']:.0f}ms\")\n\n# Run the demo\nif __name__ == \"__main__\":\n    import asyncio\n    import os\n    import time\n    \n    async def main():\n        demo = StrategyDemo()\n        await demo.run_comprehensive_demo()\n    \n    asyncio.run(main())\n```\n\nThis comprehensive strategy system provides:\n\n1. **Intelligent Strategy Selection** based on query complexity analysis\n2. **Multiple Cost Strategies** (minimize cost, balanced, maximize quality, performance-first)\n3. **Automatic Complexity Detection** using keyword analysis and query structure\n4. **Usage Analytics** and cost tracking\n5. **Strategy Comparison** to show trade-offs between approaches\n6. **Flexible Configuration** with YAML support\n7. **Real-time Performance Metrics** and cost calculation\n8. **Comprehensive Demo** showing different scenarios\n\nThe system demonstrates how easy it is to switch between OpenAI models based on requirements while optimizing for cost, quality, or performance."}