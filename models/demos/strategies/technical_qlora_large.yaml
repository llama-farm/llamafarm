# Technical Q&A Fine-Tuning Strategy
# Optimized for engineering documentation and technical support
version: "v1"

# Strategy metadata
strategy_info:
  name: "technical_qlora_large"
  description: "Technical Q&A fine-tuning with QLoRA for complex engineering topics"
  use_case: "Technical documentation, engineering support, developer assistance"
  difficulty: "advanced"
  
# Environment-specific configurations
environments:
  
  # Apple Silicon (M1/M2/M3) - Demo environment
  apple_silicon:
    active: true  # This environment will be used in the demo
    model:
      base_model: "google/t5-base"
      model_type: "Seq2SeqLM"
      device: "mps"
      load_in_8bit: false
      load_in_4bit: true  # QLoRA requires quantization
      torch_dtype: "float16"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "float16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: true
    
    training:
      method: "qlora"
      batch_size: 2
      gradient_accumulation_steps: 8
      max_steps: 250
      learning_rate: 1e-4
      warmup_steps: 25
      logging_steps: 10
      eval_steps: 50
      save_steps: 50
      weight_decay: 0.01
      
    lora_config:
      r: 32  # Higher rank for technical precision
      alpha: 64
      dropout: 0.05  # Lower dropout for technical accuracy
      target_modules: ["q", "v", "k", "o", "wi_0", "wi_1", "wo"]
      bias: "none"
      task_type: "SEQ_2_SEQ_LM"
      
    generation:
      max_length: 512
      num_beams: 4  # Beam search for technical accuracy
      temperature: 0.3  # Lower temperature for precision
      top_p: 0.8
      do_sample: false  # Deterministic for technical content
      
  # Linux Server - Enterprise deployment
  linux_server:
    active: false
    model:
      base_model: "google/t5-base"
      model_type: "Seq2SeqLM"
      device: "cuda"
      load_in_8bit: false
      load_in_4bit: true
      torch_dtype: "float16"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "float16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: true
    
    training:
      method: "qlora"
      batch_size: 8  # Larger batch for server
      gradient_accumulation_steps: 4
      max_steps: 400
      learning_rate: 8e-5
      warmup_steps: 40
      logging_steps: 20
      eval_steps: 100
      save_steps: 100
      weight_decay: 0.01
      dataloader_num_workers: 8
      
    lora_config:
      r: 64  # Maximum adaptation for enterprise
      alpha: 128
      dropout: 0.05
      target_modules: ["q", "v", "k", "o", "wi_0", "wi_1", "wo"]
      bias: "none"
      task_type: "SEQ_2_SEQ_LM"
      
    generation:
      max_length: 768
      num_beams: 6
      temperature: 0.2
      top_p: 0.85
      do_sample: false
      
  # Windows PC - Developer workstation
  windows_pc:
    active: false
    model:
      base_model: "google/t5-small"  # Smaller for PC limits
      model_type: "Seq2SeqLM"
      device: "cuda"
      load_in_8bit: false
      load_in_4bit: true
      torch_dtype: "float16"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "float16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: false  # Reduce memory usage
    
    training:
      method: "qlora"
      batch_size: 1
      gradient_accumulation_steps: 16
      max_steps: 200
      learning_rate: 2e-4
      warmup_steps: 20
      logging_steps: 10
      eval_steps: 40
      save_steps: 40
      
    lora_config:
      r: 16
      alpha: 32
      dropout: 0.1
      target_modules: ["q", "v"]  # Minimal for memory
      bias: "none"
      task_type: "SEQ_2_SEQ_LM"
      
    generation:
      max_length: 384
      num_beams: 3
      temperature: 0.3
      top_p: 0.8
      do_sample: false
      
  # CPU-only - No GPU required
  cpu_only:
    active: false
    model:
      base_model: "google/t5-small"
      model_type: "Seq2SeqLM"
      device: "cpu"
      load_in_8bit: false
      load_in_4bit: false  # No quantization on CPU
      torch_dtype: "float32"
    
    training:
      method: "lora"  # Regular LoRA instead of QLoRA
      batch_size: 1
      gradient_accumulation_steps: 32
      max_steps: 100
      learning_rate: 5e-4
      warmup_steps: 10
      logging_steps: 5
      eval_steps: 25
      save_steps: 25
      
    lora_config:
      r: 8
      alpha: 16
      dropout: 0.1
      target_modules: ["q", "v"]
      bias: "none"
      task_type: "SEQ_2_SEQ_LM"
      
    generation:
      max_length: 256
      num_beams: 2
      temperature: 0.4
      top_p: 0.8
      do_sample: false

# Dataset configuration
dataset:
  path: "datasets/engineering_qa.jsonl"
  format: "jsonl"
  input_column: "instruction"
  target_column: "output"
  prompt_template: |
    Question: {instruction}
    Answer: {output}
  max_source_length: 256
  max_target_length: 512
  validation_split: 0.15
  
# Technical evaluation metrics
evaluation:
  metrics:
    - "rouge"
    - "bleu"
    - "exact_match"
    - "technical_accuracy"
  eval_dataset: "validation"
  custom_metrics:
    technical_accuracy:
      keywords: ["protocol", "architecture", "database", "algorithm"]
      weight: 0.3

# Output configuration
output:
  model_name: "technical-qa-qlora"
  save_directory: "./models/technical-qa"
  push_to_hub: false
  export_formats: ["pytorch", "onnx"]  # Multiple formats for deployment
  
# Technical content safety
safety:
  code_execution_filter: true
  sensitive_info_detection: true
  technical_accuracy_threshold: 0.85
  
# Hardware requirements by environment
hardware_requirements:
  apple_silicon:
    min_memory: "12GB"
    recommended_memory: "24GB"
    disk_space: "6GB"
    estimated_time: "20-25 minutes"
    notes: "QLoRA significantly reduces memory usage"
    
  linux_server:
    min_memory: "24GB"
    recommended_memory: "64GB"
    gpu_memory: "12GB"
    disk_space: "10GB"
    estimated_time: "12-15 minutes"
    notes: "Optimized for enterprise deployment"
    
  windows_pc:
    min_memory: "8GB"
    recommended_memory: "16GB"
    gpu_memory: "6GB"
    disk_space: "4GB"
    estimated_time: "35-45 minutes"
    notes: "Uses smaller T5-small model"
    
  cpu_only:
    min_memory: "6GB"
    recommended_memory: "12GB"
    disk_space: "3GB"
    estimated_time: "60-90 minutes"
    notes: "No quantization, slower but no GPU required"

# Environment auto-detection
auto_environment:
  enabled: true
  detection_order:
    - "apple_silicon"
    - "linux_server"
    - "windows_pc"
    - "cpu_only"
    
  selection_criteria:
    apple_silicon:
      - "platform.system() == 'Darwin'"
      - "torch.backends.mps.is_available()"
      - "psutil.virtual_memory().total > 12 * 1024**3"
    linux_server:
      - "platform.system() == 'Linux'"
      - "torch.cuda.is_available()"
      - "torch.cuda.get_device_properties(0).total_memory > 10 * 1024**3"
    windows_pc:
      - "platform.system() == 'Windows'"
      - "torch.cuda.is_available()"
    cpu_only:
      - "True"

# Deployment configurations
# LlamaFactory Configuration (integrated for unified training)
llamafactory:
  # model
  model_name: google/t5-base
  template: default
  
  # method
  stage: sft
  do_train: true
  finetuning_type: lora
  lora_target: all
  lora_rank: 16  # Reduced for demo
  lora_alpha: 32
  
  # dataset
  dataset_dir: ../datasets/technical_qa
  dataset: engineering_qa
  template: default
  cutoff_len: 512
  max_samples: 50  # Small for demo
  overwrite_cache: true
  preprocessing_num_workers: 4
  
  # output
  output_dir: ./llamafactory_output
  logging_steps: 10
  save_steps: 50
  plot_loss: true
  overwrite_output_dir: true
  
  # train
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8
  learning_rate: 1.0e-4
  num_train_epochs: 3.0
  lr_scheduler_type: cosine
  warmup_steps: 10
  bf16: true
  ddp_timeout: 180000000
  
  # eval
  val_size: 0.1
  per_device_eval_batch_size: 1
  eval_strategy: steps
  eval_steps: 50

deployment:
  apple_silicon:
    frameworks: ["ollama", "mlx", "pytorch"]
    optimization: "coreml"
    serving: "local"
    
  linux_server:
    frameworks: ["vllm", "tgi", "pytorch"]
    optimization: "tensorrt"
    serving: "production"
    
  windows_pc:
    frameworks: ["onnx", "pytorch", "directml"]
    optimization: "onnx"
    serving: "desktop"
    
  cpu_only:
    frameworks: ["onnx", "pytorch"]
    optimization: "cpu"
    serving: "edge"