# Medical Assistant Fine-Tuning Strategy
# Optimized for safe medical Q&A with mandatory disclaimers
version: "v1"

# Strategy metadata
strategy_info:
  name: "medical_qlora_efficient"
  description: "Medical Q&A fine-tuning with QLoRA for safe healthcare information"
  use_case: "Medical information assistant, healthcare chatbots, educational tools"
  difficulty: "advanced"
  
# Environment-specific configurations
environments:
  
  # Apple Silicon (M1/M2/M3) - Demo environment
  apple_silicon:
    active: true  # This environment will be used in the demo
    model:
      base_model: "gpt2"  # Use simple model for medical demo
      model_type: "CausalLM"
      device: "mps"
      load_in_8bit: false
      load_in_4bit: false
      torch_dtype: "float16"
      
    training:
      method: "lora"
      batch_size: 1
      gradient_accumulation_steps: 8
      max_steps: 50
      learning_rate: 3e-4
      warmup_steps: 10
      logging_steps: 5
      eval_steps: 25
      save_steps: 25
      weight_decay: 0.01
      
    lora_config:
      r: 8  # Low rank for safety and efficiency
      alpha: 16
      dropout: 0.1
      target_modules: ["c_attn", "c_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.3  # Lower temperature for medical accuracy
      top_p: 0.8
      max_length: 512
      do_sample: true
      
  # Linux Server - Production medical deployment
  linux_server:
    active: false
    model:
      base_model: "microsoft/DialoGPT-medium"  # Better for medical conversations
      model_type: "CausalLM"
      device: "cuda"
      load_in_8bit: true
      load_in_4bit: false
      torch_dtype: "float16"
      
    training:
      method: "qlora"
      batch_size: 4
      gradient_accumulation_steps: 4
      max_steps: 300
      learning_rate: 1e-4
      warmup_steps: 30
      logging_steps: 20
      eval_steps: 100
      save_steps: 100
      weight_decay: 0.01
      
    lora_config:
      r: 16  # Higher rank for medical precision
      alpha: 32
      dropout: 0.05
      target_modules: ["c_attn", "c_proj", "c_fc"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.2  # Very conservative for medical accuracy
      top_p: 0.85
      max_length: 768
      do_sample: false  # Deterministic for medical content
      
  # Windows PC - Consumer medical assistant
  windows_pc:
    active: false
    model:
      base_model: "gpt2"
      model_type: "CausalLM"
      device: "cuda"
      load_in_8bit: false
      load_in_4bit: true  # 4-bit for consumer hardware
      torch_dtype: "float16"
      
    training:
      method: "qlora"
      batch_size: 1
      gradient_accumulation_steps: 16
      max_steps: 150
      learning_rate: 2e-4
      warmup_steps: 15
      logging_steps: 5
      eval_steps: 30
      save_steps: 30
      
    lora_config:
      r: 8
      alpha: 16
      dropout: 0.1
      target_modules: ["c_attn"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.3
      top_p: 0.8
      max_length: 384
      do_sample: true
      
  # CPU-only - No GPU medical assistant
  cpu_only:
    active: false
    model:
      base_model: "gpt2"
      model_type: "CausalLM"
      device: "cpu"
      load_in_8bit: false
      load_in_4bit: false
      torch_dtype: "float32"
      
    training:
      method: "lora"
      batch_size: 1
      gradient_accumulation_steps: 32
      max_steps: 100
      learning_rate: 5e-4
      warmup_steps: 10
      logging_steps: 5
      eval_steps: 25
      save_steps: 25
      
    lora_config:
      r: 4
      alpha: 8
      dropout: 0.1
      target_modules: ["c_attn"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.4
      top_p: 0.8
      max_length: 256
      do_sample: true

# Dataset configuration
dataset:
  path: "datasets/medical_qa.jsonl"
  format: "jsonl"
  input_column: "instruction"
  target_column: "output"
  prompt_template: |
    ### Medical Question:
    {instruction}
    
    ### Safe Response:
    {output}
  max_source_length: 256
  max_target_length: 512
  validation_split: 0.15
  
# Medical evaluation metrics
evaluation:
  metrics:
    - "rouge"
    - "bleu"
    - "medical_safety_score"
    - "disclaimer_compliance"
  eval_dataset: "validation"
  custom_metrics:
    medical_safety_score:
      check_disclaimers: true
      check_no_diagnosis: true
      emergency_detection: true
      weight: 0.4
    disclaimer_compliance:
      required_phrases: ["consult", "healthcare provider", "medical professional"]
      weight: 0.3

# Output configuration
output:
  model_name: "medical-assistant-safe"
  save_directory: "./models/medical-assistant"
  push_to_hub: false
  export_formats: ["pytorch", "onnx"]
  
# Medical content safety (CRITICAL)
safety:
  medical_disclaimer_required: true
  no_diagnosis_allowed: true
  emergency_symptoms_redirect: true
  professional_consultation_emphasis: true
  content_filter: true
  max_toxicity_score: 0.1  # Very strict for medical content
  
# Hardware requirements by environment
hardware_requirements:
  apple_silicon:
    min_memory: "8GB"
    recommended_memory: "16GB"
    disk_space: "3GB"
    estimated_time: "10-15 minutes"
    notes: "Medical AI requires careful monitoring"
    
  linux_server:
    min_memory: "16GB"
    recommended_memory: "32GB"
    gpu_memory: "8GB"
    disk_space: "6GB"
    estimated_time: "8-12 minutes"
    notes: "Production medical deployment ready"
    
  windows_pc:
    min_memory: "8GB"
    recommended_memory: "16GB"
    gpu_memory: "4GB"
    disk_space: "4GB"
    estimated_time: "20-30 minutes"
    notes: "Suitable for personal medical assistant"
    
  cpu_only:
    min_memory: "4GB"
    recommended_memory: "8GB"
    disk_space: "2GB"
    estimated_time: "45-60 minutes"
    notes: "Slower but no GPU required for medical AI"

# Environment auto-detection
auto_environment:
  enabled: true
  detection_order:
    - "apple_silicon"
    - "linux_server"
    - "windows_pc"
    - "cpu_only"
    
  selection_criteria:
    apple_silicon:
      - "platform.system() == 'Darwin'"
      - "torch.backends.mps.is_available()"
    linux_server:
      - "platform.system() == 'Linux'"
      - "torch.cuda.is_available()"
      - "torch.cuda.get_device_properties(0).total_memory > 8 * 1024**3"
    windows_pc:
      - "platform.system() == 'Windows'"
      - "torch.cuda.is_available()"
    cpu_only:
      - "True"

# Medical deployment considerations
deployment:
  apple_silicon:
    frameworks: ["ollama", "mlx", "pytorch"]
    optimization: "coreml"
    serving: "local"
    medical_compliance: "personal_use_only"
    
  linux_server:
    frameworks: ["vllm", "tgi", "pytorch"]
    optimization: "tensorrt"
    serving: "production"
    medical_compliance: "healthcare_ready"
    
  windows_pc:
    frameworks: ["onnx", "pytorch", "directml"]
    optimization: "onnx"
    serving: "desktop"
    medical_compliance: "educational_use"
    
  cpu_only:
    frameworks: ["onnx", "pytorch"]
    optimization: "cpu"
    serving: "edge"
    medical_compliance: "basic_information"

# Legal and compliance notes
compliance:
  regulatory_notes: |
    IMPORTANT: Medical AI systems are subject to healthcare regulations:
    - FDA guidance for AI/ML-based medical devices (US)
    - MDR regulations (EU)  
    - Health Canada guidance (Canada)
    - GDPR for health data processing
    
  liability_disclaimer: |
    This AI system is for educational/informational purposes only.
    It does not provide medical advice, diagnosis, or treatment.
    Always consult qualified healthcare professionals for medical decisions.
    
  audit_requirements:
    - "Regular medical accuracy validation"
    - "Safety compliance monitoring"  
    - "Bias detection and mitigation"
    - "Patient data privacy protection"