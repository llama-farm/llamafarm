#!/usr/bin/env python3
"""
Demo 2: Multi-Model Strategy
Shows different models optimized for different tasks.

This demo uses ONLY CLI commands - no hardcoding!
"""

import os
import sys
import subprocess
import time
from pathlib import Path

# Add colorama for colored output
try:
    from colorama import init, Fore, Style, Back
    init(autoreset=True)
except ImportError:
    # Fallback if colorama not available
    class Fore:
        BLACK = RED = GREEN = YELLOW = BLUE = MAGENTA = CYAN = WHITE = RESET = ''
    class Style:
        RESET_ALL = BRIGHT = DIM = NORMAL = ''

def run_cli_command(cmd: str, description: str = None, check_error: bool = True, show_output: bool = True):
    """Run a CLI command and show output."""
    if description:
        print(f"\n{Fore.CYAN}üìã {description}{Style.RESET_ALL}")
    
    print(f"{Fore.YELLOW}$ {cmd}{Style.RESET_ALL}")
    
    result = subprocess.run(
        cmd,
        shell=True,
        capture_output=True,
        text=True
    )
    
    if show_output:
        if result.stdout:
            print(result.stdout)
        if result.stderr and check_error:
            print(f"{Fore.RED}{result.stderr}{Style.RESET_ALL}")
    
    return result.returncode == 0, result.stdout, result.stderr

def press_enter_to_continue():
    """Wait for user input unless in automated mode."""
    if not os.getenv("DEMO_MODE") == "automated":
        input(f"\n{Fore.GREEN}Press Enter to continue...{Style.RESET_ALL}")

def print_success(msg: str):
    """Print success message."""
    print(f"{Fore.GREEN}‚úÖ {msg}{Style.RESET_ALL}")

def print_error(msg: str):
    """Print error message."""
    print(f"{Fore.RED}‚ùå {msg}{Style.RESET_ALL}")

def print_info(msg: str):
    """Print info message."""
    print(f"{Fore.CYAN}‚ÑπÔ∏è  {msg}{Style.RESET_ALL}")

def main():
    """Run the multi-model demonstration."""
    
    print("\n" + "="*70)
    print(f"{Fore.CYAN}üéØ MULTI-MODEL TASK OPTIMIZATION DEMO{Style.RESET_ALL}")
    print("="*70)
    
    # Auto-setup requirements
    print(f"\n{Fore.YELLOW}üì¶ Checking and installing requirements...{Style.RESET_ALL}")
    success, stdout, _ = run_cli_command(
        'uv run python cli.py setup demos/strategies.yaml --auto --verbose',
        "Setting up components for multi-model strategy",
        show_output=True
    )
    if not success:
        print_error("Setup failed. Please check your environment.")
        return
    print_success("All requirements are ready!")
    
    print("\nThis demo shows how different models can be optimized")
    print("for different types of tasks to maximize performance and minimize costs.")
    print("\nüìö What you'll see:")
    print("  1. Simple queries ‚Üí Fast, cheap models")
    print("  2. Complex reasoning ‚Üí Advanced models")
    print("  3. Creative tasks ‚Üí Specialized models")  
    print("  4. Code generation ‚Üí Local models")
    print("  5. Cost comparison analysis")
    
    press_enter_to_continue()
    
    # Show task routing strategy
    print(f"\n{Fore.BLUE}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.BLUE}TASK ROUTING STRATEGY{Style.RESET_ALL}")
    print(f"{Fore.BLUE}{'='*60}{Style.RESET_ALL}")
    print("\nüìä Model Selection by Task Type:")
    print("  ‚Ä¢ Simple queries    ‚Üí GPT-3.5 Turbo (fast, $0.002/1k tokens)")
    print("  ‚Ä¢ Complex reasoning ‚Üí GPT-4o Mini   (smart, $0.015/1k tokens)")
    print("  ‚Ä¢ Creative writing  ‚Üí GPT-4o        (creative, $0.03/1k tokens)")
    print("  ‚Ä¢ Code generation   ‚Üí Mistral 7B    (local, free)")
    print("\nüí° This routing can save 60-80% on API costs!")
    
    press_enter_to_continue()
    
    # Demo 1: Simple task
    print(f"\n{Fore.GREEN}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.GREEN}TASK 1: SIMPLE QUERY{Style.RESET_ALL}")
    print(f"{Fore.GREEN}{'='*60}{Style.RESET_ALL}")
    print("\nFor simple factual questions, we use fast, cheap models:")
    
    run_cli_command(
        'uv run python cli.py query "What is 2+2?" --provider openai_gpt35_turbo',
        "Simple math question ‚Üí GPT-3.5 Turbo"
    )
    
    press_enter_to_continue()
    
    # Demo 2: Complex task  
    print(f"\n{Fore.YELLOW}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}TASK 2: COMPLEX REASONING{Style.RESET_ALL}")
    print(f"{Fore.YELLOW}{'='*60}{Style.RESET_ALL}")
    print("\nFor complex reasoning, we use more advanced models:")
    
    run_cli_command(
        'uv run python cli.py query "Explain the theory of relativity and its key implications for modern physics" --provider openai_gpt4o_mini',
        "Complex physics question ‚Üí GPT-4o Mini"
    )
    
    press_enter_to_continue()
    
    # Demo 3: Creative task
    print(f"\n{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}TASK 3: CREATIVE WRITING{Style.RESET_ALL}")
    print(f"{Fore.MAGENTA}{'='*60}{Style.RESET_ALL}")
    print("\nFor creative tasks, we use the most creative models:")
    
    run_cli_command(
        'uv run python cli.py query "Write a humorous haiku about debugging code that won\'t work" --provider openai_gpt4o',
        "Creative writing ‚Üí GPT-4o",
        check_error=False
    )
    
    press_enter_to_continue()
    
    # Demo 4: Code task
    print(f"\n{Fore.BLUE}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.BLUE}TASK 4: CODE GENERATION{Style.RESET_ALL}")
    print(f"{Fore.BLUE}{'='*60}{Style.RESET_ALL}")
    print("\nFor code generation, we can use local models (free!):")
    
    # Check if Ollama is available for code generation
    check_ollama, _, _ = run_cli_command(
        "which ollama",
        "Checking if Ollama is available",
        check_error=False,
        show_output=False
    )
    
    if check_ollama:
        run_cli_command(
            'uv run python cli.py test-local mistral:7b --query "Write a Python function to calculate fibonacci numbers"',
            "Code generation ‚Üí Local Mistral (free!)",
            check_error=False
        )
    else:
        print_info("Ollama not available. Would use local Mistral model for free code generation.")
        print_info("Install Ollama and run: ollama pull mistral:7b")
        
        # Fallback to cloud for demonstration
        run_cli_command(
            'uv run python cli.py query "Write a Python function to calculate fibonacci numbers" --provider openai_gpt35_turbo',
            "Code generation ‚Üí GPT-3.5 (fallback)",
            check_error=False
        )
    
    # Final analysis
    print(f"\n{Fore.GREEN}{'='*60}{Style.RESET_ALL}")
    print(f"{Fore.GREEN}COST & PERFORMANCE ANALYSIS{Style.RESET_ALL}")
    print(f"{Fore.GREEN}{'='*60}{Style.RESET_ALL}")
    
    print("\nüìä Without intelligent routing:")
    print("   All tasks ‚Üí GPT-4o = ~$0.15 per 1k tokens")
    print("   4 tasks √ó 100 tokens = ~$0.06 total")
    
    print("\nüéØ With intelligent routing:")
    print("   Simple   ‚Üí GPT-3.5    = $0.002 per 1k tokens")
    print("   Complex  ‚Üí GPT-4o Mini = $0.015 per 1k tokens") 
    print("   Creative ‚Üí GPT-4o     = $0.030 per 1k tokens")
    print("   Code     ‚Üí Local      = $0.000 per 1k tokens")
    print("   Total cost ‚âà $0.02 (67% savings!)")
    
    print("\nüí° Key benefits:")
    print("   ‚Ä¢ Massive cost savings (60-80%)")
    print("   ‚Ä¢ Better performance per task type")
    print("   ‚Ä¢ Privacy for code generation (local)")
    print("   ‚Ä¢ Reduced latency for simple tasks")
    
    print("\n" + "=" * 60)
    print("‚úÖ Demo completed successfully!")
    print("\nüí° Key Takeaways:")
    print("   ‚Ä¢ Task routing reduces costs by 60-80%")
    print("   ‚Ä¢ Simple queries don't need expensive models")
    print("   ‚Ä¢ Specialized models excel at specific tasks")
    print("   ‚Ä¢ Local models provide free computation for suitable tasks")
    
    print("\nüìä Cost Analysis:")
    print("   Without routing: All tasks ‚Üí GPT-4 = $$$")
    print("   With routing:    Mixed models    = $")
    
    print("\nüìù To customize this demo:")
    print("   1. Edit demos/strategies.yaml")
    print("   2. Modify the 'demo2_multi_model' strategy")
    print("   3. Add your own routing rules")
    print("   4. Configure task categories and model assignments")
    
    print("\nüîç Try it yourself with specific task types:")
    print("   # Simple task")
    print("   uv run python cli.py query 'What is 2+2?' --provider cloud_api")
    print("   ")
    print("   # Complex task")
    print("   uv run python cli.py query 'Explain quantum entanglement' --provider cloud_api")
    print("   ")
    print("   # Code task")
    print("   uv run python cli.py query 'Write a Python sorting function' --provider model_app")

if __name__ == "__main__":
    main()