# Creative Writing Fine-Tuning Strategy
# M1-optimized with Ollama → LlamaFactory → PyTorch fallback strategy
version: "v1"

# Strategy metadata
strategy_info:
  name: "creative_lora_diverse"
  description: "M1-optimized creative writing fine-tuning with intelligent fallback"
  use_case: "Content creation, story writing, creative assistance"
  difficulty: "intermediate"
  hardware_optimization: "apple_silicon"
  fallback_strategy: "ollama → llamafactory → pytorch → simulation"
  
# Environment-specific configurations
# NOTE: Demo will use 'apple_silicon' by default, but all environments are supported
environments:
  
  # Apple Silicon (M1/M2/M3) - Optimized for local development
  apple_silicon:
    active: true  # This environment will be used in the demo
    model:
      base_model: "llama3.2:1b"  # Small Llama model via Ollama
      model_type: "Ollama"
      device: "mps"  # Metal Performance Shaders
      ollama_model: "llama3.2:1b"
      download_if_missing: true
      torch_dtype: "float16"
    
    training:
      method: "lora"
      batch_size: 2
      gradient_accumulation_steps: 4
      max_steps: 200
      learning_rate: 5e-4
      warmup_steps: 20
      logging_steps: 10
      eval_steps: 50
      save_steps: 50
      
    lora_config:
      r: 8  # Low rank for creativity preservation
      alpha: 16
      dropout: 0.1
      target_modules: ["c_attn", "c_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.8
      top_p: 0.9
      max_length: 512
      do_sample: true
      
  # Intel/AMD Linux Server - High throughput production
  linux_server:
    active: false
    model:
      base_model: "gpt2-medium"
      model_type: "CausalLM"
      device: "cuda"  # NVIDIA GPU
      load_in_8bit: true  # Memory optimization
      load_in_4bit: false
      torch_dtype: "float16"
    
    training:
      method: "lora"
      batch_size: 8  # Larger batch for server hardware
      gradient_accumulation_steps: 2
      max_steps: 300
      learning_rate: 3e-4
      warmup_steps: 30
      logging_steps: 20
      eval_steps: 100
      save_steps: 100
      dataloader_num_workers: 4
      
    lora_config:
      r: 16  # Higher rank for more adaptation
      alpha: 32
      dropout: 0.1
      target_modules: ["c_attn", "c_proj", "c_fc"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.85
      top_p: 0.92
      max_length: 768
      do_sample: true
      
  # Windows PC - Consumer hardware optimization
  windows_pc:
    active: false
    model:
      base_model: "gpt2-medium"
      model_type: "CausalLM"
      device: "cuda"  # Or "cpu" for non-GPU systems
      load_in_8bit: false
      load_in_4bit: true  # 4-bit for consumer GPU memory limits
      torch_dtype: "float16"
    
    training:
      method: "lora"
      batch_size: 1  # Conservative for consumer hardware
      gradient_accumulation_steps: 8
      max_steps: 150
      learning_rate: 4e-4
      warmup_steps: 15
      logging_steps: 5
      eval_steps: 30
      save_steps: 30
      
    lora_config:
      r: 8
      alpha: 16
      dropout: 0.15
      target_modules: ["c_attn"]  # Minimal targets for memory
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.8
      top_p: 0.9
      max_length: 384
      do_sample: true
      
  # CPU-only fallback - No GPU required
  cpu_only:
    active: false
    model:
      base_model: "gpt2"  # Smaller model for CPU
      model_type: "CausalLM"
      device: "cpu"
      load_in_8bit: false
      load_in_4bit: false
      torch_dtype: "float32"  # CPU prefers float32
    
    training:
      method: "lora"
      batch_size: 1
      gradient_accumulation_steps: 16  # Compensate for small batch
      max_steps: 100
      learning_rate: 1e-3  # Higher LR for CPU training
      warmup_steps: 10
      logging_steps: 5
      eval_steps: 25
      save_steps: 25
      
    lora_config:
      r: 4  # Very low rank for CPU efficiency
      alpha: 8
      dropout: 0.1
      target_modules: ["c_attn"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.8
      top_p: 0.9
      max_length: 256
      do_sample: true

# Dataset configuration (same across environments)
dataset:
  path: "datasets/creative_stories.jsonl"
  format: "jsonl"
  text_column: "output"
  prompt_template: |
    ### Instruction:
    {instruction}
    
    ### Response:
    {output}
  max_length: 512
  validation_split: 0.1

# Evaluation metrics
evaluation:
  metrics:
    - "perplexity"
    - "bleu"
    - "rouge"
  eval_dataset: "validation"
  
# Output configuration  
output:
  model_name: "creative-writing-lora"
  save_directory: "./models/creative-writing"
  push_to_hub: false
  
# Safety and content filtering
safety:
  content_filter: true
  max_toxicity_score: 0.3
  inappropriate_content_detection: true
  
# Hardware requirements by environment
hardware_requirements:
  apple_silicon:
    min_memory: "8GB"
    recommended_memory: "16GB"
    disk_space: "4GB"
    estimated_time: "15-20 minutes"
    
  linux_server:
    min_memory: "16GB"
    recommended_memory: "32GB" 
    gpu_memory: "8GB"
    disk_space: "6GB"
    estimated_time: "8-12 minutes"
    
  windows_pc:
    min_memory: "8GB"
    recommended_memory: "16GB"
    gpu_memory: "4GB"
    disk_space: "4GB"
    estimated_time: "20-30 minutes"
    
  cpu_only:
    min_memory: "4GB"
    recommended_memory: "8GB"
    disk_space: "2GB"
    estimated_time: "45-60 minutes"

# Environment detection and auto-selection
auto_environment:
  enabled: true
  detection_order:
    - "apple_silicon"  # Check for MPS first
    - "linux_server"   # Check for CUDA + high memory
    - "windows_pc"     # Check for CUDA + moderate memory  
    - "cpu_only"       # Fallback
    
  selection_criteria:
    apple_silicon:
      - "platform.system() == 'Darwin'"
      - "torch.backends.mps.is_available()"
    linux_server:
      - "platform.system() == 'Linux'"
      - "torch.cuda.is_available()"
      - "torch.cuda.get_device_properties(0).total_memory > 8 * 1024**3"
    windows_pc:
      - "platform.system() == 'Windows'"
      - "torch.cuda.is_available()"
    cpu_only:
      - "True"  # Always available as fallback