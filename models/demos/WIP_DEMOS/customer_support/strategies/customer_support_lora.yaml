# Customer Support Fine-Tuning Strategy
# Optimized for professional e-commerce customer service
version: "v1"

# Strategy metadata
strategy_info:
  name: "customer_support_lora"
  description: "Customer support fine-tuning with LoRA for professional service conversations"
  use_case: "E-commerce support, customer service automation, professional assistance"
  difficulty: "intermediate"
  
# Environment-specific configurations
environments:
  
  # Apple Silicon (M1/M2/M3) - Demo environment
  apple_silicon:
    active: true  # This environment will be used in the demo
    model:
      base_model: "microsoft/DialoGPT-small"  # Use smaller DialoGPT for demo
      model_type: "CausalLM"
      device: "mps"
      load_in_8bit: false
      load_in_4bit: false
      torch_dtype: "float16"
      
    training:
      method: "lora"
      batch_size: 2
      gradient_accumulation_steps: 4
      max_steps: 100
      learning_rate: 3e-4
      warmup_steps: 10
      logging_steps: 5
      eval_steps: 25
      save_steps: 25
      weight_decay: 0.01
      
    lora_config:
      r: 16  # Good rank for conversational adaptation
      alpha: 32
      dropout: 0.1
      target_modules: ["c_attn", "c_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.7  # Balanced for natural conversation
      top_p: 0.9
      max_length: 512
      do_sample: true
      
  # Linux Server - Production customer support
  linux_server:
    active: false
    model:
      base_model: "microsoft/DialoGPT-medium"  # Larger model for production
      model_type: "CausalLM"
      device: "cuda"
      load_in_8bit: true
      load_in_4bit: false
      torch_dtype: "float16"
      
    training:
      method: "lora"
      batch_size: 8
      gradient_accumulation_steps: 2
      max_steps: 300
      learning_rate: 2e-4
      warmup_steps: 30
      logging_steps: 20
      eval_steps: 100
      save_steps: 100
      weight_decay: 0.01
      dataloader_num_workers: 4
      
    lora_config:
      r: 32  # Higher rank for better adaptation
      alpha: 64
      dropout: 0.1
      target_modules: ["c_attn", "c_proj", "c_fc"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.6  # Slightly more conservative for production
      top_p: 0.9
      max_length: 768
      do_sample: true
      
  # Windows PC - Desktop customer support
  windows_pc:
    active: false
    model:
      base_model: "microsoft/DialoGPT-small"
      model_type: "CausalLM"
      device: "cuda"
      load_in_8bit: false
      load_in_4bit: true  # 4-bit for consumer hardware
      torch_dtype: "float16"
      
    training:
      method: "lora"
      batch_size: 1
      gradient_accumulation_steps: 8
      max_steps: 150
      learning_rate: 4e-4
      warmup_steps: 15
      logging_steps: 5
      eval_steps: 30
      save_steps: 30
      
    lora_config:
      r: 16
      alpha: 32
      dropout: 0.1
      target_modules: ["c_attn", "c_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.7
      top_p: 0.9
      max_length: 384
      do_sample: true
      
  # CPU-only - No GPU customer support
  cpu_only:
    active: false
    model:
      base_model: "gpt2"  # Simple model for CPU
      model_type: "CausalLM"
      device: "cpu"
      load_in_8bit: false
      load_in_4bit: false
      torch_dtype: "float32"
      
    training:
      method: "lora"
      batch_size: 1
      gradient_accumulation_steps: 16
      max_steps: 100
      learning_rate: 5e-4
      warmup_steps: 10
      logging_steps: 5
      eval_steps: 25
      save_steps: 25
      
    lora_config:
      r: 8  # Low rank for CPU efficiency
      alpha: 16
      dropout: 0.1
      target_modules: ["c_attn"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.8
      top_p: 0.9
      max_length: 256
      do_sample: true

# Dataset configuration
dataset:
  path: "datasets/ecommerce_support.jsonl"
  format: "jsonl"
  input_column: "instruction"
  target_column: "output"
  prompt_template: |
    Customer: {instruction}
    Support: {output}
  max_source_length: 256
  max_target_length: 512
  validation_split: 0.1
  
# Evaluation metrics for customer support
evaluation:
  metrics:
    - "rouge"
    - "bleu"
    - "support_quality_score"
    - "empathy_score"
  eval_dataset: "validation"
  custom_metrics:
    support_quality_score:
      professionalism: true
      policy_accuracy: true
      resolution_clarity: true
      weight: 0.4
    empathy_score:
      sentiment_awareness: true
      emotional_intelligence: true
      weight: 0.3

# Output configuration
output:
  model_name: "customer-support-assistant"
  save_directory: "./models/customer-support"
  push_to_hub: false
  export_formats: ["pytorch", "onnx"]
  
# Support-specific safety and quality
safety:
  professional_tone_required: true
  policy_compliance_check: true
  escalation_detection: true
  inappropriate_content_filter: true
  max_toxicity_score: 0.2
  
# Hardware requirements by environment
hardware_requirements:
  apple_silicon:
    min_memory: "8GB"
    recommended_memory: "16GB"
    disk_space: "4GB"
    estimated_time: "15-20 minutes"
    notes: "DialoGPT optimized for conversational AI"
    
  linux_server:
    min_memory: "16GB"
    recommended_memory: "32GB"
    gpu_memory: "8GB"
    disk_space: "6GB"
    estimated_time: "10-15 minutes"
    notes: "Production-ready customer support deployment"
    
  windows_pc:
    min_memory: "8GB"
    recommended_memory: "16GB"
    gpu_memory: "4GB"
    disk_space: "4GB"
    estimated_time: "25-35 minutes"
    notes: "Suitable for small business customer support"
    
  cpu_only:
    min_memory: "4GB"
    recommended_memory: "8GB"
    disk_space: "2GB"
    estimated_time: "60-90 minutes"
    notes: "Basic support assistant, no GPU required"

# Environment auto-detection
auto_environment:
  enabled: true
  detection_order:
    - "apple_silicon"
    - "linux_server"
    - "windows_pc"
    - "cpu_only"
    
  selection_criteria:
    apple_silicon:
      - "platform.system() == 'Darwin'"
      - "torch.backends.mps.is_available()"
    linux_server:
      - "platform.system() == 'Linux'"
      - "torch.cuda.is_available()"
      - "torch.cuda.get_device_properties(0).total_memory > 8 * 1024**3"
    windows_pc:
      - "platform.system() == 'Windows'"
      - "torch.cuda.is_available()"
    cpu_only:
      - "True"

# Customer support deployment
deployment:
  apple_silicon:
    frameworks: ["ollama", "mlx", "pytorch"]
    optimization: "coreml"
    serving: "local"
    integration: ["slack", "discord", "teams"]
    
  linux_server:
    frameworks: ["vllm", "tgi", "pytorch"]
    optimization: "tensorrt"
    serving: "production"
    integration: ["zendesk", "intercom", "salesforce"]
    
  windows_pc:
    frameworks: ["onnx", "pytorch", "directml"]
    optimization: "onnx"
    serving: "desktop"
    integration: ["outlook", "teams", "local_apps"]
    
  cpu_only:
    frameworks: ["onnx", "pytorch"]
    optimization: "cpu"
    serving: "edge"
    integration: ["basic_chat", "email_automation"]

# Customer support best practices
best_practices:
  response_guidelines:
    - "Always acknowledge customer concerns empathetically"
    - "Provide clear, actionable solutions"
    - "Include escalation paths when appropriate"
    - "Maintain professional tone throughout"
    - "Follow company policies consistently"
    
  quality_metrics:
    - "Response time < 2 minutes"
    - "First contact resolution > 80%"
    - "Customer satisfaction > 4.5/5"
    - "Policy compliance > 95%"
    - "Escalation rate < 15%"
    
  training_focus:
    - "Empathy and emotional intelligence"
    - "Product knowledge accuracy"
    - "Policy compliance and updates"
    - "Conflict resolution techniques"
    - "Multi-channel communication skills"