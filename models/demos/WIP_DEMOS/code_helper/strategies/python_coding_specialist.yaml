# Python Coding Specialist Strategy
# Uses Liquid Llama-3-8B Coding Agent - specialized for programming tasks

name: "python_coding_specialist"
description: "Specialized Python programming assistant using Liquid coding model"

model:
  name: "hf.co/Liquid1/llama-3-8b-liquid-coding-agent:Q4_K_M"
  type: "ollama"
  context_length: 4096
  
  # This model is specifically trained for coding tasks
  rationale: |
    The Liquid Llama-3-8B Coding Agent is specifically fine-tuned for programming tasks.
    It has deep Python knowledge, understands coding patterns, and can generate
    high-quality code examples and explanations. This makes it ideal for our
    programming assistance use case.

training:
  method: "lora"
  epochs: 3
  batch_size: 2
  learning_rate: 2e-4
  
  # LoRA configuration optimized for coding models
  lora_config:
    r: 16          # Higher rank for complex coding patterns
    alpha: 32      # Higher alpha for stronger adaptation
    dropout: 0.05  # Lower dropout to preserve coding knowledge
    target_modules:
      - "q_proj"
      - "v_proj" 
      - "k_proj"
      - "o_proj"
      - "gate_proj"
      - "up_proj"
      - "down_proj"
  
  # Training optimization
  gradient_accumulation_steps: 4
  warmup_steps: 100
  weight_decay: 0.01
  max_grad_norm: 0.3

dataset:
  path: "datasets/python_coding.jsonl"
  validation_split: 0.1
  max_sequence_length: 2048
  
  # Specific to coding tasks
  preprocessing:
    preserve_code_formatting: true
    tokenize_code_blocks: true
    handle_long_functions: true

evaluation:
  # Test the model's coding abilities
  test_prompts:
    - "How do I implement a binary search in Python?"
    - "What's the difference between lists and tuples?"
    - "How do I handle exceptions properly?"
    - "Explain Python decorators with an example"
    - "How do I work with APIs using requests?"
  
  # Metrics specific to coding assistance
  metrics:
    - "code_syntax_accuracy"
    - "explanation_clarity" 
    - "best_practices_adherence"
    - "example_quality"

hardware:
  recommended_gpu: "RTX 3070 or better"
  min_ram: "12GB"
  estimated_training_time: "4-6 minutes"
  model_size_mb: 280  # LoRA adapters only

deployment:
  use_case: "Interactive Python coding assistant"
  target_audience: "Developers learning Python or seeking quick help"
  integration_examples:
    - "IDE plugin for code suggestions"
    - "Interactive coding tutor"
    - "Code review assistant"

educational_focus: |
  This demo showcases:
  
  1. **Model Selection**: Why specialized coding models outperform general models
     - Pre-trained on massive code datasets
     - Understands programming patterns and idioms
     - Better at generating syntactically correct code
  
  2. **LoRA for Coding**: How parameter-efficient methods work with code
     - Preserves pre-trained coding knowledge
     - Fast adaptation to specific coding styles
     - Memory efficient for deployment
  
  3. **Dataset Quality**: Importance of high-quality coding examples
     - Clear explanations with working code
     - Best practices and common patterns
     - Error handling and edge cases
  
  4. **Practical Applications**: Real-world deployment scenarios
     - Code completion and suggestions
     - Educational programming assistance
     - Code review and optimization

comparison_baseline:
  base_model_performance: |
    Before fine-tuning, the base Liquid model already has strong coding abilities
    but may give generic explanations or miss Python-specific best practices.
  
  expected_improvements: |
    After fine-tuning with our Python dataset:
    - More Pythonic code examples
    - Better explanation of Python-specific concepts
    - Improved handling of common Python patterns
    - Enhanced focus on best practices and error handling