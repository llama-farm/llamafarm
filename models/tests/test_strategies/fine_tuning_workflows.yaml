# Fine-Tuning Workflows Strategy
# Complete fine-tuning pipelines with different frameworks

strategies:
  - name: pytorch_lora_finetuning
    description: Fine-tune models using PyTorch with LoRA for efficiency

    components:
      fine_tuner:
        type: pytorch
        config:
          base_model:
            name: meta-llama/Llama-2-7b-hf
            huggingface_id: meta-llama/Llama-2-7b-hf
            cache_dir: ./model_cache
            torch_dtype: float16

          method:
            type: lora
            r: 16
            alpha: 32
            dropout: 0.1
            target_modules:
              - q_proj
              - v_proj
              - k_proj
              - o_proj

          dataset:
            path: ./datasets/training_data.jsonl
            format: alpaca
            max_length: 2048
            train_split: 0.9
            val_split: 0.1

          training_args:
            output_dir: ./fine_tuned_models/lora
            num_train_epochs: 3
            per_device_train_batch_size: 4
            per_device_eval_batch_size: 4
            gradient_accumulation_steps: 4
            learning_rate: 2e-4
            warmup_ratio: 0.1
            logging_steps: 10
            save_steps: 100
            eval_steps: 100
            save_total_limit: 3
            load_best_model_at_end: true
            metric_for_best_model: eval_loss
            greater_is_better: false
            fp16: true
            gradient_checkpointing: true

      repository:
        type: huggingface
        config:
          token: ${HF_TOKEN}
          push_to_hub: false
          model_id: my-org/my-finetuned-model
          private: true
    
    constraints:
      requires_gpu: true
      min_gpu_memory_gb: 16
      max_training_time_hours: 24

    monitoring:
      use_tensorboard: true
      use_wandb: false
      log_level: INFO

  - name: llamafactory_comprehensive
    description: Comprehensive fine-tuning with LlamaFactory's advanced features
    
    components:
      fine_tuner:
        type: llamafactory
        config:

          model_name_or_path: meta-llama/Llama-2-13b-hf
          stage: sft
          do_train: true
          finetuning_type: lora
          lora_target: q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj
          
          dataset: alpaca
          template: default
          cutoff_len: 4096
          overwrite_cache: true
          preprocessing_num_workers: 16

          # LoRA configuration
          lora_rank: 32
          lora_alpha: 64
          lora_dropout: 0.1
          
          # Quantization settings
          quantization_bit: 4
          double_quantization: true
          quantization_type: nf4
          
          # Training hyperparameters
          output_dir: ./fine_tuned_models/llamafactory
          overwrite_output_dir: true
          per_device_train_batch_size: 2
          per_device_eval_batch_size: 2
          gradient_accumulation_steps: 8
          learning_rate: 5e-5
          num_train_epochs: 3
          lr_scheduler_type: cosine
          warmup_ratio: 0.1
          bf16: true
          gradient_checkpointing: true
          flash_attn: auto
          use_unsloth: false

      model_app:
        type: ollama
        config:
          base_url: http://localhost:11434
          default_model: custom-llamafactory

    deployment:
      export_format: ollama # Options: pytorch, onnx, ollama, gguf
      merge_lora: true
      quantize_after_merge: true
      quantization_method: q4_k_m

    constraints:
      requires_gpu: true
      min_vram_gb: 12
      recommended_cuda_version: "11.8"
      neftune_noise_alpha: 5

      # Evaluation and saving
      logging_steps: 10
      save_steps: 500
      eval_steps: 100
      evaluation_strategy: steps
      save_total_limit: 3
      load_best_model_at_end: true
      metric_for_best_model: eval_loss
      greater_is_better: false

      # Additional features
      report_to: tensorboard
      plot_loss: true
          
    constraints:
      requires_gpu: true
      min_gpu_memory_gb: 24
      recommended_gpu: A100

    monitoring:
      track_carbon_emissions: true
      log_level: INFO

  - name: multi_stage_training
    description: Multi-stage training pipeline with different methods
    
    stages:
      - name: stage1_full_finetune
        components:
          fine_tuner:
            type: pytorch
            config:
              base_model:
                name: microsoft/DialoGPT-small
                cache_dir: ./model_cache
              method:
                type: full
              training_args:
                num_train_epochs: 1
                learning_rate: 5e-5
                output_dir: ./checkpoints/stage1
                
      - name: stage2_lora_adaptation
        components:
          fine_tuner:
            type: pytorch
            config:
              base_model:
                name: ./checkpoints/stage1/best_model
              method:
                type: lora
                r: 8
                alpha: 16
              training_args:
                num_train_epochs: 2
                learning_rate: 2e-4
                output_dir: ./checkpoints/stage2
                
      - name: stage3_quantization
        components:
          converter:
            type: gguf_converter
            config:
              input_model: ./checkpoints/stage2/best_model
              output_path: ./final_models/model.gguf
              quantization: q4_k_m
    
    pipeline:
      execute_sequentially: true
      stop_on_failure: true
      save_intermediate: true
    
    constraints:
      total_budget_usd: 100
      max_total_hours: 48

