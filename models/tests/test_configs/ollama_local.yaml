# LlamaFarm Models - Ollama Local Configuration
# Configuration for running models locally via Ollama

name: "Ollama Local Models Configuration"
version: "1.0.0"
description: "Complete Ollama local model configuration with all popular models"

# Default to a good general-purpose model
default_provider: "ollama_llama3_1_8b"

# Fallback chain within local models (by size/capability)
fallback_chain:
  - "ollama_llama3_1_8b"        # Primary: Good balance
  - "ollama_llama3_2_3b"        # Secondary: Faster, smaller
  - "ollama_phi3_mini"          # Tertiary: Very fast
  - "ollama_mistral_7b"         # Alternative: Different model family

providers:
  # Llama Models
  ollama_llama3_1_8b:
    type: "local"
    provider: "ollama"
    model: "llama3.1:8b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 120
    temperature: 0.7
    description: "Llama 3.1 8B - excellent general-purpose model"

  ollama_llama3_1_70b:
    type: "local"
    provider: "ollama"
    model: "llama3.1:70b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 300                 # Longer timeout for larger model
    temperature: 0.7
    description: "Llama 3.1 70B - highest capability (requires 40GB+ RAM)"

  ollama_llama3_2_3b:
    type: "local"
    provider: "ollama"
    model: "llama3.2:3b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 60
    temperature: 0.7
    description: "Llama 3.2 3B - fast and lightweight"

  ollama_llama3_2_1b:
    type: "local"
    provider: "ollama"
    model: "llama3.2:1b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 30
    temperature: 0.7
    description: "Llama 3.2 1B - ultra-fast, minimal resource usage"

  # Mistral Models
  ollama_mistral_7b:
    type: "local"
    provider: "ollama"
    model: "mistral:7b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 90
    temperature: 0.7
    description: "Mistral 7B - excellent alternative to Llama"

  ollama_mixtral_8x7b:
    type: "local"
    provider: "ollama"
    model: "mixtral:8x7b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 180
    temperature: 0.7
    description: "Mixtral 8x7B - mixture of experts, high capability"

  # Microsoft Models
  ollama_phi3_mini:
    type: "local"
    provider: "ollama"
    model: "phi3:mini"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 30
    temperature: 0.7
    description: "Microsoft Phi-3 Mini - very fast, good for simple tasks"

  ollama_phi3_medium:
    type: "local"
    provider: "ollama"
    model: "phi3:medium"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 120
    temperature: 0.7
    description: "Microsoft Phi-3 Medium - balanced speed and capability"

  # Code-Specialized Models  
  ollama_codellama_13b:
    type: "local"
    provider: "ollama"
    model: "codellama:13b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 180
    temperature: 0.3             # Lower temperature for code generation
    description: "Code Llama 13B - specialized for programming tasks"

  ollama_codellama_34b:
    type: "local"
    provider: "ollama"
    model: "codellama:34b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 240
    temperature: 0.3
    description: "Code Llama 34B - most capable code model (requires 20GB+ RAM)"

  # Embedding Models
  ollama_nomic_embed:
    type: "local"
    provider: "ollama"
    model: "nomic-embed-text:latest"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 60
    description: "Nomic Embed - text embedding model"

  ollama_mxbai_embed:
    type: "local"
    provider: "ollama"
    model: "mxbai-embed-large:latest"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 60
    description: "MixedBread AI Large Embed - high-quality embeddings"

  # Multilingual Models
  ollama_qwen2_7b:
    type: "local"
    provider: "ollama"
    model: "qwen2:7b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 90
    temperature: 0.7
    description: "Qwen2 7B - excellent multilingual capabilities"

  # Small/Fast Models
  ollama_gemma_2b:
    type: "local"
    provider: "ollama"
    model: "gemma:2b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 45
    temperature: 0.7
    description: "Google Gemma 2B - fast and efficient"

  ollama_gemma_7b:
    type: "local"
    provider: "ollama"
    model: "gemma:7b"
    host: "${OLLAMA_HOST:localhost}"
    port: ${OLLAMA_PORT:11434}
    base_url: "${OLLAMA_BASE_URL:http://localhost:11434}"
    timeout: 90
    temperature: 0.7
    description: "Google Gemma 7B - good balance of speed and capability"

# Local-specific settings
local_settings:
  ollama:
    # Connection settings
    api_endpoint: "${OLLAMA_BASE_URL:http://localhost:11434}"
    health_check_interval: 30
    
    # Performance settings
    concurrent_requests: 4       # Limit concurrent requests to local server
    keep_alive: "5m"            # Keep model loaded for 5 minutes
    
    # Resource management
    # num_gpu: 1                 # Number of GPUs to use (if available)
    # num_cpu: 4                 # Number of CPU cores
    # num_thread: 8              # Number of threads per request

# Model selection rules for local deployment
model_selection:
  rules:
    - condition: "task_type == 'quick_test'"
      preferred_providers:
        - "ollama_phi3_mini"
        - "ollama_gemma_2b"
        - "ollama_llama3_2_1b"
    
    - condition: "task_type == 'general'"
      preferred_providers:
        - "ollama_llama3_1_8b"
        - "ollama_mistral_7b"
        - "ollama_llama3_2_3b"
    
    - condition: "task_type == 'code'"
      preferred_providers:
        - "ollama_codellama_13b"
        - "ollama_codellama_34b"
        - "ollama_llama3_1_8b"
    
    - condition: "task_type == 'complex'"
      preferred_providers:
        - "ollama_llama3_1_70b"
        - "ollama_mixtral_8x7b"
        - "ollama_codellama_34b"
    
    - condition: "speed_critical == true"
      preferred_providers:
        - "ollama_phi3_mini"
        - "ollama_llama3_2_1b"
        - "ollama_gemma_2b"
    
    - condition: "multilingual == true"
      preferred_providers:
        - "ollama_qwen2_7b"
        - "ollama_llama3_1_8b"
    
    - condition: "resource_limited == true"
      preferred_providers:
        - "ollama_llama3_2_1b"
        - "ollama_phi3_mini"
        - "ollama_gemma_2b"

# Performance monitoring for local models
monitoring:
  track_usage: true
  log_requests: true
  track_local_performance: true
  model_load_time_threshold: 30    # Alert if model takes >30s to load
  response_time_threshold: 10      # Alert if response takes >10s

# Automatic model management
# auto_management:
#   pull_missing_models: true       # Auto-pull models that aren't available
#   cleanup_unused_models: false    # Don't auto-cleanup (user decision)
#   update_models: false           # Don't auto-update (user decision)