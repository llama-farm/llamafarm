{
  "name": "Local Inference Engines Configuration",
  "version": "1.0.0",
  "default_provider": "vllm_llama2_7b",
  "providers": {
    "vllm_llama2_7b": {
      "type": "local",
      "provider": "vllm",
      "model": "meta-llama/Llama-2-7b-chat-hf",
      "max_model_len": 2048,
      "gpu_memory_utilization": 0.8,
      "trust_remote_code": true,
      "temperature": 0.7,
      "max_tokens": 100,
      "description": "Llama 2 7B Chat via vLLM"
    },
    "vllm_mistral_7b": {
      "type": "local",
      "provider": "vllm",
      "model": "mistralai/Mistral-7B-Instruct-v0.1",
      "max_model_len": 2048,
      "gpu_memory_utilization": 0.8,
      "trust_remote_code": true,
      "temperature": 0.7,
      "max_tokens": 100,
      "description": "Mistral 7B Instruct via vLLM"
    },
    "tgi_local": {
      "type": "local",
      "provider": "tgi",
      "endpoint": "http://localhost:8080",
      "timeout": 30,
      "temperature": 0.7,
      "max_tokens": 100,
      "top_p": 0.9,
      "description": "Local Text Generation Inference server"
    },
    "ollama_reference": {
      "type": "local",
      "provider": "ollama",
      "model": "llama3.1:8b",
      "host": "localhost",
      "port": 11434,
      "timeout": 120,
      "description": "Reference to existing Ollama setup"
    }
  },
  "engine_settings": {
    "vllm": {
      "gpu_memory_utilization": 0.8,
      "max_model_len": 2048,
      "trust_remote_code": true,
      "tensor_parallel_size": 1
    },
    "tgi": {
      "timeout": 30,
      "stream": false,
      "details": true
    },
    "ollama": {
      "host": "localhost",
      "port": 11434,
      "timeout": 120
    }
  },
  "resource_requirements": {
    "vllm_llama2_7b": {
      "gpu_memory_gb": 16,
      "ram_gb": 32
    },
    "vllm_mistral_7b": {
      "gpu_memory_gb": 16,
      "ram_gb": 32
    },
    "tgi_local": {
      "gpu_memory_gb": 8,
      "ram_gb": 16
    },
    "ollama_reference": {
      "ram_gb": 8
    }
  }
}