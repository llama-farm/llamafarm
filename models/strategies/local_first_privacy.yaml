# Local-First Privacy-Focused Strategy
# Complete data privacy with local model deployment and zero external API calls
version: "v1"

# Strategy metadata
strategy_info:
  name: "local_first_privacy"
  description: "Privacy-first strategy with local models and zero external dependencies"
  use_case: "Healthcare, legal, financial, sensitive enterprise data"
  difficulty: "intermediate"
  privacy_level: "maximum"
  deployment_type: "local_only"
  fallback_strategy: "local_high → local_medium → local_minimal"
  
# Environment-specific configurations for maximum privacy
environments:
  
  # Apple Silicon - Privacy-optimized local deployment
  apple_silicon:
    active: true
    model:
      base_model: "llama3.2:3b"      # Fast, efficient for M1/M2/M3
      model_type: "Ollama"
      device: "mps"
      ollama_model: "llama3.2:3b"
      download_if_missing: true
      torch_dtype: "float16"
      memory_fraction: 0.8           # Conservative memory usage
      
    training:
      method: "lora"                 # Local fine-tuning only
      batch_size: 2
      gradient_accumulation_steps: 4
      max_steps: 300
      learning_rate: 3e-4
      warmup_steps: 30
      logging_steps: 10
      eval_steps: 50
      save_steps: 50
      local_only: true               # Never send data externally
      
    lora_config:
      r: 16
      alpha: 32
      dropout: 0.1
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.7
      top_p: 0.9
      max_length: 1024
      do_sample: true
      
  # NVIDIA GPU - High-performance privacy setup
  nvidia_gpu:
    active: false
    model:
      base_model: "llama3.1:8b"     # Full capability with GPU
      model_type: "Ollama"
      device: "cuda"
      ollama_model: "llama3.1:8b"
      download_if_missing: true
      torch_dtype: "float16"
      memory_fraction: 0.9
      
    training:
      method: "lora"
      batch_size: 4
      gradient_accumulation_steps: 2
      max_steps: 400
      learning_rate: 2e-4
      warmup_steps: 40
      logging_steps: 20
      eval_steps: 100
      save_steps: 100
      local_only: true
      
    lora_config:
      r: 32
      alpha: 64
      dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.7
      top_p: 0.9
      max_length: 2048
      do_sample: true
      
  # CPU-only - Air-gapped systems
  cpu_only:
    active: false
    model:
      base_model: "phi3:mini"        # Optimized for CPU inference
      model_type: "Ollama"
      device: "cpu"
      ollama_model: "phi3:mini"
      download_if_missing: true
      torch_dtype: "float32"
      
    training:
      method: "lora"
      batch_size: 1
      gradient_accumulation_steps: 8
      max_steps: 200
      learning_rate: 5e-4
      warmup_steps: 20
      logging_steps: 10
      eval_steps: 40
      save_steps: 40
      local_only: true
      
    lora_config:
      r: 8
      alpha: 16
      dropout: 0.1
      target_modules: ["q_proj", "v_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.8
      top_p: 0.9
      max_length: 512
      do_sample: true

# Dataset configuration - local processing only
dataset:
  path: "datasets/private_data.jsonl"
  format: "jsonl"
  text_column: "output"
  prompt_template: |
    ### Private Query:
    {instruction}
    
    ### Confidential Response:
    {output}
  max_length: 1024
  validation_split: 0.15
  local_processing_only: true        # Never upload to external services

# Privacy-focused evaluation
evaluation:
  metrics:
    - "perplexity"
    - "privacy_compliance"
    - "data_leakage_check"
  eval_dataset: "validation"
  custom_metrics:
    privacy_compliance:
      no_external_calls: true
      encrypted_storage: true
      audit_trail_complete: true
  
# Output configuration - secure local storage
output:
  model_name: "privacy-local"
  save_directory: "./models/private"
  push_to_hub: false                 # Never upload models
  export_formats: ["pytorch"]
  encryption: true                   # Encrypt saved models
  
# Security and privacy features
safety:
  content_filter: true
  privacy_filter: true               # Filter potentially identifying info
  no_telemetry: true                 # Disable all telemetry
  audit_logging: true                # Local audit logs only
  memory_clearing: true              # Clear sensitive data from memory
  
# Hardware requirements for privacy deployment
hardware_requirements:
  apple_silicon:
    min_memory: "16GB"
    recommended_memory: "32GB"
    disk_space: "50GB"
    estimated_time: "20-30 minutes"
    notes: "Privacy-optimized for Apple Silicon"
    
  nvidia_gpu:
    min_memory: "24GB"
    recommended_memory: "48GB"
    gpu_memory: "12GB"
    disk_space: "75GB"
    estimated_time: "15-25 minutes"
    notes: "High-performance private training"
    
  cpu_only:
    min_memory: "8GB"
    recommended_memory: "16GB"
    disk_space: "25GB"
    estimated_time: "45-60 minutes"
    notes: "Air-gapped systems compatible"

# Environment auto-detection with privacy priority
auto_environment:
  enabled: true
  detection_order:
    - "apple_silicon"
    - "nvidia_gpu"
    - "cpu_only"
    
  selection_criteria:
    apple_silicon:
      - "platform.system() == 'Darwin'"
      - "torch.backends.mps.is_available()"
      - "psutil.virtual_memory().total > 16 * 1024**3"
    nvidia_gpu:
      - "torch.cuda.is_available()"
      - "torch.cuda.get_device_properties(0).total_memory > 10 * 1024**3"
    cpu_only:
      - "True"                       # Always available for air-gapped systems

# Privacy optimization features
privacy_optimization:
  network_isolation: true            # Block all external network calls
  encrypted_storage: true            # Encrypt all local data
  secure_deletion: true              # Secure deletion of temporary files
  memory_encryption: true            # Encrypt data in memory
  audit_trail: true                  # Complete local audit logging
  
# Compliance and standards
compliance:
  hipaa_compliant: true              # Healthcare data protection
  gdpr_compliant: true               # EU privacy regulation
  sox_compliant: true                # Financial data protection
  fisma_compatible: true             # Federal information security
  
# Local model management
model_management:
  ollama:
    auto_start: true                 # Start Ollama service automatically
    host: "localhost"
    port: 11434
    offline_mode: true               # No external model downloads during runtime
    
# LlamaFactory integration - privacy mode
llamafactory:
  model_name: microsoft/phi-3-mini-4k-instruct
  template: phi
  
  # method
  stage: sft
  do_train: true
  finetuning_type: lora
  lora_target: all
  lora_rank: 16
  lora_alpha: 32
  
  # dataset
  dataset_dir: ../datasets/private
  dataset: confidential_data
  cutoff_len: 1024
  max_samples: 100
  overwrite_cache: true
  preprocessing_num_workers: 2
  
  # output
  output_dir: ./private_output
  logging_steps: 10
  save_steps: 50
  plot_loss: true
  overwrite_output_dir: true
  
  # train - privacy optimized
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 3.0e-4
  num_train_epochs: 3.0
  lr_scheduler_type: cosine
  warmup_steps: 30
  bf16: true
  
  # eval
  val_size: 0.15
  per_device_eval_batch_size: 1
  eval_strategy: steps
  eval_steps: 50

# Use cases for maximum privacy
use_cases:
  - "Medical record analysis and patient consultation"
  - "Legal document review and contract analysis"
  - "Financial data processing and compliance"
  - "Government and defense applications"
  - "Research with proprietary data"
  - "Personal AI assistant for sensitive tasks"