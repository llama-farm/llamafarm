# Performance Enterprise Fine-Tuning Strategy
# High-performance fine-tuning for production enterprise applications
version: "v1"

# Strategy metadata
strategy_info:
  name: "performance_enterprise"
  description: "High-performance enterprise fine-tuning with maximum quality and throughput"
  use_case: "Production APIs, enterprise applications, high-volume services"
  difficulty: "advanced"
  performance_level: "maximum"
  fallback_strategy: "enterprise_gpu → enterprise_multi_gpu → cloud_fallback"
  
# Environment-specific configurations optimized for maximum performance
environments:
  
  # Apple Silicon Pro/Max/Ultra - High-performance configuration
  apple_silicon_pro:
    active: true
    model:
      base_model: "llama3.1:8b"     # Larger, high-quality model
      model_type: "Ollama"
      device: "mps"
      ollama_model: "llama3.1:8b"
      download_if_missing: true
      torch_dtype: "float16"
      memory_fraction: 0.9          # Use 90% of available memory
      compile_model: true           # JIT compilation for speed
    
    training:
      method: "qlora"               # QLoRA for memory efficiency with quality
      batch_size: 4                 # Larger batch size
      gradient_accumulation_steps: 4
      max_steps: 500                # More training for quality
      learning_rate: 2e-4
      warmup_steps: 50
      logging_steps: 20
      eval_steps: 100
      save_steps: 100
      weight_decay: 0.01
      
    lora_config:
      r: 32                         # High rank for maximum adaptation
      alpha: 64
      dropout: 0.05                 # Lower dropout for precision
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "float16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: true
      
    generation:
      temperature: 0.7
      top_p: 0.9
      max_length: 1024              # Longer responses for quality
      do_sample: true
      num_beams: 4                  # Beam search for quality
      
  # NVIDIA Enterprise GPU - Maximum performance
  nvidia_enterprise:
    active: false
    model:
      base_model: "llama3.1:13b"    # Even larger model for enterprise
      model_type: "CausalLM"
      device: "cuda"
      load_in_4bit: true
      torch_dtype: "bfloat16"       # Better precision for enterprise
      flash_attention: true         # Memory-efficient attention
    
    training:
      method: "qlora"
      batch_size: 8                 # Large batch for enterprise hardware
      gradient_accumulation_steps: 2
      max_steps: 800
      learning_rate: 1e-4
      warmup_steps: 80
      logging_steps: 20
      eval_steps: 100
      save_steps: 100
      weight_decay: 0.01
      dataloader_num_workers: 8
      gradient_checkpointing: true
      
    lora_config:
      r: 64                         # Maximum rank for enterprise quality
      alpha: 128
      dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "bfloat16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: true
      
    generation:
      temperature: 0.6              # More focused for enterprise
      top_p: 0.9
      max_length: 2048              # Long responses for detailed answers
      do_sample: true
      num_beams: 6
      
  # Multi-GPU Enterprise - Distributed training
  multi_gpu_enterprise:
    active: false
    model:
      base_model: "llama3.1:70b"    # Largest model for maximum quality
      model_type: "CausalLM"
      device: "cuda"
      load_in_4bit: true
      torch_dtype: "bfloat16"
      flash_attention: true
      
    training:
      method: "qlora"
      batch_size: 16               # Very large batch
      gradient_accumulation_steps: 1
      max_steps: 1000
      learning_rate: 8e-5
      warmup_steps: 100
      logging_steps: 25
      eval_steps: 200
      save_steps: 200
      weight_decay: 0.01
      dataloader_num_workers: 16
      gradient_checkpointing: true
      ddp_find_unused_parameters: false
      
    lora_config:
      r: 128                        # Maximum possible rank
      alpha: 256
      dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "bfloat16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: true
      
    generation:
      temperature: 0.5              # Very focused for enterprise precision
      top_p: 0.9
      max_length: 4096              # Very long responses
      do_sample: true
      num_beams: 8                  # Maximum beam search

# Dataset configuration - optimized for quality
dataset:
  path: "datasets/enterprise_data.jsonl"
  format: "jsonl"
  text_column: "output"
  prompt_template: |
    ### Enterprise Query:
    {instruction}
    
    ### Professional Response:
    {output}
  max_length: 2048                  # Long contexts for enterprise
  validation_split: 0.15            # More validation data
  quality_filter: true              # Filter low-quality examples
  balance_classes: true             # Ensure balanced training

# Comprehensive evaluation for enterprise
evaluation:
  metrics:
    - "perplexity"
    - "bleu"
    - "rouge"
    - "bertscore"
    - "enterprise_quality"
  eval_dataset: "validation"
  custom_metrics:
    enterprise_quality:
      professional_tone: 0.3
      accuracy: 0.4
      completeness: 0.3
  
# Output configuration - enterprise grade
output:
  model_name: "enterprise-performance"
  save_directory: "./models/enterprise"
  push_to_hub: false
  export_formats: ["pytorch", "onnx", "tensorrt"]
  checkpointing: "comprehensive"
  backup_strategy: "redundant"
  
# Enterprise safety and compliance
safety:
  content_filter: true
  bias_detection: true
  toxicity_screening: true
  compliance_checks: true
  audit_logging: true
  max_toxicity_score: 0.1           # Very strict for enterprise
  
# Hardware requirements - enterprise grade
hardware_requirements:
  apple_silicon_pro:
    min_memory: "32GB"
    recommended_memory: "64GB"
    disk_space: "20GB"
    estimated_time: "30-45 minutes"
    notes: "Requires M2 Pro/Max or better"
    
  nvidia_enterprise:
    min_memory: "64GB"
    recommended_memory: "128GB"
    gpu_memory: "24GB"
    disk_space: "40GB"
    estimated_time: "20-30 minutes"
    notes: "RTX 4090 or A100 recommended"
    
  multi_gpu_enterprise:
    min_memory: "256GB"
    recommended_memory: "512GB"
    gpu_memory: "80GB per GPU"
    gpu_count: "4-8"
    disk_space: "100GB"
    estimated_time: "15-25 minutes"
    notes: "Multiple H100 or A100 GPUs required"

# Environment auto-detection - prefer highest performance
auto_environment:
  enabled: true
  detection_order:
    - "multi_gpu_enterprise"
    - "nvidia_enterprise"
    - "apple_silicon_pro"
    
  selection_criteria:
    multi_gpu_enterprise:
      - "torch.cuda.device_count() > 2"
      - "torch.cuda.get_device_properties(0).total_memory > 40 * 1024**3"
    nvidia_enterprise:
      - "torch.cuda.is_available()"
      - "torch.cuda.get_device_properties(0).total_memory > 20 * 1024**3"
    apple_silicon_pro:
      - "platform.system() == 'Darwin'"
      - "torch.backends.mps.is_available()"
      - "psutil.virtual_memory().total > 30 * 1024**3"

# Performance optimization features
performance_optimization:
  mixed_precision: true
  gradient_checkpointing: true
  flash_attention: true
  compile_optimization: true
  memory_efficient_attention: true
  
# Enterprise features
enterprise_features:
  monitoring:
    real_time_metrics: true
    performance_alerts: true
    resource_tracking: true
    
  scalability:
    horizontal_scaling: true
    load_balancing: true
    auto_scaling: true
    
  reliability:
    fault_tolerance: true
    automatic_recovery: true
    redundant_storage: true
    
  security:
    encryption_at_rest: true
    encryption_in_transit: true
    access_control: true
    audit_logging: true

# Production deployment configurations
deployment:
  apple_silicon_pro:
    frameworks: ["mlx", "coreml", "ollama"]
    optimization: "coreml"
    serving: "high_performance"
    
  nvidia_enterprise:
    frameworks: ["vllm", "tensorrt", "triton"]
    optimization: "tensorrt"
    serving: "production"
    
  multi_gpu_enterprise:
    frameworks: ["vllm", "deepspeed", "megatron"]
    optimization: "distributed"
    serving: "enterprise"

# LlamaFactory integration for enterprise
llamafactory:
  model_name: meta-llama/Llama-2-13b-chat-hf
  template: llama2
  
  # Advanced training configuration
  stage: sft
  do_train: true
  finetuning_type: qlora
  lora_target: all
  lora_rank: 64
  lora_alpha: 128
  
  # Enterprise dataset handling
  dataset_dir: ../datasets/enterprise
  dataset: enterprise_data
  cutoff_len: 2048
  max_samples: 1000
  overwrite_cache: true
  preprocessing_num_workers: 16
  
  # High-performance training
  output_dir: ./enterprise_output
  logging_steps: 20
  save_steps: 100
  plot_loss: true
  overwrite_output_dir: true
  
  per_device_train_batch_size: 4
  gradient_accumulation_steps: 4
  learning_rate: 1.0e-4
  num_train_epochs: 5.0
  lr_scheduler_type: cosine
  warmup_steps: 50
  bf16: true
  
  # Enterprise evaluation
  val_size: 0.15
  per_device_eval_batch_size: 2
  eval_strategy: steps
  eval_steps: 100