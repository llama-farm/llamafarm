# Budget Local-Optimized Strategy
# Minimal cost fine-tuning with local models and efficient resource usage
version: "v1"

# Strategy metadata
strategy_info:
  name: "budget_local_optimized"
  description: "Ultra-low-cost local fine-tuning for budget-conscious projects"
  use_case: "Startups, students, hobbyists, proof-of-concepts"
  difficulty: "easy"
  cost_level: "minimal"
  fallback_strategy: "local_small → local_tiny → simulation"
  
# Environment-specific configurations optimized for minimal resource usage
environments:
  
  # Apple Silicon - Optimized for minimal memory usage
  apple_silicon:
    active: true
    model:
      base_model: "phi3:mini"       # Very efficient 3.8B model
      model_type: "Ollama"
      device: "mps"
      ollama_model: "phi3:mini"
      download_if_missing: true
      torch_dtype: "float16"
      memory_fraction: 0.6          # Use only 60% of available memory
    
    training:
      method: "lora"
      batch_size: 1                 # Minimal batch size
      gradient_accumulation_steps: 8
      max_steps: 100                # Shorter training for speed
      learning_rate: 5e-4
      warmup_steps: 10
      logging_steps: 5
      eval_steps: 25
      save_steps: 25
      
    lora_config:
      r: 4                          # Very low rank for efficiency
      alpha: 8
      dropout: 0.1
      target_modules: ["q_proj", "v_proj"]  # Minimal targets
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.7
      top_p: 0.9
      max_length: 256               # Shorter responses for speed
      do_sample: true
      
  # Linux Budget Server - Consumer hardware
  linux_budget:
    active: false
    model:
      base_model: "gemma2:2b"       # Google's efficient 2B model
      model_type: "Ollama"
      device: "cuda"
      ollama_model: "gemma2:2b"
      download_if_missing: true
      torch_dtype: "float16"
    
    training:
      method: "lora"
      batch_size: 2
      gradient_accumulation_steps: 4
      max_steps: 150
      learning_rate: 3e-4
      warmup_steps: 15
      logging_steps: 10
      eval_steps: 30
      save_steps: 30
      
    lora_config:
      r: 8
      alpha: 16
      dropout: 0.1
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.7
      top_p: 0.9
      max_length: 384
      do_sample: true
      
  # CPU-only - No GPU required, ultra-minimal
  cpu_only:
    active: false
    model:
      base_model: "tinyllama:1.1b"  # Smallest viable model
      model_type: "Ollama"
      device: "cpu"
      ollama_model: "tinyllama:1.1b"
      download_if_missing: true
      torch_dtype: "float32"
    
    training:
      method: "lora"
      batch_size: 1
      gradient_accumulation_steps: 16
      max_steps: 50                 # Very short training
      learning_rate: 1e-3
      warmup_steps: 5
      logging_steps: 5
      eval_steps: 15
      save_steps: 15
      
    lora_config:
      r: 2                          # Extremely low rank
      alpha: 4
      dropout: 0.1
      target_modules: ["q_proj"]    # Single target for minimal impact
      bias: "none"
      task_type: "CAUSAL_LM"
      
    generation:
      temperature: 0.8
      top_p: 0.9
      max_length: 128               # Very short responses
      do_sample: true

# Dataset configuration - minimal for speed
dataset:
  path: "datasets/sample_data.jsonl"
  format: "jsonl"
  text_column: "output"
  prompt_template: |
    Q: {instruction}
    A: {output}
  max_length: 256                   # Shorter contexts
  validation_split: 0.1
  max_examples: 50                  # Limit training data for speed

# Minimal evaluation
evaluation:
  metrics:
    - "perplexity"
  eval_dataset: "validation"
  
# Output configuration
output:
  model_name: "budget-optimized"
  save_directory: "./models/budget"
  push_to_hub: false
  
# Safety (minimal but present)
safety:
  content_filter: false            # Disable for speed
  max_toxicity_score: 0.5
  
# Hardware requirements - very minimal
hardware_requirements:
  apple_silicon:
    min_memory: "4GB"
    recommended_memory: "8GB"
    disk_space: "2GB"
    estimated_time: "5-10 minutes"
    notes: "Uses smallest viable model"
    
  linux_budget:
    min_memory: "6GB"
    recommended_memory: "12GB"
    gpu_memory: "4GB"
    disk_space: "3GB"
    estimated_time: "8-12 minutes"
    
  cpu_only:
    min_memory: "2GB"
    recommended_memory: "4GB"
    disk_space: "1GB"
    estimated_time: "15-25 minutes"
    notes: "Fastest CPU-only option"

# Environment auto-detection - prefer minimal resources
auto_environment:
  enabled: true
  detection_order:
    - "apple_silicon"
    - "linux_budget"
    - "cpu_only"
    
  selection_criteria:
    apple_silicon:
      - "platform.system() == 'Darwin'"
      - "torch.backends.mps.is_available()"
    linux_budget:
      - "platform.system() == 'Linux'"
      - "torch.cuda.is_available()"
    cpu_only:
      - "True"

# Cost optimization features
cost_optimization:
  electricity_cost_awareness: true
  prefer_smaller_models: true
  short_training_cycles: true
  minimal_logging: true
  
# Scaling path
scaling:
  next_tier: "hybrid_cloud_local"
  upgrade_triggers:
    - "need_better_quality"
    - "higher_volume_requirements"
    - "budget_increase"