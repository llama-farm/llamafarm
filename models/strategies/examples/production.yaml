# LlamaFarm Models - Production Configuration
# Optimized for reliability, performance, and cost efficiency in production

name: "LlamaFarm Models - Production Configuration"
version: "1.0.0"
description: "Production configuration with robust fallback chains and monitoring"

# Production-grade primary provider
default_provider: "openai_gpt4o_mini"

# Multi-tier fallback chain for high availability
fallback_chain:
  - "openai_gpt4o_mini"          # Primary: Fast, reliable cloud
  - "anthropic_claude_3_haiku"   # Secondary: Alternative cloud provider
  - "together_llama3_70b"        # Tertiary: Alternative cloud platform
  - "ollama_llama3_1_8b"         # Final: Local fallback if all cloud fails

providers:
  # Primary cloud providers
  openai_gpt4o_mini:
    type: "cloud"
    provider: "openai"
    model: "gpt-4o-mini"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.00015
    timeout: 30
    max_retries: 3
    description: "Primary production model - fast and cost-effective"

  openai_gpt4_turbo:
    type: "cloud"
    provider: "openai"
    model: "gpt-4-turbo-preview"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.01
    timeout: 60
    max_retries: 3
    description: "Advanced model for complex production tasks"

  # Alternative cloud providers for redundancy
  anthropic_claude_3_haiku:
    type: "cloud"
    provider: "anthropic"
    model: "claude-3-haiku-20240307"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.00025
    timeout: 45
    max_retries: 3
    description: "Secondary provider for redundancy"

  anthropic_claude_3_sonnet:
    type: "cloud"
    provider: "anthropic"
    model: "claude-3-sonnet-20240229"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.003
    timeout: 45
    max_retries: 3
    description: "Balanced performance model for complex tasks"

  # High-performance cloud alternatives
  together_llama3_70b:
    type: "cloud"
    provider: "together"
    model: "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
    api_key: "${TOGETHER_API_KEY}"
    base_url: "https://api.together.xyz/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.00088
    timeout: 60
    max_retries: 2
    description: "High-performance alternative platform"

  groq_llama3_70b:
    type: "cloud"
    provider: "groq"
    model: "llama3-70b-8192"
    api_key: "${GROQ_API_KEY}"
    base_url: "https://api.groq.com/openai/v1"
    max_tokens: 8192
    temperature: 0.7
    cost_per_1k_tokens: 0.00064
    timeout: 30
    max_retries: 2
    description: "Ultra-fast inference for speed-critical production tasks"

  # Local backup for complete cloud outages
  ollama_llama3_1_8b:
    type: "local"
    provider: "ollama"
    model: "llama3.1:8b"
    host: "${OLLAMA_HOST:localhost}"
    port: 11434
    timeout: 120
    temperature: 0.7
    description: "Local backup model for emergency fallback"

# Production global settings
global_settings:
  timeout: 120                   # Longer timeout for reliability
  max_retries: 5                 # More retries in production
  default_temperature: 0.7       # Consistent, balanced temperature
  default_max_tokens: 2048       # Standard response length
  exponential_backoff: true      # Intelligent retry backoff
  backoff_multiplier: 2
  max_backoff_seconds: 60

# Production model selection rules
model_selection:
  rules:
    - condition: "task_type == 'customer_support'"
      preferred_providers: 
        - "openai_gpt4o_mini"
        - "anthropic_claude_3_haiku" 
    
    - condition: "task_type == 'content_generation'"
      preferred_providers:
        - "anthropic_claude_3_sonnet"
        - "openai_gpt4_turbo"
    
    - condition: "task_type == 'code_generation'"
      preferred_providers:
        - "openai_gpt4_turbo"
        - "anthropic_claude_3_sonnet"
    
    - condition: "speed_critical == true"
      preferred_providers:
        - "groq_llama3_70b"
        - "openai_gpt4o_mini"
    
    - condition: "cost_sensitive == true"
      preferred_providers:
        - "openai_gpt4o_mini"
        - "anthropic_claude_3_haiku"
    
    - condition: "high_complexity == true"
      preferred_providers:
        - "openai_gpt4_turbo"
        - "anthropic_claude_3_sonnet"
        - "together_llama3_70b"

# Aggressive rate limiting for production stability
rate_limits:
  openai:
    requests_per_minute: 1000
    tokens_per_minute: 500000
    requests_per_day: 50000
    burst_limit: 20
    
  anthropic:
    requests_per_minute: 100
    tokens_per_minute: 200000
    requests_per_day: 10000
    
  together:
    requests_per_minute: 600
    tokens_per_minute: 1000000
    
  groq:
    requests_per_minute: 30
    tokens_per_minute: 20000

# Production caching for performance
caching:
  enabled: true
  backend: "redis"               # Redis for production
  ttl_seconds: 7200              # 2 hours
  max_size: "10GB"               # Large cache for production
  
  redis:
    host: "${REDIS_HOST:localhost}"
    port: 6379
    db: 0
    password: "${REDIS_PASSWORD}"

# Comprehensive production monitoring
monitoring:
  # Core monitoring
  track_usage: true
  log_requests: true
  log_responses: false           # Don't log responses in prod (privacy)
  alert_on_errors: true
  
  # Cost monitoring
  cost_alert_threshold: 100.0    # Alert at $100/day
  cost_tracking_enabled: true
  
  # Performance monitoring
  latency_alert_threshold_ms: 10000  # Alert if >10s response time
  track_latency_percentiles: true
  
  # Error monitoring
  error_rate_threshold: 0.02     # Alert at 2% error rate
  consecutive_failures_threshold: 5
  
  # Advanced monitoring
  metrics_backend: "prometheus"
  metrics_port: 9090
  log_level: "WARNING"           # Less verbose in production
  log_format: "json"             # Structured logging

# Production security
security:
  validate_api_keys: true
  mask_api_keys_in_logs: true
  input_max_length: 100000       # Limit input size
  output_max_length: 50000       # Limit output size

# Load balancing for high availability
load_balancing:
  strategy: "weighted"
  health_check_interval: 30
  health_check_timeout: 5
  
  providers:
    openai_cluster:
      instances:
        - provider: "openai_gpt4o_mini"
          weight: 3
        - provider: "openai_gpt4_turbo"
          weight: 1
    
    anthropic_cluster:
      instances:
        - provider: "anthropic_claude_3_haiku"
          weight: 2
        - provider: "anthropic_claude_3_sonnet"
          weight: 1