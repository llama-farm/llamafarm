# LlamaFarm Models System - Comprehensive Default Configuration
# This file demonstrates ALL possible configuration options with detailed comments
# Uncomment and modify sections as needed for your use case

name: "LlamaFarm Models - Complete Configuration Template"
version: "1.0.0"
description: "Comprehensive template showing all possible configuration options"

# =============================================================================
# GLOBAL SETTINGS
# =============================================================================

# Default provider to use when no specific provider is requested
default_provider: "openai_gpt4o_mini"

# Fallback chain - providers to try in order if primary fails
fallback_chain: 
  - "openai_gpt4o_mini"      # Primary: Fast and cost-effective
  - "anthropic_claude_3_haiku" # Secondary: Alternative cloud provider
  - "ollama_llama3_1"         # Tertiary: Local fallback

# Global timeout settings (can be overridden per provider)
global_settings:
  timeout: 120                 # Default timeout in seconds
  max_retries: 3              # Maximum retry attempts
  default_temperature: 0.7    # Default temperature for all models
  default_max_tokens: 2048    # Default max tokens
  
  # Advanced settings
  # exponential_backoff: true    # Use exponential backoff for retries
  # backoff_multiplier: 2        # Backoff multiplier
  # max_backoff_seconds: 60      # Maximum backoff time

# =============================================================================
# CLOUD PROVIDERS - API-based models
# =============================================================================

providers:
  
  # ---------------------------------------------------------------------------
  # OpenAI Models
  # ---------------------------------------------------------------------------
  
  openai_gpt4o_mini:
    type: "cloud"
    provider: "openai"
    model: "gpt-4o-mini"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    
    # Generation parameters
    max_tokens: 4096
    temperature: 0.7
    top_p: 0.9
    frequency_penalty: 0.0
    presence_penalty: 0.0
    
    # Metadata and costs
    cost_per_1k_tokens: 0.00015
    description: "Fast, cost-effective model for general tasks"
    
    # Advanced OpenAI-specific options
    # stream: false                 # Enable streaming responses
    # stop: ["\n", "END"]          # Stop sequences
    # logit_bias: {}               # Modify likelihood of tokens
    # user: "user_id"              # User identifier for abuse monitoring
    # seed: 42                     # Deterministic outputs (beta)
    # response_format: {"type": "json_object"}  # Force JSON output
    
    # Custom headers
    # custom_headers:
    #   "X-Custom-Header": "value"
    
    # Timeout overrides
    timeout: 60                   # Override global timeout
    max_retries: 5               # Override global retries

  openai_gpt4_turbo:
    type: "cloud"
    provider: "openai"
    model: "gpt-4-turbo-preview"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.01
    description: "Advanced reasoning and complex tasks"

  openai_gpt4:
    type: "cloud"
    provider: "openai"
    model: "gpt-4"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    max_tokens: 8192
    temperature: 0.7
    cost_per_1k_tokens: 0.03
    description: "Most capable OpenAI model"

  openai_gpt35_turbo:
    type: "cloud"
    provider: "openai"
    model: "gpt-3.5-turbo"
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.0005
    description: "Legacy fast model, good for simple tasks"

  # ---------------------------------------------------------------------------
  # Anthropic Claude Models
  # ---------------------------------------------------------------------------
  
  anthropic_claude_3_opus:
    type: "cloud"
    provider: "anthropic"
    model: "claude-3-opus-20240229"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.015
    description: "Most capable Claude model"
    
    # Anthropic-specific parameters
    # system: "You are a helpful assistant"  # System message
    # top_k: 40                             # Top-k sampling

  anthropic_claude_3_sonnet:
    type: "cloud"
    provider: "anthropic"
    model: "claude-3-sonnet-20240229"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.003
    description: "Balanced performance and cost"

  anthropic_claude_3_haiku:
    type: "cloud"
    provider: "anthropic"
    model: "claude-3-haiku-20240307"
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.00025
    description: "Fast and affordable Claude model"

  # ---------------------------------------------------------------------------
  # Together AI Models
  # ---------------------------------------------------------------------------
  
  together_llama3_70b:
    type: "cloud"
    provider: "together"
    model: "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
    api_key: "${TOGETHER_API_KEY}"
    base_url: "https://api.together.xyz/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.00088
    description: "Llama 3.1 70B via Together AI"

  together_mixtral_8x7b:
    type: "cloud"
    provider: "together"
    model: "mistralai/Mixtral-8x7B-Instruct-v0.1"
    api_key: "${TOGETHER_API_KEY}"
    base_url: "https://api.together.xyz/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.0006
    description: "Mixtral MoE model via Together AI"

  # ---------------------------------------------------------------------------
  # Groq Models (Ultra-fast inference)
  # ---------------------------------------------------------------------------
  
  groq_llama3_70b:
    type: "cloud"
    provider: "groq"
    model: "llama3-70b-8192"
    api_key: "${GROQ_API_KEY}"
    base_url: "https://api.groq.com/openai/v1"
    max_tokens: 8192
    temperature: 0.7
    cost_per_1k_tokens: 0.00064
    description: "Llama 3 70B on Groq LPU (very fast inference)"

  groq_mixtral_8x7b:
    type: "cloud"
    provider: "groq"
    model: "mixtral-8x7b-32768"
    api_key: "${GROQ_API_KEY}"
    base_url: "https://api.groq.com/openai/v1"
    max_tokens: 32768
    temperature: 0.7
    cost_per_1k_tokens: 0.00027
    description: "Mixtral on Groq LPU (32k context window)"

  groq_gemma_7b:
    type: "cloud"
    provider: "groq"
    model: "gemma-7b-it"
    api_key: "${GROQ_API_KEY}"
    base_url: "https://api.groq.com/openai/v1"
    max_tokens: 8192
    temperature: 0.7
    cost_per_1k_tokens: 0.00007
    description: "Google Gemma 7B on Groq (very affordable)"

  # ---------------------------------------------------------------------------
  # Cohere Models
  # ---------------------------------------------------------------------------
  
  cohere_command_r_plus:
    type: "cloud"
    provider: "cohere"
    model: "command-r-plus"
    api_key: "${COHERE_API_KEY}"
    base_url: "https://api.cohere.ai/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.003
    description: "Cohere's most capable model, great for RAG"
    
    # Cohere-specific settings
    # search_queries_only: false    # For RAG applications
    # documents: []                 # Pre-provided documents

  cohere_command_r:
    type: "cloud"
    provider: "cohere"
    model: "command-r"
    api_key: "${COHERE_API_KEY}"
    base_url: "https://api.cohere.ai/v1"
    max_tokens: 4096
    temperature: 0.7
    cost_per_1k_tokens: 0.0005
    description: "Cohere's fast model"

# =============================================================================
# LOCAL PROVIDERS - Self-hosted models
# =============================================================================

  # ---------------------------------------------------------------------------
  # Ollama Models (Local)
  # ---------------------------------------------------------------------------
  
  ollama_llama3_1_8b:
    type: "local"
    provider: "ollama"
    model: "llama3.1:8b"
    host: "${OLLAMA_HOST:localhost}"
    port: 11434
    timeout: 120
    temperature: 0.7
    description: "Llama 3.1 8B running locally via Ollama"
    
    # Ollama-specific options
    # options:
    #   num_gpu: 1                  # Number of GPUs to use
    #   num_cpu: 4                  # Number of CPU cores
    #   num_thread: 8               # Number of threads
    #   num_ctx: 2048              # Context window size
    #   repeat_penalty: 1.1         # Repetition penalty
    #   seed: -1                    # Random seed
    #   tfs_z: 1.0                 # Tail-free sampling
    #   top_k: 40                   # Top-k sampling
    #   top_p: 0.9                 # Top-p sampling

  ollama_llama3_1_70b:
    type: "local"
    provider: "ollama"
    model: "llama3.1:70b"
    host: "${OLLAMA_HOST:localhost}"
    port: 11434
    timeout: 300                   # Longer timeout for larger model
    temperature: 0.7
    description: "Llama 3.1 70B - requires significant RAM/VRAM"

  ollama_llama3_2_3b:
    type: "local"
    provider: "ollama"
    model: "llama3.2:3b"
    host: "${OLLAMA_HOST:localhost}"
    port: 11434
    timeout: 60
    temperature: 0.7
    description: "Llama 3.2 3B - smaller, faster local model"

  ollama_mistral_7b:
    type: "local"
    provider: "ollama"
    model: "mistral:7b"
    host: "${OLLAMA_HOST:localhost}"
    port: 11434
    timeout: 90
    temperature: 0.7
    description: "Mistral 7B running locally"

  ollama_phi3_mini:
    type: "local"
    provider: "ollama"
    model: "phi3:mini"
    host: "${OLLAMA_HOST:localhost}"
    port: 11434
    timeout: 30
    temperature: 0.7
    description: "Microsoft Phi-3 Mini - very fast local model"

  ollama_codellama_13b:
    type: "local"
    provider: "ollama"
    model: "codellama:13b"
    host: "${OLLAMA_HOST:localhost}"
    port: 11434
    timeout: 180
    temperature: 0.3              # Lower temperature for code
    description: "Code Llama 13B for programming tasks"

  ollama_qwen2_7b:
    type: "local"
    provider: "ollama"
    model: "qwen2:7b"
    host: "${OLLAMA_HOST:localhost}"
    port: 11434
    timeout: 90
    temperature: 0.7
    description: "Qwen2 7B - multilingual capabilities"

  # ---------------------------------------------------------------------------
  # Hugging Face Models (Local)
  # ---------------------------------------------------------------------------
  
  hf_gpt2:
    type: "local"
    provider: "huggingface"
    model: "gpt2"
    cache_dir: "~/.cache/huggingface/hub"
    device: "cpu"                 # "cpu", "cuda", "mps", "auto"
    token: "${HF_TOKEN}"          # Optional for public models
    max_tokens: 100
    temperature: 0.7
    description: "GPT-2 base model from Hugging Face"
    
    # Hugging Face specific options
    # torch_dtype: "float16"       # Model precision
    # device_map: "auto"           # Automatic device mapping
    # load_in_8bit: false         # 8-bit quantization
    # load_in_4bit: false         # 4-bit quantization
    # trust_remote_code: false    # Allow custom code
    # revision: "main"             # Model revision/branch

  hf_distilgpt2:
    type: "local"
    provider: "huggingface"
    model: "distilgpt2"
    cache_dir: "~/.cache/huggingface/hub"
    device: "cpu"
    token: "${HF_TOKEN}"
    max_tokens: 100
    temperature: 0.7
    description: "DistilGPT-2 - smaller, faster GPT-2"

  hf_flan_t5_base:
    type: "local"
    provider: "huggingface"
    model: "google/flan-t5-base"
    cache_dir: "~/.cache/huggingface/hub"
    device: "cpu"
    token: "${HF_TOKEN}"
    max_tokens: 256
    temperature: 0.7
    description: "FLAN-T5 Base - instruction-tuned T5"

  # ---------------------------------------------------------------------------
  # vLLM Models (High-performance local inference)
  # ---------------------------------------------------------------------------
  
  vllm_llama2_7b:
    type: "local"
    provider: "vllm"
    model: "meta-llama/Llama-2-7b-chat-hf"
    max_model_len: 2048
    gpu_memory_utilization: 0.8
    trust_remote_code: true
    temperature: 0.7
    max_tokens: 512
    description: "Llama 2 7B Chat via vLLM (requires GPU)"
    
    # vLLM specific options
    # tensor_parallel_size: 1      # Number of GPUs for tensor parallelism
    # pipeline_parallel_size: 1    # Number of pipeline stages
    # dtype: "float16"             # Model data type
    # quantization: "awq"          # Quantization method
    # max_num_seqs: 256           # Maximum number of sequences
    # max_num_batched_tokens: 2048 # Maximum tokens per batch
    # block_size: 16              # Size of KV cache blocks

  vllm_mistral_7b:
    type: "local"
    provider: "vllm"
    model: "mistralai/Mistral-7B-Instruct-v0.1"
    max_model_len: 4096
    gpu_memory_utilization: 0.9
    trust_remote_code: false
    temperature: 0.7
    max_tokens: 1024
    description: "Mistral 7B Instruct via vLLM"

  # ---------------------------------------------------------------------------
  # Text Generation Inference (TGI) Models
  # ---------------------------------------------------------------------------
  
  tgi_endpoint:
    type: "local"
    provider: "tgi"
    endpoint: "http://localhost:8080"
    timeout: 30
    temperature: 0.7
    max_tokens: 512
    description: "Text Generation Inference server endpoint"
    
    # TGI specific options
    # headers:
    #   "Authorization": "Bearer ${TGI_TOKEN}"
    # parameters:
    #   do_sample: true
    #   top_k: 50
    #   top_p: 0.9
    #   repetition_penalty: 1.03

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================

# Model selection rules for automatic provider selection
model_selection:
  rules:
    - condition: "task_type == 'code'"
      preferred_providers: 
        - "openai_gpt4_turbo"
        - "anthropic_claude_3_opus"
        - "ollama_codellama_13b"
        - "groq_llama3_70b"
      
    - condition: "task_type == 'chat'"
      preferred_providers:
        - "openai_gpt4o_mini"
        - "anthropic_claude_3_haiku"
        - "ollama_llama3_1_8b"
        - "groq_mixtral_8x7b"
      
    - condition: "task_type == 'analysis'"
      preferred_providers:
        - "anthropic_claude_3_sonnet"
        - "openai_gpt4_turbo"
        - "together_llama3_70b"
        - "cohere_command_r_plus"
      
    - condition: "task_type == 'creative'"
      preferred_providers:
        - "anthropic_claude_3_opus"
        - "openai_gpt4"
        - "together_mixtral_8x7b"
      
    - condition: "cost_sensitive == true"
      preferred_providers:
        - "openai_gpt4o_mini"
        - "anthropic_claude_3_haiku"
        - "groq_gemma_7b"
        - "ollama_llama3_2_3b"
        - "hf_distilgpt2"
      
    - condition: "speed_critical == true"
      preferred_providers:
        - "groq_llama3_70b"
        - "groq_mixtral_8x7b"
        - "ollama_phi3_mini"
        - "vllm_mistral_7b"
      
    - condition: "privacy_required == true"
      preferred_providers:
        - "ollama_llama3_1_8b"
        - "vllm_llama2_7b"
        - "hf_flan_t5_base"
        - "tgi_endpoint"
      
    - condition: "multilingual == true"
      preferred_providers:
        - "anthropic_claude_3_sonnet"
        - "openai_gpt4_turbo"
        - "ollama_qwen2_7b"

# Rate limiting configuration per provider
rate_limits:
  openai:
    requests_per_minute: 500
    tokens_per_minute: 200000
    requests_per_day: 10000
    # burst_limit: 10              # Allow burst requests
    
  anthropic:
    requests_per_minute: 50
    tokens_per_minute: 100000
    requests_per_day: 1000
    
  together:
    requests_per_minute: 600
    tokens_per_minute: 1000000
    
  groq:
    requests_per_minute: 30
    tokens_per_minute: 20000
    # Note: Groq has very high token/second but low requests/minute
    
  cohere:
    requests_per_minute: 100
    tokens_per_minute: 500000
  
  # Local providers typically don't have rate limits
  # ollama:
  #   concurrent_requests: 4       # Limit concurrent requests
  # vllm:
  #   concurrent_requests: 8
  # huggingface:
  #   concurrent_requests: 2

# Caching configuration
caching:
  enabled: true
  backend: "memory"               # "memory", "redis", "file"
  ttl_seconds: 3600              # Time to live
  max_size: "1GB"                # Maximum cache size
  
  # Redis configuration (if backend is "redis")
  # redis:
  #   host: "localhost"
  #   port: 6379
  #   db: 0
  #   password: "${REDIS_PASSWORD}"
  
  # File cache configuration (if backend is "file")
  # file:
  #   cache_dir: "./cache"
  #   max_files: 10000

# Monitoring and observability
monitoring:
  # Basic monitoring
  track_usage: true
  log_requests: true
  log_responses: false            # Set to true for debugging
  alert_on_errors: true
  
  # Cost monitoring
  cost_alert_threshold: 10.0      # Alert when daily cost exceeds $10
  cost_tracking_enabled: true
  
  # Performance monitoring
  latency_alert_threshold_ms: 5000
  track_latency_percentiles: true
  
  # Error monitoring
  error_rate_threshold: 0.05      # Alert at 5% error rate
  consecutive_failures_threshold: 3
  
  # Metrics collection
  # metrics_backend: "prometheus"  # "prometheus", "datadog", "cloudwatch"
  # metrics_port: 9090
  # metrics_endpoint: "/metrics"
  
  # Logging configuration
  # log_level: "INFO"              # "DEBUG", "INFO", "WARNING", "ERROR"
  # log_format: "json"             # "json", "text"
  # log_file: "./logs/models.log"

# Load balancing for multiple instances of same model
# load_balancing:
#   strategy: "round_robin"        # "round_robin", "least_connections", "weighted"
#   health_check_interval: 30      # Seconds between health checks
#   health_check_timeout: 5        # Timeout for health checks
#   
#   providers:
#     ollama_cluster:
#       instances:
#         - host: "localhost"
#           port: 11434
#           weight: 1
#         - host: "gpu-server-1"
#           port: 11434
#           weight: 2
#         - host: "gpu-server-2"
#           port: 11434
#           weight: 2

# Security settings
security:
  # API key validation
  validate_api_keys: true
  mask_api_keys_in_logs: true
  
  # Input/output filtering
  # input_max_length: 100000       # Maximum input length
  # output_max_length: 50000       # Maximum output length
  # filter_pii: false              # Basic PII filtering
  
  # Request signing (advanced)
  # request_signing: false
  # signing_key: "${SIGNING_KEY}"

# Development and testing settings
development:
  # Mock responses for testing
  mock_mode: false
  mock_responses:
    default: "This is a mock response for testing."
  
  # Debug settings  
  debug_mode: false
  verbose_logging: false
  save_request_history: false
  
  # Testing helpers
  # dry_run: false                 # Don't actually call APIs
  # response_delay_ms: 0           # Artificial delay for testing

# =============================================================================
# ENVIRONMENT-SPECIFIC OVERRIDES
# =============================================================================

# Environment-specific settings (uncomment as needed)
# environments:
#   development:
#     default_provider: "ollama_llama3_1_8b"  # Use local models in dev
#     monitoring:
#       log_responses: true                     # More verbose logging
#     development:
#       debug_mode: true
#   
#   staging:
#     fallback_chain: 
#       - "openai_gpt4o_mini"
#       - "ollama_llama3_1_8b"
#     monitoring:
#       cost_alert_threshold: 5.0              # Lower threshold in staging
#   
#   production:
#     fallback_chain:
#       - "openai_gpt4o_mini"
#       - "anthropic_claude_3_haiku"
#       - "together_llama3_70b"
#     monitoring:
#       track_usage: true
#       alert_on_errors: true
#       cost_alert_threshold: 100.0            # Higher threshold in prod
#     caching:
#       enabled: true
#       backend: "redis"
#     security:
#       validate_api_keys: true
#       mask_api_keys_in_logs: true

# =============================================================================
# USAGE EXAMPLES IN COMMENTS
# =============================================================================

# Example usage patterns:
#
# 1. Basic query with default provider:
#    uv run python cli.py query "What is machine learning?"
#
# 2. Use specific provider:
#    uv run python cli.py query "Explain quantum computing" --provider anthropic_claude_3_sonnet
#
# 3. Cost-sensitive queries:
#    uv run python cli.py query "Simple question" --provider openai_gpt4o_mini
#
# 4. Code generation:
#    uv run python cli.py query "Write a Python function to sort a list" --provider ollama_codellama_13b
#
# 5. Local-only for privacy:
#    uv run python cli.py query "Sensitive data question" --provider ollama_llama3_1_8b
#
# 6. Batch processing:
#    uv run python cli.py batch questions.txt --provider groq_mixtral_8x7b
#
# 7. Interactive chat:
#    uv run python cli.py chat --provider anthropic_claude_3_haiku