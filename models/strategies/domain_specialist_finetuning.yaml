# Domain Specialist Fine-Tuning Strategy
# Advanced fine-tuning for domain-specific expertise and high-performance applications
version: "v1"

# Strategy metadata
strategy_info:
  name: "domain_specialist_finetuning"
  description: "Advanced fine-tuning strategy for creating domain-specific AI specialists"
  use_case: "Medical AI, legal assistants, technical support, specialized consulting"
  difficulty: "advanced"
  training_time: "2-8 hours"
  deployment_type: "fine_tuned_local"
  fallback_strategy: "specialist_high → specialist_medium → general_purpose"
  
# Environment-specific configurations for domain specialization
environments:
  
  # Apple Silicon - Medical/Legal Domain Specialization
  medical_apple_silicon:
    active: true
    model:
      base_model: "llama3.1:8b"
      model_type: "Ollama"
      device: "mps"
      ollama_model: "llama3.1:8b"
      download_if_missing: true
      torch_dtype: "float16"
      memory_fraction: 0.85
      special_tokens: ["<PATIENT>", "<DIAGNOSIS>", "<TREATMENT>", "<MEDICATION>"]
      
    training:
      method: "qlora"
      batch_size: 2
      gradient_accumulation_steps: 8
      max_steps: 1000              # Longer training for specialization
      learning_rate: 2e-4
      warmup_steps: 100
      logging_steps: 25
      eval_steps: 200
      save_steps: 200
      weight_decay: 0.01
      
    lora_config:
      r: 32                        # Higher rank for domain adaptation
      alpha: 64
      dropout: 0.05                # Lower dropout for precision
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "float16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: true
      
    generation:
      temperature: 0.3             # Lower temperature for factual accuracy
      top_p: 0.85
      max_length: 1024
      do_sample: true
      
  # NVIDIA GPU - Technical Support Specialization
  technical_nvidia_gpu:
    active: false
    model:
      base_model: "codellama:13b"
      model_type: "Ollama"
      device: "cuda"
      ollama_model: "codellama:13b"
      download_if_missing: true
      torch_dtype: "bfloat16"
      special_tokens: ["<ERROR>", "<SOLUTION>", "<STEPS>", "<CODE>"]
      
    training:
      method: "qlora"
      batch_size: 4
      gradient_accumulation_steps: 4
      max_steps: 1500
      learning_rate: 1e-4
      warmup_steps: 150
      logging_steps: 30
      eval_steps: 300
      save_steps: 300
      weight_decay: 0.01
      dataloader_num_workers: 8
      
    lora_config:
      r: 64                        # High rank for technical complexity
      alpha: 128
      dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "bfloat16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: true
      
    generation:
      temperature: 0.2             # Very low for technical accuracy
      top_p: 0.8
      max_length: 2048
      do_sample: true
      
  # Legal Specialization - Enterprise Grade
  legal_enterprise:
    active: false
    model:
      base_model: "llama3.1:70b"
      model_type: "CausalLM"
      device: "cuda"
      load_in_4bit: true
      torch_dtype: "bfloat16"
      special_tokens: ["<CASE>", "<PRECEDENT>", "<STATUTE>", "<CONTRACT>"]
      
    training:
      method: "qlora"
      batch_size: 8
      gradient_accumulation_steps: 2
      max_steps: 2000              # Extensive training for legal precision
      learning_rate: 8e-5          # Conservative learning rate
      warmup_steps: 200
      logging_steps: 50
      eval_steps: 400
      save_steps: 400
      weight_decay: 0.01
      gradient_checkpointing: true
      
    lora_config:
      r: 128                       # Maximum rank for legal complexity
      alpha: 256
      dropout: 0.05
      target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
      bias: "none"
      task_type: "CAUSAL_LM"
      
    quantization:
      load_in_4bit: true
      bnb_4bit_compute_dtype: "bfloat16"
      bnb_4bit_quant_type: "nf4"
      bnb_4bit_use_double_quant: true
      
    generation:
      temperature: 0.1             # Extremely low for legal precision
      top_p: 0.75
      max_length: 4096             # Long responses for legal analysis
      do_sample: false             # Deterministic for legal consistency

# Domain-specific dataset configurations
dataset:
  path: "datasets/domain_specialist.jsonl"
  format: "jsonl"
  text_column: "output"
  prompt_template: |
    ### Domain Expert Query:
    {instruction}
    
    ### Specialist Response:
    {output}
  max_length: 2048
  validation_split: 0.2            # Larger validation for domain accuracy
  min_examples: 100                # Minimum for domain adaptation
  recommended_examples: 500        # Recommended for good results
  optimal_examples: 2000           # Optimal for specialist performance

# Specialized evaluation for domain expertise
evaluation:
  metrics:
    - "perplexity"
    - "domain_accuracy"
    - "specialist_precision"
    - "safety_compliance"
    - "factual_consistency"
  eval_dataset: "validation"
  custom_metrics:
    domain_accuracy:
      threshold: 0.85              # 85% accuracy on domain tasks
      expert_review: true
    safety_compliance:
      medical_disclaimers: true
      legal_caveats: true
      ethical_guidelines: true
  
# Output configuration for domain specialists
output:
  model_name: "domain-specialist"
  save_directory: "./models/specialists"
  push_to_hub: false
  export_formats: ["pytorch", "onnx"]
  versioning: true                 # Track model versions
  
# Advanced safety for domain applications
safety:
  content_filter: true
  domain_safety_check: true       # Domain-specific safety rules
  expert_validation_required: true
  disclaimer_injection: true      # Add appropriate disclaimers
  confidence_thresholding: true   # Flag low-confidence responses
  max_toxicity_score: 0.05        # Stricter safety for professional use
  
# Hardware requirements for domain specialization
hardware_requirements:
  medical_apple_silicon:
    min_memory: "16GB"
    recommended_memory: "32GB"
    disk_space: "30GB"
    estimated_time: "2-4 hours"
    notes: "Medical domain specialization on Apple Silicon"
    
  technical_nvidia_gpu:
    min_memory: "32GB"
    recommended_memory: "64GB"
    gpu_memory: "16GB"
    disk_space: "50GB"
    estimated_time: "3-6 hours"
    notes: "Technical support specialization requires substantial resources"
    
  legal_enterprise:
    min_memory: "128GB"
    recommended_memory: "256GB"
    gpu_memory: "40GB"
    disk_space: "100GB"
    estimated_time: "6-12 hours"
    notes: "Legal specialization requires enterprise-grade hardware"

# Environment auto-detection for domain specialization
auto_environment:
  enabled: true
  detection_order:
    - "legal_enterprise"
    - "technical_nvidia_gpu"
    - "medical_apple_silicon"
    
  selection_criteria:
    legal_enterprise:
      - "torch.cuda.device_count() >= 4"
      - "torch.cuda.get_device_properties(0).total_memory > 35 * 1024**3"
      - "psutil.virtual_memory().total > 120 * 1024**3"
    technical_nvidia_gpu:
      - "torch.cuda.is_available()"
      - "torch.cuda.get_device_properties(0).total_memory > 14 * 1024**3"
      - "psutil.virtual_memory().total > 30 * 1024**3"
    medical_apple_silicon:
      - "platform.system() == 'Darwin'"
      - "torch.backends.mps.is_available()"
      - "psutil.virtual_memory().total > 14 * 1024**3"

# Domain specialization features
domain_specialization:
  medical:
    safety_checks: true
    disclaimer_required: true
    hipaa_compliance: true
    training_focus:
      - "medical_terminology"
      - "patient_interaction"
      - "diagnosis_support"
      - "treatment_recommendations"
      
  legal:
    citation_format: "bluebook"
    jurisdiction_aware: true
    ethics_compliance: true
    training_focus:
      - "legal_reasoning"
      - "case_analysis"
      - "contract_review"
      - "compliance_checking"
      
  technical:
    programming_languages: ["python", "javascript", "java", "go"]
    code_execution_safe: true
    training_focus:
      - "troubleshooting"
      - "code_debugging"
      - "system_administration"
      - "api_documentation"

# LlamaFactory integration for domain specialists
llamafactory:
  model_name: meta-llama/Llama-2-13b-chat-hf
  template: llama2
  
  # Advanced training configuration
  stage: sft
  do_train: true
  finetuning_type: qlora
  lora_target: all
  lora_rank: 64
  lora_alpha: 128
  
  # Domain-specific dataset handling
  dataset_dir: ../datasets/domain_specialist
  dataset: medical_legal_technical
  cutoff_len: 2048
  max_samples: 2000
  overwrite_cache: true
  preprocessing_num_workers: 8
  
  # High-quality training
  output_dir: ./specialist_output
  logging_steps: 25
  save_steps: 200
  plot_loss: true
  overwrite_output_dir: true
  
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-4
  num_train_epochs: 5.0
  lr_scheduler_type: cosine
  warmup_steps: 100
  bf16: true
  
  # Specialist evaluation
  val_size: 0.2
  per_device_eval_batch_size: 1
  eval_strategy: steps
  eval_steps: 200

# Continuous improvement for specialists
continuous_learning:
  retrain_frequency: "monthly"
  feedback_integration: true
  expert_review_cycle: "weekly"
  performance_monitoring: true
  
# Use cases for domain specialists
specialist_applications:
  medical:
    - "Clinical decision support system"
    - "Medical literature analysis"
    - "Patient history summarization"
    - "Drug interaction checking"
    
  legal:
    - "Contract analysis and review"
    - "Legal research assistant"
    - "Compliance monitoring"
    - "Case precedent search"
    
  technical:
    - "Advanced coding assistant"
    - "System troubleshooting expert"
    - "API documentation generator"
    - "Code review automation"