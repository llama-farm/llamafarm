# Models System Unified Schema
# Version: 2.0.0
# This comprehensive schema combines strategy-level configuration with all component schemas
# Unified schema for model orchestration strategies and components
#
# IMPLEMENTATION STATUS:
# âœ… IMPLEMENTED:
#    - OpenAI provider integration
#    - Ollama local model support
#    - Basic strategy configuration
#    - Fine-tuning strategies (LoRA, QLoRA)
#    - Fine Tuning Frameworks (PyTorch, LLamaFactory)
#    - Basic cache implementations
#    - Basic strategy configuration (simple_chat, multi_provider, local_first, cost_optimized)
#    - Fine-tuning strategies (LoRA, QLoRA, Full)
#
# ðŸš§ COMING SOON:
#    - Full 
#    - Anthropic provider
#    - Google AI (Gemini) provider
#    - HuggingFace Transformers
#    - vLLM and TGI inference engines
#    - All routers (SimpleRouter, LoadBalancedRouter, etc.)
#    - All processors (PromptProcessor, ResponseProcessor, etc.)
#    - All validators (SchemaValidator, QualityValidator, etc.)
#    - All cache implementations
#    - Advanced routing rules
#    - Full monitoring and observability
#    - Cost management and tracking
#
# ðŸ“ PARTIALLY IMPLEMENTED:
#    - Optimization settings (basic features available)
#    - Strategy templates (simple configurations work)

version: "2.0.0"
schema_type: "unified"
description: "Complete Models system schema defining strategies, providers, and their configurations"

# ==============================================================================
# STRATEGY DEFINITION SCHEMA
# ==============================================================================

strategy_definition:
  type: "object"
  required: ["name", "description", "components"]
  properties:
    name:
      type: "string"
      pattern: "^[a-z][a-z0-9_]*$"
      description: "Unique strategy identifier (lowercase, underscores)"
      examples: ["simple_chat", "multi_provider", "local_first", "cost_optimized"]
    
    description:
      type: "string"
      minLength: 10
      maxLength: 500
      description: "Clear description of the strategy's purpose and use cases"
    
    version:
      type: "string"
      pattern: "^\\d+\\.\\d+\\.\\d+$"
      default: "1.0.0"
      description: "Strategy version (semantic versioning)"
    
    tags:
      type: "array"
      items:
        type: "string"
      description: "Tags for categorization and discovery"
      examples: [["production", "optimized"], ["development", "testing"], ["local", "privacy"]]
    
    use_cases:
      type: "array"
      items:
        type: "string"
      minItems: 1
      description: "Specific use cases this strategy is designed for"
      examples: [["chatbot", "code_generation"], ["data_analysis", "summarization"]]
    
    components:
      type: "object"
      required: ["primary_provider"]
      properties:
        primary_provider:
          $ref: "#/components/providers"
        fallback_providers:
          type: "array"
          items:
            $ref: "#/components/providers"
        router:
          $ref: "#/components/routers"
        processors:
          type: "array"
          items:
            $ref: "#/components/processors"
        validators:
          type: "array"
          items:
            $ref: "#/components/validators"
        cache:
          $ref: "#/components/cache"
    
    routing_rules:
      $ref: "#/routing_rules"
    
    optimization:
      $ref: "#/optimization_settings"
    
    monitoring:
      $ref: "#/monitoring_settings"
    
    cost_management:
      $ref: "#/cost_management"

# ==============================================================================
# COMPONENT DEFINITIONS
# ==============================================================================

components:
  # =============================================================================
  # PROVIDERS - Model Provider Components
  # =============================================================================
  providers:
    # Cloud API Providers
    OpenAI:
      type: "cloud_api"
      description: "OpenAI API integration for GPT models"
      class_name: "OpenAIProvider"
      module: "components.providers.openai"
      capabilities: ["text_generation", "chat", "embeddings", "function_calling", "vision", "audio"]
      cost: "paid"
      privacy: "cloud"
      config_schema:
        type: "object"
        required: ["api_key"]
        properties:
          api_key:
            type: "string"
            description: "OpenAI API key (can use ${OPENAI_API_KEY} for env var)"
          organization:
            type: "string"
            description: "OpenAI organization ID"
          default_model:
            type: "string"
            default: "gpt-3.5-turbo"
            enum: ["gpt-4-turbo", "gpt-4", "gpt-3.5-turbo", "gpt-3.5-turbo-16k"]
          temperature:
            type: "number"
            minimum: 0.0
            maximum: 2.0
            default: 0.7
          max_tokens:
            type: "integer"
            minimum: 1
            maximum: 128000
          timeout:
            type: "integer"
            default: 60
          max_retries:
            type: "integer"
            default: 3
          base_url:
            type: "string"
            default: "https://api.openai.com/v1"
      models:
        gpt-4-turbo:
          context_length: 128000
          cost_per_1k_input: 0.01
          cost_per_1k_output: 0.03
          capabilities: ["chat", "function_calling", "vision"]
        gpt-3.5-turbo:
          context_length: 16385
          cost_per_1k_input: 0.0005
          cost_per_1k_output: 0.0015
          capabilities: ["chat", "function_calling"]
      use_cases: ["production", "rapid_prototyping", "advanced_reasoning"]

    Anthropic:  # COMING SOON
      type: "cloud_api"
      description: "Anthropic Claude models (COMING SOON)"
      class_name: "AnthropicProvider"
      module: "components.providers.anthropic"
      capabilities: ["text_generation", "chat", "vision", "long_context"]
      cost: "paid"
      privacy: "cloud"
      config_schema:
        type: "object"
        required: ["api_key"]
        properties:
          api_key:
            type: "string"
            description: "Anthropic API key"
          default_model:
            type: "string"
            default: "claude-3-sonnet-20240229"
            enum: ["claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"]
          max_tokens:
            type: "integer"
            default: 4096
          temperature:
            type: "number"
            default: 0.7
      models:
        claude-3-opus-20240229:
          context_length: 200000
          cost_per_1k_input: 0.015
          cost_per_1k_output: 0.075
          capabilities: ["chat", "vision", "long_context"]
        claude-3-sonnet-20240229:
          context_length: 200000
          cost_per_1k_input: 0.003
          cost_per_1k_output: 0.015
          capabilities: ["chat", "vision", "long_context"]
      use_cases: ["long_documents", "complex_reasoning", "creative_writing"]

    GoogleAI:  # COMING SOON
      type: "cloud_api"
      description: "Google AI (Gemini) models (COMING SOON)"
      class_name: "GoogleAIProvider"
      module: "components.providers.google"
      capabilities: ["text_generation", "chat", "vision", "multi_modal"]
      cost: "paid"
      privacy: "cloud"
      config_schema:
        type: "object"
        required: ["api_key"]
        properties:
          api_key:
            type: "string"
            description: "Google AI API key"
          default_model:
            type: "string"
            default: "gemini-pro"
            enum: ["gemini-pro", "gemini-pro-vision"]
          safety_settings:
            type: "object"
            description: "Content safety filter settings"
      models:
        gemini-pro:
          context_length: 32000
          cost_per_1k_input: 0.0005
          cost_per_1k_output: 0.0015
          capabilities: ["chat", "function_calling"]
        gemini-pro-vision:
          context_length: 32000
          cost_per_1k_input: 0.0005
          cost_per_1k_output: 0.0015
          capabilities: ["chat", "vision", "multi_modal"]
      use_cases: ["multi_modal", "safety_focused", "google_ecosystem"]

    # Local Model Providers
    Ollama:
      type: "local_app"
      description: "Local model serving with Ollama"
      class_name: "OllamaProvider"
      module: "components.providers.ollama"
      capabilities: ["text_generation", "chat", "embeddings", "model_management"]
      cost: "free"
      privacy: "local"
      config_schema:
        type: "object"
        properties:
          base_url:
            type: "string"
            default: "http://localhost:11434"
          default_model:
            type: "string"
            default: "llama3.2"
          timeout:
            type: "integer"
            default: 300
          auto_start:
            type: "boolean"
            default: true
          gpu_layers:
            type: "integer"
            minimum: 0
          num_thread:
            type: "integer"
            minimum: 1
          models:
            type: "array"
            items:
              type: "object"
              properties:
                name:
                  type: "string"
                pull_on_start:
                  type: "boolean"
                  default: false
      popular_models:
        llama3.2:
          size: "3B"
          quantization: ["Q4_K_M", "Q5_K_M", "Q8_0"]
          capabilities: ["chat", "instruct"]
        mistral:
          size: "7B"
          quantization: ["Q4_K_M", "Q5_K_M"]
          capabilities: ["chat", "instruct"]
        codellama:
          size: "7B-34B"
          quantization: ["Q4_K_M", "Q5_K_M"]
          capabilities: ["code", "chat"]
      use_cases: ["local_development", "privacy_sensitive", "offline_deployment"]

    HuggingFace:  # COMING SOON
      type: "local_library"
      description: "HuggingFace Transformers models (COMING SOON)"
      class_name: "HuggingFaceProvider"
      module: "components.providers.huggingface"
      capabilities: ["text_generation", "classification", "embeddings", "custom_models"]
      cost: "free"
      privacy: "local"
      config_schema:
        type: "object"
        properties:
          model_name:
            type: "string"
            default: "gpt2"
          device:
            type: "string"
            enum: ["cpu", "cuda", "mps", "auto"]
            default: "auto"
          quantization:
            type: "string"
            enum: ["none", "8bit", "4bit"]
            default: "none"
          cache_dir:
            type: "string"
            default: "~/.cache/huggingface"
          use_auth_token:
            type: "boolean"
            default: false
          max_memory:
            type: "object"
            description: "Memory allocation per device"
      use_cases: ["research", "custom_models", "fine_tuning"]

    vLLM:  # COMING SOON
      type: "inference_engine"
      description: "High-performance inference with vLLM (COMING SOON)"
      class_name: "vLLMProvider"
      module: "components.providers.vllm"
      capabilities: ["text_generation", "batch_inference", "continuous_batching"]
      cost: "free"
      privacy: "local"
      performance: "very_high"
      config_schema:
        type: "object"
        required: ["model"]
        properties:
          model:
            type: "string"
            description: "Model name or path"
          tensor_parallel_size:
            type: "integer"
            default: 1
          pipeline_parallel_size:
            type: "integer"
            default: 1
          dtype:
            type: "string"
            enum: ["auto", "half", "float16", "bfloat16", "float32"]
            default: "auto"
          max_model_len:
            type: "integer"
            description: "Maximum sequence length"
          gpu_memory_utilization:
            type: "number"
            minimum: 0.0
            maximum: 1.0
            default: 0.9
      use_cases: ["production", "high_throughput", "batch_processing"]

    TGI:  # COMING SOON
      type: "inference_engine"
      description: "Text Generation Inference by HuggingFace (COMING SOON)"
      class_name: "TGIProvider"
      module: "components.providers.tgi"
      capabilities: ["text_generation", "streaming", "quantization"]
      cost: "free"
      privacy: "local"
      performance: "high"
      config_schema:
        type: "object"
        properties:
          endpoint:
            type: "string"
            default: "http://localhost:8080"
          model_id:
            type: "string"
            description: "HuggingFace model ID"
          quantize:
            type: "string"
            enum: ["bitsandbytes", "gptq", "awq"]
          sharded:
            type: "boolean"
            default: false
          max_batch_prefill_tokens:
            type: "integer"
            default: 4096
      use_cases: ["production", "api_serving", "containerized"]

  # =============================================================================
  # ROUTERS - Request Routing Components (COMING SOON)
  # =============================================================================
  routers:  # COMING SOON - All routers below are not yet implemented
    SimpleRouter:
      description: "Basic request routing with fallback"
      class_name: "SimpleRouter"
      module: "components.routers.simple"
      config_schema:
        type: "object"
        properties:
          strategy:
            type: "string"
            enum: ["sequential", "random", "round_robin"]
            default: "sequential"
          retry_on_failure:
            type: "boolean"
            default: true
          max_retries:
            type: "integer"
            default: 3

    LoadBalancedRouter:
      description: "Load-balanced routing across providers"
      class_name: "LoadBalancedRouter"
      module: "components.routers.load_balanced"
      config_schema:
        type: "object"
        properties:
          algorithm:
            type: "string"
            enum: ["round_robin", "least_connections", "weighted", "response_time"]
            default: "round_robin"
          weights:
            type: "object"
            additionalProperties:
              type: "number"
          health_check_interval:
            type: "integer"
            default: 60

    CostOptimizedRouter:
      description: "Route based on cost optimization"
      class_name: "CostOptimizedRouter"
      module: "components.routers.cost_optimized"
      config_schema:
        type: "object"
        properties:
          max_cost_per_request:
            type: "number"
            minimum: 0.0
          prefer_free:
            type: "boolean"
            default: true
          quality_threshold:
            type: "number"
            minimum: 0.0
            maximum: 1.0
            default: 0.7

    CapabilityRouter:
      description: "Route based on model capabilities"
      class_name: "CapabilityRouter"
      module: "components.routers.capability"
      config_schema:
        type: "object"
        properties:
          required_capabilities:
            type: "array"
            items:
              type: "string"
          preferred_capabilities:
            type: "array"
            items:
              type: "string"
          capability_weights:
            type: "object"
            additionalProperties:
              type: "number"

  # =============================================================================
  # PROCESSORS - Request/Response Processing Components (COMING SOON)
  # =============================================================================
  processors:  # COMING SOON - All processors below are not yet implemented
    PromptProcessor:
      description: "Process and enhance prompts"
      class_name: "PromptProcessor"
      module: "components.processors.prompt"
      config_schema:
        type: "object"
        properties:
          templates:
            type: "object"
            additionalProperties:
              type: "string"
          add_system_prompt:
            type: "boolean"
            default: true
          max_length:
            type: "integer"
            minimum: 1
          truncation_strategy:
            type: "string"
            enum: ["end", "start", "middle"]
            default: "end"

    ResponseProcessor:
      description: "Process and format responses"
      class_name: "ResponseProcessor"
      module: "components.processors.response"
      config_schema:
        type: "object"
        properties:
          format:
            type: "string"
            enum: ["text", "json", "markdown", "html"]
            default: "text"
          strip_whitespace:
            type: "boolean"
            default: true
          remove_special_tokens:
            type: "boolean"
            default: true
          max_length:
            type: "integer"

    TokenCounter:
      description: "Count and track token usage"
      class_name: "TokenCounter"
      module: "components.processors.token_counter"
      config_schema:
        type: "object"
        properties:
          encoding:
            type: "string"
            default: "cl100k_base"
          track_cost:
            type: "boolean"
            default: true
          warn_threshold:
            type: "integer"
            default: 3000

    ContentFilter:
      description: "Filter inappropriate content"
      class_name: "ContentFilter"
      module: "components.processors.content_filter"
      config_schema:
        type: "object"
        properties:
          filter_level:
            type: "string"
            enum: ["none", "low", "medium", "high", "strict"]
            default: "medium"
          custom_filters:
            type: "array"
            items:
              type: "object"
              properties:
                pattern:
                  type: "string"
                action:
                  type: "string"
                  enum: ["block", "warn", "redact"]

  # =============================================================================
  # VALIDATORS - Response Validation Components (COMING SOON)
  # =============================================================================
  validators:  # COMING SOON - All validators below are not yet implemented
    SchemaValidator:
      description: "Validate responses against schemas"
      class_name: "SchemaValidator"
      module: "components.validators.schema"
      config_schema:
        type: "object"
        properties:
          schema:
            type: "object"
            description: "JSON schema for validation"
          strict:
            type: "boolean"
            default: false
          coerce_types:
            type: "boolean"
            default: true

    QualityValidator:
      description: "Validate response quality"
      class_name: "QualityValidator"
      module: "components.validators.quality"
      config_schema:
        type: "object"
        properties:
          min_length:
            type: "integer"
            minimum: 0
          max_length:
            type: "integer"
            minimum: 1
          check_completeness:
            type: "boolean"
            default: true
          check_coherence:
            type: "boolean"
            default: false
          coherence_threshold:
            type: "number"
            minimum: 0.0
            maximum: 1.0
            default: 0.7

    SafetyValidator:
      description: "Validate content safety"
      class_name: "SafetyValidator"
      module: "components.validators.safety"
      config_schema:
        type: "object"
        properties:
          check_pii:
            type: "boolean"
            default: true
          check_toxicity:
            type: "boolean"
            default: true
          check_bias:
            type: "boolean"
            default: false
          toxicity_threshold:
            type: "number"
            minimum: 0.0
            maximum: 1.0
            default: 0.8

  # =============================================================================
  # CACHE - Caching Components (COMING SOON)
  # =============================================================================
  cache:  # COMING SOON - All cache implementations below are not yet implemented
    MemoryCache:
      description: "In-memory response cache"
      class_name: "MemoryCache"
      module: "components.cache.memory"
      config_schema:
        type: "object"
        properties:
          max_size:
            type: "integer"
            default: 1000
          ttl_seconds:
            type: "integer"
            default: 3600
          eviction_policy:
            type: "string"
            enum: ["lru", "lfu", "fifo"]
            default: "lru"

    RedisCache:
      description: "Redis-based distributed cache"
      class_name: "RedisCache"
      module: "components.cache.redis"
      config_schema:
        type: "object"
        properties:
          host:
            type: "string"
            default: "localhost"
          port:
            type: "integer"
            default: 6379
          db:
            type: "integer"
            default: 0
          password:
            type: "string"
          ttl_seconds:
            type: "integer"
            default: 3600
          key_prefix:
            type: "string"
            default: "model_cache"

    DiskCache:
      description: "Disk-based persistent cache"
      class_name: "DiskCache"
      module: "components.cache.disk"
      config_schema:
        type: "object"
        properties:
          cache_dir:
            type: "string"
            default: "./cache"
          max_size_mb:
            type: "integer"
            default: 1000
          ttl_seconds:
            type: "integer"
            default: 86400
          compression:
            type: "boolean"
            default: true

# ==============================================================================
# ROUTING RULES (COMING SOON)
# ==============================================================================

routing_rules:  # COMING SOON - Advanced routing rules not yet implemented
  type: "object"
  description: "Rules for routing requests to providers (COMING SOON)"
  properties:
    rules:
      type: "array"
      items:
        type: "object"
        properties:
          name:
            type: "string"
            description: "Rule name"
          condition:
            type: "object"
            properties:
              token_count:
                type: "object"
                properties:
                  min:
                    type: "integer"
                  max:
                    type: "integer"
              capabilities:
                type: "array"
                items:
                  type: "string"
              content_type:
                type: "string"
                enum: ["text", "code", "math", "creative", "analytical"]
              priority:
                type: "string"
                enum: ["low", "normal", "high", "critical"]
          action:
            type: "object"
            properties:
              provider:
                type: "string"
              model:
                type: "string"
              fallback_provider:
                type: "string"
              temperature_override:
                type: "number"
              max_tokens_override:
                type: "integer"
    
    default_rule:
      type: "object"
      properties:
        provider:
          type: "string"
        model:
          type: "string"

# ==============================================================================
# OPTIMIZATION SETTINGS (PARTIALLY IMPLEMENTED)
# ==============================================================================

optimization_settings:  # PARTIALLY IMPLEMENTED - Basic optimization available
  type: "object"
  description: "Performance and resource optimization (PARTIALLY IMPLEMENTED)"
  properties:
    performance_priority:
      type: "string"
      enum: ["latency", "throughput", "cost", "quality", "balanced"]
      default: "balanced"
      description: "Primary optimization goal"
    
    resource_constraints:
      type: "object"
      properties:
        max_memory_gb:
          type: "number"
          minimum: 0.5
          description: "Maximum memory usage in GB"
        max_concurrent_requests:
          type: "integer"
          minimum: 1
          default: 10
          description: "Maximum concurrent requests"
        gpu_enabled:
          type: "boolean"
          default: false
          description: "GPU acceleration available"
        gpu_memory_gb:
          type: "number"
          minimum: 1
          description: "GPU memory limit in GB"
    
    batching:
      type: "object"
      properties:
        enable_batching:
          type: "boolean"
          default: false
          description: "Enable request batching"
        batch_size:
          type: "integer"
          minimum: 1
          default: 10
          description: "Maximum batch size"
        batch_timeout_ms:
          type: "integer"
          minimum: 10
          default: 100
          description: "Maximum wait time for batch"
    
    caching:
      type: "object"
      properties:
        enable_cache:
          type: "boolean"
          default: true
          description: "Enable response caching"
        cache_similarity_threshold:
          type: "number"
          minimum: 0.0
          maximum: 1.0
          default: 0.95
          description: "Similarity threshold for cache hits"
        cache_backend:
          type: "string"
          enum: ["memory", "redis", "disk"]
          default: "memory"
          description: "Cache storage backend"

# ==============================================================================
# MONITORING SETTINGS (COMING SOON)
# ==============================================================================

monitoring_settings:  # COMING SOON - Monitoring features not yet implemented
  type: "object"
  description: "Monitoring and observability configuration (COMING SOON)"
  properties:
    metrics:
      type: "object"
      properties:
        enable_metrics:
          type: "boolean"
          default: true
          description: "Enable metrics collection"
        metrics_backend:
          type: "string"
          enum: ["console", "prometheus", "datadog", "cloudwatch"]
          default: "console"
          description: "Metrics export backend"
        track_metrics:
          type: "array"
          items:
            type: "string"
            enum: ["latency", "tokens", "cost", "errors", "cache_hits"]
          default: ["latency", "tokens", "cost", "errors"]
    
    logging:
      type: "object"
      properties:
        log_level:
          type: "string"
          enum: ["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"]
          default: "INFO"
        log_requests:
          type: "boolean"
          default: false
          description: "Log all requests"
        log_responses:
          type: "boolean"
          default: false
          description: "Log all responses"
        redact_sensitive:
          type: "boolean"
          default: true
          description: "Redact sensitive information"
    
    tracing:
      type: "object"
      properties:
        enable_tracing:
          type: "boolean"
          default: false
        tracing_backend:
          type: "string"
          enum: ["jaeger", "zipkin", "datadog", "xray"]
        sample_rate:
          type: "number"
          minimum: 0.0
          maximum: 1.0
          default: 0.1

# ==============================================================================
# COST MANAGEMENT (COMING SOON)
# ==============================================================================

cost_management:  # COMING SOON - Cost management features not yet implemented
  type: "object"
  description: "Cost tracking and optimization (COMING SOON)"
  properties:
    budgets:
      type: "object"
      properties:
        daily_limit:
          type: "number"
          minimum: 0.0
          description: "Daily spending limit in USD"
        monthly_limit:
          type: "number"
          minimum: 0.0
          description: "Monthly spending limit in USD"
        per_request_limit:
          type: "number"
          minimum: 0.0
          description: "Per-request spending limit in USD"
    
    alerts:
      type: "object"
      properties:
        enable_alerts:
          type: "boolean"
          default: true
        alert_thresholds:
          type: "array"
          items:
            type: "object"
            properties:
              percentage:
                type: "integer"
                minimum: 0
                maximum: 100
              action:
                type: "string"
                enum: ["warn", "throttle", "block"]
    
    optimization:
      type: "object"
      properties:
        prefer_free_models:
          type: "boolean"
          default: false
        use_smaller_models:
          type: "boolean"
          default: false
        cache_expensive_calls:
          type: "boolean"
          default: true

# ==============================================================================
# PREDEFINED STRATEGY TEMPLATES
# ==============================================================================

strategy_templates:
  simple_chat:
    name: "simple_chat"
    description: "Simple chat configuration with single provider"
    components:
      primary_provider:
        type: "OpenAI"
        config:
          api_key: "${OPENAI_API_KEY}"
          default_model: "gpt-3.5-turbo"
          temperature: 0.7
      cache:
        type: "MemoryCache"
        config:
          max_size: 100
          ttl_seconds: 3600
    optimization:
      performance_priority: "latency"
    monitoring:
      metrics:
        enable_metrics: true
        track_metrics: ["latency", "tokens", "cost"]

  multi_provider:
    name: "multi_provider"
    description: "Multi-provider setup with fallback"
    components:
      primary_provider:
        type: "OpenAI"
        config:
          api_key: "${OPENAI_API_KEY}"
          default_model: "gpt-3.5-turbo"
      fallback_providers:
        - type: "Anthropic"
          config:
            api_key: "${ANTHROPIC_API_KEY}"
            default_model: "claude-3-haiku-20240307"
        - type: "GoogleAI"
          config:
            api_key: "${GOOGLE_API_KEY}"
            default_model: "gemini-pro"
      router:
        type: "LoadBalancedRouter"
        config:
          algorithm: "weighted"
          weights:
            OpenAI: 0.5
            Anthropic: 0.3
            GoogleAI: 0.2
    routing_rules:
      rules:
        - name: "long_context"
          condition:
            token_count:
              min: 8000
          action:
            provider: "Anthropic"
            model: "claude-3-sonnet-20240229"
        - name: "vision_tasks"
          condition:
            capabilities: ["vision"]
          action:
            provider: "GoogleAI"
            model: "gemini-pro-vision"
    optimization:
      performance_priority: "balanced"
      caching:
        enable_cache: true
        cache_backend: "redis"

  local_first:
    name: "local_first"
    description: "Privacy-focused local model with cloud fallback"
    components:
      primary_provider:
        type: "Ollama"
        config:
          base_url: "http://localhost:11434"
          default_model: "llama3.2"
          gpu_layers: 35
      fallback_providers:
        - type: "OpenAI"
          config:
            api_key: "${OPENAI_API_KEY}"
            default_model: "gpt-3.5-turbo"
      router:
        type: "SimpleRouter"
        config:
          strategy: "sequential"
          retry_on_failure: true
      processors:
        - type: "ContentFilter"
          config:
            filter_level: "high"
      validators:
        - type: "SafetyValidator"
          config:
            check_pii: true
            check_toxicity: true
    routing_rules:
      rules:
        - name: "complex_tasks"
          condition:
            content_type: "analytical"
            priority: "high"
          action:
            provider: "OpenAI"
            model: "gpt-4"
    optimization:
      performance_priority: "latency"
      resource_constraints:
        gpu_enabled: true
        gpu_memory_gb: 8

  cost_optimized:
    name: "cost_optimized"
    description: "Cost-optimized configuration with smart routing"
    components:
      primary_provider:
        type: "OpenAI"
        config:
          api_key: "${OPENAI_API_KEY}"
          default_model: "gpt-3.5-turbo"
      fallback_providers:
        - type: "Ollama"
          config:
            base_url: "http://localhost:11434"
            default_model: "mistral"
      router:
        type: "CostOptimizedRouter"
        config:
          max_cost_per_request: 0.01
          prefer_free: true
          quality_threshold: 0.7
      cache:
        type: "DiskCache"
        config:
          cache_dir: "./model_cache"
          max_size_mb: 5000
          compression: true
    routing_rules:
      rules:
        - name: "simple_queries"
          condition:
            token_count:
              max: 500
            content_type: "text"
          action:
            provider: "Ollama"
            model: "mistral"
        - name: "complex_queries"
          condition:
            token_count:
              min: 2000
            priority: "high"
          action:
            provider: "OpenAI"
            model: "gpt-4-turbo"
    cost_management:
      budgets:
        daily_limit: 10.0
        per_request_limit: 0.05
      optimization:
        prefer_free_models: true
        cache_expensive_calls: true

  production:
    name: "production"
    description: "Production-ready configuration with full monitoring"
    components:
      primary_provider:
        type: "OpenAI"
        config:
          api_key: "${OPENAI_API_KEY}"
          default_model: "gpt-4-turbo"
          max_retries: 5
          timeout: 120
      fallback_providers:
        - type: "Anthropic"
          config:
            api_key: "${ANTHROPIC_API_KEY}"
            default_model: "claude-3-sonnet-20240229"
        - type: "GoogleAI"
          config:
            api_key: "${GOOGLE_API_KEY}"
            default_model: "gemini-pro"
      router:
        type: "LoadBalancedRouter"
        config:
          algorithm: "response_time"
          health_check_interval: 30
      processors:
        - type: "PromptProcessor"
          config:
            add_system_prompt: true
            max_length: 4000
        - type: "ResponseProcessor"
          config:
            format: "json"
            strip_whitespace: true
        - type: "TokenCounter"
          config:
            track_cost: true
            warn_threshold: 5000
      validators:
        - type: "SchemaValidator"
          config:
            strict: false
        - type: "QualityValidator"
          config:
            min_length: 10
            check_completeness: true
        - type: "SafetyValidator"
          config:
            check_pii: true
            check_toxicity: true
      cache:
        type: "RedisCache"
        config:
          host: "redis.production.internal"
          ttl_seconds: 7200
    optimization:
      performance_priority: "quality"
      resource_constraints:
        max_concurrent_requests: 100
      batching:
        enable_batching: true
        batch_size: 20
        batch_timeout_ms: 50
      caching:
        enable_cache: true
        cache_similarity_threshold: 0.98
        cache_backend: "redis"
    monitoring:
      metrics:
        enable_metrics: true
        metrics_backend: "prometheus"
        track_metrics: ["latency", "tokens", "cost", "errors", "cache_hits"]
      logging:
        log_level: "INFO"
        log_requests: true
        log_responses: false
        redact_sensitive: true
      tracing:
        enable_tracing: true
        tracing_backend: "jaeger"
        sample_rate: 0.1
    cost_management:
      budgets:
        daily_limit: 500.0
        monthly_limit: 10000.0
      alerts:
        enable_alerts: true
        alert_thresholds:
          - percentage: 50
            action: "warn"
          - percentage: 80
            action: "throttle"
          - percentage: 100
            action: "block"

# ==============================================================================
# FINE-TUNING STRATEGIES
# ==============================================================================

fine_tuning_strategies:
  LoRA:
    description: "Low-Rank Adaptation for efficient fine-tuning"
    supported_models: ["llama", "mistral", "phi", "gemma"]
    config_schema:
      type: "object"
      properties:
        r:
          type: "integer"
          default: 8
          description: "LoRA rank"
        lora_alpha:
          type: "integer"
          default: 16
          description: "LoRA alpha parameter"
        lora_dropout:
          type: "number"
          default: 0.05
          description: "LoRA dropout"
        target_modules:
          type: "array"
          items:
            type: "string"
          default: ["q_proj", "v_proj"]

  QLoRA:
    description: "Quantized LoRA for memory-efficient fine-tuning"
    supported_models: ["llama", "mistral", "phi"]
    config_schema:
      type: "object"
      properties:
        quantization:
          type: "string"
          enum: ["4bit", "8bit"]
          default: "4bit"
        bnb_4bit_compute_dtype:
          type: "string"
          default: "float16"
        bnb_4bit_quant_type:
          type: "string"
          default: "nf4"

  FullFineTuning:
    description: "Full model fine-tuning"
    supported_models: ["gpt2", "bert", "t5"]
    config_schema:
      type: "object"
      properties:
        learning_rate:
          type: "number"
          default: 5e-5
        batch_size:
          type: "integer"
          default: 8
        gradient_accumulation_steps:
          type: "integer"
          default: 4
        num_epochs:
          type: "integer"
          default: 3

# ==============================================================================
# COMPATIBILITY AND VALIDATION RULES
# ==============================================================================

compatibility_rules:
  provider_model_compatibility:
    description: "Provider and model compatibility matrix"
    rules:
      - provider: "OpenAI"
        compatible_models: ["gpt-4*", "gpt-3.5*", "text-embedding*"]
        capabilities: ["chat", "embeddings", "function_calling", "vision"]
      
      - provider: "Anthropic"
        compatible_models: ["claude-3*"]
        capabilities: ["chat", "vision", "long_context"]
      
      - provider: "Ollama"
        compatible_models: ["llama*", "mistral*", "codellama*", "phi*"]
        capabilities: ["chat", "embeddings", "local"]
      
      - provider: "HuggingFace"
        compatible_models: ["*"]
        capabilities: ["custom_models", "fine_tuning"]
  
  router_provider_compatibility:
    description: "Router and provider compatibility"
    rules:
      - router: "SimpleRouter"
        compatible_providers: ["all"]
        recommended_for: ["development", "simple_setups"]
      
      - router: "LoadBalancedRouter"
        compatible_providers: ["cloud_api"]
        recommended_for: ["production", "high_availability"]
      
      - router: "CostOptimizedRouter"
        compatible_providers: ["all"]
        recommended_for: ["cost_sensitive", "mixed_workloads"]
      
      - router: "CapabilityRouter"
        compatible_providers: ["all"]
        recommended_for: ["complex_routing", "capability_based"]

validation_rules:
  required_fields:
    strategy:
      - "name"
      - "description"
      - "components.primary_provider"
  
  field_constraints:
    strategy_name:
      pattern: "^[a-z][a-z0-9_]*$"
      max_length: 50
    temperature:
      minimum: 0.0
      maximum: 2.0
    max_tokens:
      minimum: 1
      maximum: 128000
    timeout:
      minimum: 1
      maximum: 600
  
  performance_warnings:
    - condition: "fallback_providers.length > 3"
      message: "Using more than 3 fallback providers may impact latency"
      severity: "warning"
    
    - condition: "cache.type == 'MemoryCache' && cache.config.max_size > 10000"
      message: "Large memory cache may impact application memory"
      severity: "warning"
    
    - condition: "optimization.batching.batch_size > 50"
      message: "Large batch sizes may cause timeout issues"
      severity: "warning"

# ==============================================================================
# USAGE EXAMPLES
# ==============================================================================

examples:
  chatbot:
    description: "Simple chatbot configuration"
    strategy:
      name: "chatbot"
      description: "Interactive chatbot with conversation memory"
      components:
        primary_provider:
          type: "OpenAI"
          config:
            api_key: "${OPENAI_API_KEY}"
            default_model: "gpt-3.5-turbo"
            temperature: 0.8
        processors:
          - type: "PromptProcessor"
            config:
              add_system_prompt: true
        cache:
          type: "MemoryCache"
          config:
            max_size: 50
      optimization:
        performance_priority: "latency"

  code_assistant:
    description: "Code generation and assistance"
    strategy:
      name: "code_assistant"
      description: "Specialized code generation and debugging assistant"
      components:
        primary_provider:
          type: "OpenAI"
          config:
            api_key: "${OPENAI_API_KEY}"
            default_model: "gpt-4"
            temperature: 0.2
        fallback_providers:
          - type: "Ollama"
            config:
              default_model: "codellama"
        processors:
          - type: "PromptProcessor"
            config:
              templates:
                system: "You are an expert programmer. Provide clean, well-commented code."
        validators:
          - type: "SchemaValidator"
            config:
              strict: true
      routing_rules:
        rules:
          - name: "simple_code"
            condition:
              token_count:
                max: 1000
            action:
              provider: "Ollama"
              model: "codellama"

  data_analyst:
    description: "Data analysis and insights"
    strategy:
      name: "data_analyst"
      description: "Analyze data and provide insights"
      components:
        primary_provider:
          type: "OpenAI"
          config:
            api_key: "${OPENAI_API_KEY}"
            default_model: "gpt-4-turbo"
            temperature: 0.3
        processors:
          - type: "ResponseProcessor"
            config:
              format: "markdown"
        validators:
          - type: "QualityValidator"
            config:
              min_length: 100
              check_completeness: true
      optimization:
        performance_priority: "quality"

# ==============================================================================
# BEST PRACTICES
# ==============================================================================

best_practices:
  provider_selection:
    - "Use OpenAI GPT-4 for complex reasoning and high-quality outputs"
    - "Use Claude for long-context tasks and creative writing"
    - "Use local models (Ollama) for privacy-sensitive data"
    - "Use Google Gemini for multi-modal tasks"
    - "Always configure fallback providers for production systems"
  
  routing_configuration:
    - "Start with SimpleRouter for development"
    - "Use LoadBalancedRouter for production high-availability"
    - "Configure routing rules based on content type and size"
    - "Set appropriate timeouts for each provider"
  
  optimization:
    - "Enable caching for repetitive queries"
    - "Use batching for high-throughput scenarios"
    - "Configure appropriate cache TTLs based on data freshness needs"
    - "Monitor token usage to optimize costs"
  
  monitoring:
    - "Always enable metrics in production"
    - "Set up alerts for budget thresholds"
    - "Log errors but redact sensitive information"
    - "Use distributed tracing for debugging complex flows"
  
  cost_management:
    - "Set daily and monthly budget limits"
    - "Use smaller models for simple tasks"
    - "Cache expensive API calls"
    - "Route to local models when possible"
    - "Monitor cost per request metrics"